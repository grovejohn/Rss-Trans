<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>Huggingface 日报</title>
    <link>https://huggingface.co/papers</link>
    <description>Huggingface Daily Papers - 由 RSSHub 用爱制作（https://github.com/DIYgod/RSSHub）</description>
    <lastBuildDate>Thu, 02 May 2024 07:35:56 GMT</lastBuildDate>
    <item>
      <title>Clover：具有顺序知识的回归轻量级推测解码</title>
      <link>https://arxiv.org/abs/2405.00263</link>
      <description><![CDATA[由于自回归解码的要求与大多数当代 GPU 的设计不匹配，大型语言模型 (LLM) 效率低下。具体来说，数十亿到数万亿的参数必须通过其有限的内存带宽加载到GPU缓存中进行计算，但实际上只计算了一小批令牌。因此，GPU 的大部分时间都花在内存传输上，而不是计算上。最近，并行解码（一种推测解码算法）变得越来越流行，并且在生成过程中表现出了令人印象深刻的效率提高。它向大型模型引入了额外的解码头，使它们能够同时预测多个后续标记，并在单个解码步骤中验证这些候选延续。然而，这种方法偏离了预训练期间使用的下一个令牌预测的训练目标，导致候选令牌的命中率较低。在本文中，我们提出了一种新的推测性解码算法 Clover，它将顺序知识集成到并行解码过程中。这一增强提高了投机者的命中率，从而提高了整体效率。 Clover 通过回归连接传输预先推测的令牌的顺序知识，然后使用注意力解码器来整合这些推测的令牌。此外，Clover 还包含一个增强块，可以修改隐藏状态，以更好地符合推测生成的目的，而不是下一个代币预测。实验结果表明，Clover 在 Baichuan-Small 上的性能比基线高出 91%，在 Baichuan-Large 上的性能比基线高出 146%，在 Baichuan-Large 上的性能比之前表现最好的方法 Medusa 的性能高出 37%。小号和百川大号分别占 57%。]]></description>
      <guid>https://arxiv.org/abs/2405.00263</guid>
      <pubDate>Thu, 02 May 2024 04:50:08 GMT</pubDate>
    </item>
    <item>
      <title>SemantiCodec：适用于一般声音的超低比特率语义音频编解码器</title>
      <link>https://arxiv.org/abs/2405.00233</link>
      <description><![CDATA[大型语言模型 (LLM) 通过将音频转换为离散标记的音频编解码器具有显着先进的音频处理能力，从而能够将语言建模技术应用于音频数据。然而，传统编解码器通常以高比特率或在语音等狭窄领域内运行，并且缺乏有效语言建模所需的语义线索。为了解决这些挑战，我们推出了 SemantiCodec，这是一种新颖的编解码器，旨在将不同音频类型（包括语音、一般音频和音乐）的音频压缩成每秒不到一百个令牌，而不会影响质量。 SemantiCodec 采用双编码器架构：使用自监督 AudioMAE 的语义编码器，在大量音频数据上使用 k 均值聚类进行离散化，以及用于捕获剩余细节的声学编码器。语义和声学编码器输出用于通过基于扩散模型的解码器重建音频。 SemantiCodec 提供三种变体，令牌速率分别为每秒 25、50 和 100，支持 0.31 kbps 到 1.43 kbps 之间的一系列超低比特率。实验结果表明 SemantiCodec 在重建质量方面明显优于最先进的 Descript 编解码器。我们的结果还表明，SemantiCodec 比所有评估的音频编解码器包含更丰富的语义信息，即使比特率明显较低。我们的代码和演示可在 https://haoheliu.github.io/SemantiCodec/ 获取。]]></description>
      <guid>https://arxiv.org/abs/2405.00233</guid>
      <pubDate>Thu, 02 May 2024 04:44:14 GMT</pubDate>
    </item>
    <item>
      <title>编辑批量大小越大越好吗？ -- Llama-3模型编辑的实证研究</title>
      <link>https://arxiv.org/abs/2405.00664</link>
      <description><![CDATA[本研究提出了针对最新大型语言模型 Llama-3 的有针对性的模型编辑分析。我们探索流行的模型编辑技术 - ROME、MEMIT 和 EMMET 的功效，这些技术专为精确的图层干预而设计。我们通过评估确定了最有效的目标编辑层，该评估涵盖三种不同策略的多达 4096 次编辑：顺序编辑、批量编辑和我们称为顺序批量编辑的混合方法。我们的研究结果表明，与在相同数量的编辑中顺序使用较小的编辑批次相比，增加编辑批次大小可能会更显着地降低模型性能。因此，我们认为顺序模型编辑是扩展模型编辑方法的重要组成部分，未来的研究应该集中在结合批量和顺序编辑的方法上。这一观察表明当前模型编辑方法存在潜在的局限性，这些方法推动了更大的编辑批量大小，我们希望它为未来优化批量大小和模型编辑性能的研究铺平道路。]]></description>
      <guid>https://arxiv.org/abs/2405.00664</guid>
      <pubDate>Thu, 02 May 2024 03:09:06 GMT</pubDate>
    </item>
    <item>
      <title>仔细检查大型语言模型在小学算术中的表现</title>
      <link>https://arxiv.org/abs/2405.00332</link>
      <description><![CDATA[大型语言模型 (LLM) 在许多数学推理基准上取得了令人瞩目的成功。然而，人们越来越担心，这种性能的一部分实际上反映了数据集污染，即与基准问题非常相似的数据泄漏到训练数据中，而不是真正的推理能力。为了严格调查这一说法，我们委托 Grade School Math 1000 (GSM1k)。 GSM1k 的设计反映了已建立的 GSM8k 基准的风格和复杂性，GSM8k 基准是衡量基本数学推理的黄金标准。我们确保这两个基准在人类解决率、解决步骤数、答案大小等重要指标上具有可比性。在评估 GSM1k 上领先的开源和闭源法学硕士时，我们观察到准确性下降高达 13%，几个模型系列（例如 Phi 和 Mistral）显示出几乎所有模型大小的系统过度拟合的证据。与此同时，许多模型，尤其是前沿模型（例如 Gemini/GPT/Claude）显示出最小的过度拟合迹象。进一步的分析表明，模型从 GSM8k 生成示例的概率与其 GSM8k 和 GSM1k 之间的性能差距之间存在正相关关系（Spearman 的 r^2=0.32），这表明许多模型可能部分记住了 GSM8k。]]></description>
      <guid>https://arxiv.org/abs/2405.00332</guid>
      <pubDate>Thu, 02 May 2024 02:56:03 GMT</pubDate>
    </item>
    <item>
      <title>具有神经补偿的光谱修剪高斯场</title>
      <link>https://arxiv.org/abs/2405.00676</link>
      <description><![CDATA[近年来，3D高斯溅射作为一种新颖的3D表示方法，因其渲染速度快、渲染质量高而受到人们的关注。然而，这会带来高内存消耗，例如，训练有素的高斯场可能会利用 300 万个高斯基元和超过 700 MB 的内存。我们将这种高内存占用归因于缺乏对基元之间关系的考虑。在本文中，我们提出了一种具有谱修剪和神经补偿功能的内存高效高斯场，名为 SUNDAE。一方面，我们在高斯基元集上构建一个图来建模它们的关系，并设计一个频谱下采样模块来修剪基元，同时保留所需的信号。另一方面，为了补偿剪枝高斯的质量损失，我们利用轻量级神经网络头来混合分散特征，这可以有效地补偿质量损失，同时捕获其权重中基元之间的关系。我们通过广泛的成果展示了圣代的性能。例如，在 Mip-NeRF360 数据集上，SUNDAE 使用 104 MB 内存可在 145 FPS 下实现 26.80 PSNR，而普通高斯泼溅算法可使用 523 MB 内存在 160 FPS 下实现 25.60 PSNR。代码可在 https://runyiyang.github.io/projects/SUNDAE/ 上公开获取。]]></description>
      <guid>https://arxiv.org/abs/2405.00676</guid>
      <pubDate>Thu, 02 May 2024 02:14:33 GMT</pubDate>
    </item>
    </channel>
</rss>