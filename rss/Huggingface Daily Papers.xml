<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>拥抱日报</title>
    <link>https://huggingface.co/papers</link>
    <description>Huggingface Daily Papers - 由 RSSHub 用爱制作（https://github.com/DIYgod/RSSHub）</description>
    <lastBuildDate>Wed, 03 Apr 2024 19:11:49 GMT</lastBuildDate>
    <item>
      <title>3D 凝结：野外 3D 感知图像对齐</title>
      <link>https://arxiv.org/abs/2404.02125</link>
      <description><![CDATA[我们提出了 3D Congealing，这是一个针对捕获语义相似对象的 2D 图像的 3D 感知对齐的新问题。给定一组未标记的互联网图像，我们的目标是将输入中的共享语义部分关联起来，并将 2D 图像中的知识聚合到共享的 3D 规范空间。我们引入了一个通用框架，可以在不假设形状模板、姿势或任何相机参数的情况下处理该任务。其核心是封装几何和语义信息的规范 3D 表示。该框架优化了规范表示以及每个输入图像的姿势，以及将 2D 像素坐标扭曲到 3D 规范框架以考虑形状匹配的每图像坐标图。优化过程融合了来自预训练图像生成模型的先验知识和来自输入图像的语义信息。前者为这种约束下的任务提供了强有力的知识指导，而后者则提供了必要的信息来减轻预训练模型的训练数据偏差。我们的框架可用于各种任务，例如对应匹配、姿态估计和图像编辑，在具有挑战性的照明条件下的真实世界图像数据集和野外在线图像集合上取得了良好的结果。]]></description>
      <guid>https://arxiv.org/abs/2404.02125</guid>
      <pubDate>Wed, 03 Apr 2024 05:09:25 GMT</pubDate>
    </item>
    <item>
      <title>HyperCLOVA X 技术报告</title>
      <link>https://arxiv.org/abs/2404.01954</link>
      <description><![CDATA[我们推出 HyperCLOVA X，这是一个针对韩国语言和文化量身定制的大型语言模型 (LLM) 系列，以及英语、数学和编码方面的竞争能力。 HyperCLOVA X 接受了韩语、英语和代码数据的均衡组合训练，然后使用高质量的人工注释数据集进行指令调整，同时遵守严格的安全准则，反映了我们对负责任的 AI 的承诺。该模型通过韩语和英语的各种基准进行评估，包括综合推理、知识、常识、事实性、编码、数学、聊天、遵循指令和无害性。 HyperCLOVA X 凭借对语言和文化细微差别的深刻理解，展现出强大的韩语推理能力。对固有双语性质及其向多语言的扩展的进一步分析凸显了该模型的跨语言熟练程度和对非目标语言的强大泛化能力，包括多个语言对之间的机器翻译和跨语言推理任务。我们相信HyperCLOVA X可以为地区或国家发展其主权法学硕士提供有益的指导。]]></description>
      <guid>https://arxiv.org/abs/2404.01954</guid>
      <pubDate>Wed, 03 Apr 2024 04:59:14 GMT</pubDate>
    </item>
    <item>
      <title>Octopus v2：超级代理的设备上语言模型</title>
      <link>https://arxiv.org/abs/2404.01744</link>
      <description><![CDATA[语言模型已在各种软件应用程序中显示出有效性，特别是在与自动工作流程相关的任务中。这些模型拥有调用函数的关键能力，这对于创建人工智能代理至关重要。尽管云环境中的大规模语言模型具有高性能，但它们通常与隐私和成本问题相关。当前用于函数调用的设备上模型面临延迟和准确性问题。我们的研究提出了一种新方法，使具有 20 亿个参数的设备上模型能够在准确性和延迟方面超越 GPT-4 的性能，并将上下文长度减少 95%。与具有基于 RAG 的函数调用机制的 Llama-7B 相比，我们的方法将延迟提高了 35 倍。此方法将延迟降低到适合在生产环境中的各种边缘设备上部署的水平，从而符合实际应用程序的性能要求。]]></description>
      <guid>https://arxiv.org/abs/2404.01744</guid>
      <pubDate>Wed, 03 Apr 2024 03:26:09 GMT</pubDate>
    </item>
    <item>
      <title>Poro 34B 与多语言能力的祝福</title>
      <link>https://arxiv.org/abs/2404.01856</link>
      <description><![CDATA[最先进的大型语言模型的预训练现在需要数万亿个单词的文本，这比绝大多数语言的可用数量级要多几个数量级。虽然包含多种语言的文本是获取更多预训练数据的一种明显方法，但多语言通常被视为一种诅咒，并且大多数模型训练工作仍然几乎完全专注于个别大语言。我们相信多语言是一种福祉，并且应该可以通过多语言训练大幅提高小语言单语言模型的能力。在这项研究中，我们介绍了 Poro 34B，这是一个针对芬兰语、英语和编程语言的 1 万亿个标记进行训练的 340 亿个参数模型，并证明多语言训练方法可以生成一个模型，该模型不仅可以大幅提高现有模型的能力芬兰语，但在翻译方面也很出色，并且在生成英语和编程语言方面在同类中具有竞争力。我们在开放许可下发布模型参数、脚本和数据：https://huggingface.co/LumiOpen/Poro-34B。]]></description>
      <guid>https://arxiv.org/abs/2404.01856</guid>
      <pubDate>Wed, 03 Apr 2024 03:22:41 GMT</pubDate>
    </item>
    <item>
      <title>越大并不总是越好：潜在扩散模型的缩放特性</title>
      <link>https://arxiv.org/abs/2404.01367</link>
      <description><![CDATA[我们研究潜在扩散模型（LDM）的缩放特性，重点关注其采样效率。虽然改进的网络架构和推理算法已被证明可以有效提高扩散模型的采样效率，但模型大小（采样效率的关键决定因素）的作用尚未得到彻底检验。通过对已建立的文本到图像扩散模型的实证分析，我们深入研究了模型大小如何影响不同采样步骤的采样效率。我们的研究结果揭示了一个令人惊讶的趋势：在给定的推理预算下运行时，较小的模型在生成高质量结果方面常常优于较大的模型。此外，我们通过应用各种扩散采样器、探索不同的下游任务、评估蒸馏后模型以及比较相对于训练计算的性能，扩展了我们的研究，以证明这些发现的普遍性。这些发现为 LDM 扩展策略的开发开辟了新的途径，可用于在有限的推理预算内增强生成能力。]]></description>
      <guid>https://arxiv.org/abs/2404.01367</guid>
      <pubDate>Wed, 03 Apr 2024 03:18:17 GMT</pubDate>
    </item>
    <item>
      <title>LLM-ABR：通过大型语言模型设计自适应比特率算法</title>
      <link>https://arxiv.org/abs/2404.01617</link>
      <description><![CDATA[我们推出了 LLM-ABR，这是第一个利用大语言模型 (LLM) 的生成能力来自主设计针对不同网络特征量身定制的自适应比特率 (ABR) 算法的系统。 LLM-ABR 在强化学习框架内运行，使法学硕士能够设计状态和神经网络架构等关键组件。我们在不同的网络设置（包括宽带、卫星、4G 和 5G）中评估 LLM-ABR。 LLM-ABR 始终优于默认 ABR 算法。]]></description>
      <guid>https://arxiv.org/abs/2404.01617</guid>
      <pubDate>Wed, 03 Apr 2024 02:32:57 GMT</pubDate>
    </item>
    <item>
      <title>大型语言模型是超人化学家吗？</title>
      <link>https://arxiv.org/abs/2404.01475</link>
      <description><![CDATA[大型语言模型（LLM）因其处理人类语言和执行未经明确训练的任务的能力而引起了广泛的兴趣。这与化学科学相关，化学科学面临着通常采用文本形式的小而多样化的数据集的问题。法学硕士在解决这些问题方面表现出了希望，并且越来越多地被用来预测化学性质、优化反应，甚至自主设计和进行实验。然而，我们对法学硕士的化学推理能力的系统性了解仍然非常有限，这需要改进模型并减轻潜在危害。在这里，我们介绍“ChemBench”，这是一个自动化框架，旨在根据人类化学家的专业知识严格评估最先进的法学硕士的化学知识和推理能力。我们为化学科学的各个子领域策划了 7,000 多个问答对，评估了领先的开源和闭源法学硕士，发现在我们的研究中，最好的模型平均表现优于最好的人类化学家。然而，这些模型在处理一些对人类专家来说很容易的化学推理任务时遇到了困难，并提供了过于自信、误导性的预测，例如关于化学品安全概况的预测。这些发现强调了一个双重现实，即尽管法学硕士在化学任务方面表现出卓越的熟练程度，但进一步的研究对于提高其在化学科学中的安全性和实用性至关重要。我们的研究结果还表明需要对化学课程进行调整，并强调继续开发评估框架以提高安全和有用的法学硕士的重要性。]]></description>
      <guid>https://arxiv.org/abs/2404.01475</guid>
      <pubDate>Wed, 03 Apr 2024 02:26:40 GMT</pubDate>
    </item>
    <item>
      <title>通过偏好树提升法学硕士推理通才</title>
      <link>https://arxiv.org/abs/2404.02078</link>
      <description><![CDATA[我们引入了 Eurus，这是一套针对推理而优化的大型语言模型 (LLM)。经过 Mistral-7B 和 CodeLlama-70B 的微调，Eurus 模型在涵盖数学、代码生成和逻辑推理问题的各种基准测试中实现了开源模型中最先进的结果。值得注意的是，Eurus-70B 通过涵盖 5 项任务的 12 项测试的综合基准测试，在推理方面击败了 GPT-3.5 Turbo，并在 LeetCode 和 TheoremQA 这两个具有挑战性的基准测试中分别达到了 33.3% 的 pass@1 准确率和 32.6% 的准确率，大大优于现有的开源软件模型的利润率超过13.3%。 Eurus 的强劲性能主要归功于 UltraInteract，这是我们新策划的大规模、高质量的对齐数据集，专为复杂的推理任务而设计。 UltraInteract 可用于监督微调和偏好学习。对于每条指令，它都包含一个偏好树，其中包含（1）统一格式的具有多种规划策略的推理链，（2）与环境和批评的多轮交互轨迹，以及（3）促进偏好学习的成对数据。 UltraInteract使我们能够对推理任务的偏好学习进行深入探索。我们的调查表明，与一般对话中的有效性相比，一些成熟的偏好学习算法可能不太适合推理任务。受此启发，我们得出了一个新颖的奖励建模目标，它与 UltraInteract 一起形成了一个强大的奖励模型。]]></description>
      <guid>https://arxiv.org/abs/2404.02078</guid>
      <pubDate>Wed, 03 Apr 2024 02:22:53 GMT</pubDate>
    </item>
    <item>
      <title>LLaVA-Gemma：使用紧凑语言模型加速多模态基础模型</title>
      <link>https://arxiv.org/abs/2404.01331</link>
      <description><![CDATA[我们使用流行的 LLaVA 框架和最近发布的 Gemma 系列大语言模型 (LLM) 来训练一套多模态基础模型 (MMFM)。特别令人感兴趣的是 2B 参数 Gemma 模型，它提供了构建小型 MMFM 的机会。根据该领域其他论文的发现，我们测试了消除三个设计特征的效果：预训练连接器、利用更强大的图像主干以及增加语言主干的大小。由此产生的模型（我们称之为 LLaVA-Gemma）在一系列评估中表现出中等的性能，但未能超越当前同等大小的 SOTA 模型。对性能的更仔细分析表明效果参差不齐；跳过预训练往往会降低性能，较大的视觉模型有时会提高性能，而增加语言模型大小会产生不一致的效果。我们公开发布了 LLaVA-Gemma 模型的训练方案、代码和权重。]]></description>
      <guid>https://arxiv.org/abs/2404.01331</guid>
      <pubDate>Wed, 03 Apr 2024 02:20:09 GMT</pubDate>
    </item>
    <item>
      <title>长情境法学硕士在长时间的情境学习中苦苦挣扎</title>
      <link>https://arxiv.org/abs/2404.02060</link>
      <description><![CDATA[大型语言模型 (LLM) 在处理超过 32K 标记的长序列方面取得了重大进展。然而，他们的绩效评估很大程度上局限于复杂度和综合任务等指标，这些指标可能无法完全捕捉他们在更细致的现实场景中的能力。这项研究引入了一个专门的基准（LIConBench），专注于极端标签分类领域内的长期上下文学习。我们精心选择了 6 个数据集，其标签范围涵盖 28 到 174 个类别，涵盖从 2K 到 50K 的不同输入（少量演示）长度。我们的基准要求法学硕士理解整个输入，识别大量标签空间，从而做出正确的预测。我们根据基准评估了 13 个长背景法学硕士。我们发现长上下文 LLM 在 20K 的 token 长度下表现相对较好，并且性能受益于利用长上下文窗口。然而，上下文窗口超过 20K 后，除 GPT-4 之外的大多数 LLM 都会急剧下降。这表明当前法学硕士在处理和理解长的、上下文丰富的序列方面的能力存在显着差距。进一步的分析揭示了模型倾向于预测序列末尾出现的标签的趋势。他们对长序列中的多个片段进行推理的能力还有待提高。我们的研究表明，对于现有的法学硕士来说，长上下文理解和推理仍然是一项具有挑战性的任务。我们相信 LIConBench 可以为未来的长期背景法学硕士提供更现实的评估。]]></description>
      <guid>https://arxiv.org/abs/2404.02060</guid>
      <pubDate>Wed, 03 Apr 2024 02:15:18 GMT</pubDate>
    </item>
    </channel>
</rss>