<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>拥抱日报</title>
    <link>https://huggingface.co/papers</link>
    <description>Huggingface Daily Papers - 由 RSSHub 用爱制作（https://github.com/DIYgod/RSSHub）</description>
    <lastBuildDate>Thu, 25 Apr 2024 18:35:55 GMT</lastBuildDate>
    <item>
      <title>BASS：批量注意力优化推测采样</title>
      <link>https://arxiv.org/abs/2404.15778</link>
      <description><![CDATA[推测性解码已成为改善托管大型语言模型的延迟和吞吐量的强大方法。然而，大多数现有的实现都集中于生成单个序列。现实世界的生成式人工智能应用程序通常需要多个响应，如何在批量设置中执行推测性解码，同时保留其延迟优势带来了不小的挑战。本文描述了一种批量推测解码系统，该系统在多序列生成延迟方面设定了新的技术水平，并展示了卓越的 GPU 利用率以及时间预算内的生成质量。例如，对于单个 A100 GPU 上的 7.8B 大小的模型，批量大小为 8，每个序列的生成速度平均为每个令牌 5.8 毫秒，总体吞吐量为每秒 1.1K 个令牌。这些结果代表了最先进的延迟，并且比优化的常规解码速度提高了 2.15 倍。在常规解码无法完成的时间预算内，我们的系统能够生成 HumanEval Pass@First 为 43% 和 Pass@All 为 61% 的序列，远远超过单序列推测解码的可行性。我们在解码过程中的峰值 GPU 利用率高达 15.8%，是常规解码最高利用率的 3 倍以上，是单序列推测解码最高利用率的 10 倍左右。]]></description>
      <guid>https://arxiv.org/abs/2404.15778</guid>
      <pubDate>Thu, 25 Apr 2024 17:30:25 GMT</pubDate>
    </item>
    <item>
      <title>MotionMaster：用于视频生成的免训练相机运动传输</title>
      <link>https://arxiv.org/abs/2404.15789</link>
      <description><![CDATA[扩散模型的出现极大地推动了图像和视频生成的进步。最近，在可控视频生成方面做出了一些努力，包括文本到视频生成和视频运动控制，其中相机运动控制是一个重要的课题。然而，现有的相机运动控制方法依赖于训练时间相机模块，并且由于视频生成模型中的大量参数而需要大量的计算资源。此外，现有方法在训练期间预先定义相机运动类型，这限制了它们在相机控制方面的灵活性。因此，为了降低训练成本并实现灵活的摄像机控制，我们提出了 COMD，一种新颖的免训练视频运动传输模型，它将源视频中的摄像机运动和物体运动分开，并将提取的摄像机运动传输到新视频。我们首先提出了一种一次性摄像机运动解缠方法，从单个源视频中提取摄像机运动，该方法将运动对象与背景分离，并通过求解泊松方程，根据背景中的运动来估计运动对象区域中的摄像机运动。方程。此外，我们提出了一种少镜头相机运动解缠结方法，从具有相似相机运动的多个视频中提取共同的相机运动，该方法采用基于窗口的聚类技术来提取多个视频的时间注意力图中的共同特征。最后，我们提出了一种运动组合方法，将不同类型的相机运动组合在一起，使我们的模型更加可控和灵活的相机控制。大量的实验表明，我们的免训练方法可以有效地解耦相机与物体的运动，并将解耦的相机运动应用于广泛的可控视频生成任务，实现灵活多样的相机运动控制。]]></description>
      <guid>https://arxiv.org/abs/2404.15789</guid>
      <pubDate>Thu, 25 Apr 2024 15:28:40 GMT</pubDate>
    </item>
    <item>
      <title>PuLID：通过对比对齐进行 Pure 和 Lightning ID 定制</title>
      <link>https://arxiv.org/abs/2404.16022</link>
      <description><![CDATA[我们提出了 Pure 和 Lightning ID 定制（PuLID），这是一种用于文本到图像生成的新颖的免调整 ID 定制方法。通过将 Lightning T2I 分支与标准扩散分支相结合，PuLID 引入了对比对齐损失和精确 ID 损失，最大限度地减少对原始模型的破坏并确保高 ID 保真度。实验表明，PuLID 在 ID 保真度和可编辑性方面均取得了优异的性能。 PuLID 的另一个吸引人的特性是 ID 插入前后的图像元素（例如背景、灯光、构图和风格）尽可能保持一致。代码和模型将在 https://github.com/ToTheBeginning/PuLID 上提供]]></description>
      <guid>https://arxiv.org/abs/2404.16022</guid>
      <pubDate>Thu, 25 Apr 2024 15:08:29 GMT</pubDate>
    </item>
    <item>
      <title>ID-Aligner：通过奖励反馈学习增强保留身份的文本到图像的生成</title>
      <link>https://arxiv.org/abs/2404.15449</link>
      <description><![CDATA[扩散模型的快速发展引发了多样化的应用。身份保留文本到图像生成（ID-T2I）因其广泛的应用场景（例如人工智能肖像和广告）而受到广泛关注。虽然现有的 ID-T2I 方法已经展示了令人印象深刻的结果，但仍然存在一些关键挑战：（1）很难准确地保持参考肖像的身份特征，（2）生成的图像缺乏审美吸引力，特别是在执行身份保留时，以及（3） ）存在无法同时兼容基于LoRA和基于Adapter的方法的限制。为了解决这些问题，我们提出了 ID-Aligner，这是一个用于增强 ID-T2I 性能的通用反馈学习框架。为了解决身份特征丢失的问题，我们引入了身份一致性奖励微调，以利用人脸检测和识别模型的反馈来改善生成的身份保留。此外，我们提出身份美学奖励微调，利用人类注释的偏好数据的奖励和自动构建的角色结构生成反馈来提供美学调整信号。得益于其通用反馈微调框架，我们的方法可以轻松应用于 LoRA 和 Adapter 模型，从而实现一致的性能增益。 SD1.5 和 SDXL 扩散模型的大量实验验证了我们方法的有效性。项目页面：\url{https://idaligner.github.io/}]]></description>
      <guid>https://arxiv.org/abs/2404.15449</guid>
      <pubDate>Thu, 25 Apr 2024 15:02:19 GMT</pubDate>
    </item>
    </channel>
</rss>