<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>拥抱日报</title>
    <link>https://huggingface.co/papers</link>
    <description>Huggingface Daily Papers - 由 RSSHub 用爱制作（https://github.com/DIYgod/RSSHub）</description>
    <lastBuildDate>Fri, 19 Apr 2024 03:13:00 GMT</lastBuildDate>
    <item>
      <title>MoA：个性化图像生成中主题上下文解开的混合注意力</title>
      <link>https://arxiv.org/abs/2404.11565</link>
      <description><![CDATA[我们引入了一种用于文本到图像扩散模型个性化的新架构，创造了混合注意力（MoA）。受到大型语言模型 (LLM) 中使用的专家混合机制的启发，MoA 将生成工作负载分配在两个注意力路径之间：个性化分支和非个性化先验分支。 MoA 旨在通过将注意力层固定在先前分支中来保留原始模型的先验，同时通过学习将主题嵌入到先前分支生成的布局和上下文中的个性化分支最小化地干预生成过程。一种新颖的路由机制管理跨这些分支的每一层中的像素分布，以优化个性化和通用内容创建的混合。经过训练后，MoA 可以促进创建高质量、个性化的图像，该图像具有多个主题，其构图和交互与原始模型生成的图像一样多样化。至关重要的是，MoA 增强了模型预先存在的功能与新增强的个性化干预之间的区别，从而提供了以前无法实现的更加清晰的主题上下文控制。项目页面：https://snap-research.github.io/mixture-of-attention]]></description>
      <guid>https://arxiv.org/abs/2404.11565</guid>
      <pubDate>Fri, 19 Apr 2024 02:30:36 GMT</pubDate>
    </item>
    <item>
      <title>通过想象力、探索和批评实现法学硕士的自我完善</title>
      <link>https://arxiv.org/abs/2404.12253</link>
      <description><![CDATA[尽管大型语言模型 (LLM) 在各种任务上具有令人印象深刻的能力，但它们仍然难以处理涉及复杂推理和规划的场景。最近的工作提出了先进的提示技术以及使用高质量数据进行微调以增强法学硕士推理能力的必要性。然而，这些方法本质上受到数据可用性和质量的限制。有鉴于此，自我纠正和自我学习成为可行的解决方案，采用的策略允许法学硕士改进他们的成果并从自我评估的奖励中学习。然而，法学硕士在自我完善其反应方面的有效性，特别是在复杂的推理和规划任务中，仍然值得怀疑。在本文中，我们引入了用于LLM自我改进的AlphaLLM，它将蒙特卡罗树搜索（MCTS）与LLM集成，建立自我改进循环，从而在无需额外注释的情况下增强LLM的能力。 AlphaLLM 从 AlphaGo 的成功中汲取灵感，解决了将 MCTS 与 LLM 相结合以实现自我提升的独特挑战，包括数据稀缺、语言任务的巨大搜索空间以及语言任务中反馈的主观性。 AlphaLLM 由即时合成组件、专为语言任务量身定制的高效 MCTS 方法以及用于精确反馈的三个批评模型组成。我们在数学推理任务中的实验结果表明，AlphaLLM 在无需额外注释的情况下显着提高了法学硕士的性能，显示了法学硕士自我提升的潜力。]]></description>
      <guid>https://arxiv.org/abs/2404.12253</guid>
      <pubDate>Fri, 19 Apr 2024 01:59:44 GMT</pubDate>
    </item>
    <item>
      <title>推出 MLCommons 的 AI 安全基准 v0.5</title>
      <link>https://arxiv.org/abs/2404.12241</link>
      <description><![CDATA[本文介绍了由 MLCommons AI 安全工作组创建的 AI 安全基准 v0.5。人工智能安全基准旨在评估使用聊天调整语言模型的人工智能系统的安全风险。我们引入了一种原则性方法来指定和构建基准，对于 v0.5，该基准仅涵盖单个用例（成人用英语与通用助理聊天）和一组有限的角色（即典型用户、恶意用户）用户和易受攻击的用户）。我们创建了 13 个危险类别的新分类法，其中 7 个在 v0.5 基准测试中进行了测试。我们计划在 2024 年底之前发布 AI 安全基准 1.0 版本。v1.0 基准将为 AI 系统的安全性提供有意义的见解。然而，v0.5基准不应该用来评估人工智能系统的安全性。我们力求完整记录 v0.5 的限制、缺陷和挑战。此版本的 AI 安全基准 v0.5 包括 (1) 指定和构建基准的原则性方法，其中包括用例、被测系统 (SUT) 类型、语言和上下文、角色、测试和测试项目; (2) 13 个危险类别的分类法及其定义和子类别； (3) 对七个危险类别进行测试，每个危险类别包含一组独特的测试项目，即提示。总共有43,090个测试项，是我们用模板创建的； （4）针对基准的人工智能系统评分系统； (5) 一个名为 ModelBench 的开放平台和可下载工具，可用于在基准上评估人工智能系统的安全性； (6) 评估报告示例，对十多个公开可用的聊天调整语言模型的性能进行基准测试； (7) 基准测试规范。]]></description>
      <guid>https://arxiv.org/abs/2404.12241</guid>
      <pubDate>Fri, 19 Apr 2024 01:54:29 GMT</pubDate>
    </item>
    <item>
      <title>TriForce：使用分层推测解码对长序列生成进行无损加速</title>
      <link>https://arxiv.org/abs/2404.11912</link>
      <description><![CDATA[近年来，随着大型语言模型（LLM）在长内容生成中的广泛部署，对高效长序列推理支持的需求日益增长。然而，存储键值（KV）缓存以避免重新计算，其大小随着序列长度线性增长，已成为关键瓶颈。由于LLM的自回归性质，将为每个生成的令牌加载整个KV缓存，导致计算核心利用率低和延迟高。虽然已经提出了各种 KV 缓存压缩方法来缓解这个问题，但它们的生成质量会下降。我们引入了 TriForce，这是一种分层推测解码系统，可扩展至长序列生成。这种方法通过检索利用原始模型权重和动态稀疏 KV 缓存作为草稿模型，充当层次结构中的中间层，并由较小的模型进一步推测以减少其草稿延迟。 TriForce 不仅为 Llama2-7B-128K 带来令人印象深刻的加速，在 A100 GPU 上实现高达 2.31 倍，而且还展示了处理更长上下文的可扩展性。对于两个 RTX 4090 GPU 上的卸载设置，TriForce 实现了 0.108s/tokenx2014，仅比 A100 上的自回归基线慢一半，在我们优化的卸载系统上达到了 7.78 倍。此外，TriForce 在单个 RTX 4090 GPU 上的性能是 DeepSpeed-Zero-Inference 的 4.86 倍。 TriForce 的坚固性因其在各种温度下始终如一的出色性能而得以凸显。该代码可在 https://github.com/Infini-AI-Lab/TriForce 获取。]]></description>
      <guid>https://arxiv.org/abs/2404.11912</guid>
      <pubDate>Fri, 19 Apr 2024 01:37:40 GMT</pubDate>
    </item>
    <item>
      <title>OpenBezoar：小型、经济高效且开放的模型，基于混合指令数据进行训练</title>
      <link>https://arxiv.org/abs/2404.12195</link>
      <description><![CDATA[针对不同下游任务对预训练的法学硕士进行指令微调已取得了显着的成功，并引起了学者和从业者的兴趣。为了确保这种经过微调的法学硕士符合人类的偏好，诸如 RLHF 和 DPO 等技术应运而生。与此同时，人们对模型的较小参数数量越来越感兴趣。在这项工作中，我们使用 OpenLLaMA 3Bv2 作为基本模型，描述了用于微调 OpenBezoar 系列模型的方法。在本秘籍中：我们首先使用开放且商业上非限制性的 Falcon-40B 模型指令微调变体在基于 LaMini-LM、WizardLM/Evol-Instruct（带有 databricks）的三种方案下生成合成指令微调数据-dolly-15k 作为种子数据集）和 Orca（以 Flan Collection 作为种子数据集），然后使用 GPT-4 作为人类代理来过滤这些代。然后，我们对每个方案依次执行经济有效的基于 QLoRA 的监督微调。在使用 DPO 损失获得最终检查点之前，使用 HH-RLHF 数据集的子集进一步微调生成的检查点，以最小化分布偏移。使用 LM Eval Harness 任务/指标以及使用 Claude 2.1 的“LLM-as-a-judge”框架在 MT-Bench 上完成评估，结果发现最终检查点“OpenBezoar-HH-RLHF-DPO” ”，在 3B 参数尺度上表现出了优于许多模型的性能，甚至超越了 Huggingface Open LLM 排行榜的某一类别中的顶级模型。我们发布了“OpenBezoar-SFT”、“OpenBezoar-HH-RLHF-SFT”、“OpenBezoar-HH-RLHF-DPO”检查点，以及我们在 HuggingFace 上生成的数据集：https://huggingface.co/collections/Sur​​geGlobal/open- bezoar-6620a24923e12127e9e2b9cc 和我们的代码库 https://bitbucket.org/paladinanalytics/workspace/projects/OP。]]></description>
      <guid>https://arxiv.org/abs/2404.12195</guid>
      <pubDate>Fri, 19 Apr 2024 01:22:28 GMT</pubDate>
    </item>
    <item>
      <title>BLINK：多模态大型语言模型可以看到但无法感知</title>
      <link>https://arxiv.org/abs/2404.12390</link>
      <description><![CDATA[我们推出了 Blink，这是多模式语言模型 (LLM) 的新基准，专注于其他评估中未发现的核心视觉感知能力。大多数 Blink 任务都可以由人类“眨眼间”解决（例如，相对深度估计、视觉对应、取证检测和多视图推理）。然而，我们发现这些需要感知的任务给当前的多模式法学硕士带来了重大挑战，因为它们抵制通过自然语言进行调解。 Blink 将 14 个经典计算机视觉任务重新格式化为 3,807 个多项选择题，并配有单个或多个图像和视觉提示。虽然人类的平均准确率达到 95.70%，但 Blink 对于现有的多模态 LLM 来说却具有惊人的挑战性：即使是表现最好的 GPT-4V 和 Gemini 也能达到 51.26% 和 45.72% 的准确率，仅比随机猜测高 13.17% 和 7.63%，这表明在最近的多模式法学硕士中，这种感知能力尚未“出现”。我们的分析还强调，专业的 CV 模型可以更好地解决这些问题，并提出未来改进的潜在途径。我们相信 Blink 将激励社区帮助多模式法学硕士赶上人类水平的视觉感知。]]></description>
      <guid>https://arxiv.org/abs/2404.12390</guid>
      <pubDate>Fri, 19 Apr 2024 01:11:42 GMT</pubDate>
    </item>
    <item>
      <title>Reka Core、Flash 和 Edge：一系列强大的多模式语言模型</title>
      <link>https://arxiv.org/abs/2404.12387</link>
      <description><![CDATA[我们介绍 Reka Core、Flash 和 Edge，这是 Reka 从头开始​​训练的一系列强大的多模态语言模型。 Reka 模型能够对文本、图像、视频和音频输入进行处理和推理。该技术报告讨论了其中一些模型的训练细节，并提供了综合评估结果。我们证明 Reka Edge 和 Reka Flash 不仅是最先进的，而且还优于许多更大的模型，为各自的计算类别提供了巨​​大的价值。与此同时，我们最强大、最大的模型 Reka Core 在自动评估和盲人评估方面都接近最佳前沿模型。在图像问答基准（例如 MMMU、VQAv2）上，Core 的表现与 GPT4-V 相当。同时，在多模态聊天中，Core 在第三方盲人评估设置下排名第二，优于 Claude 3 Opus 等其他模型。在文本基准测试中，Core 不仅在一组完善的基准测试（例如 MMLU、GSM8K）上与其他前沿模型相比具有竞争力，而且在人类评估方面也优于 GPT4-0613。在视频问答（感知测试）方面，Core 优于 Gemini Ultra。模型在生产中发货：http://chat.reka.ai。还可以在 http://showcase.reka.ai 上找到非精选定性示例的展示。]]></description>
      <guid>https://arxiv.org/abs/2404.12387</guid>
      <pubDate>Fri, 19 Apr 2024 01:07:46 GMT</pubDate>
    </item>
    <item>
      <title>AniClipart：具有文本到视频先验的剪贴画动画</title>
      <link>https://arxiv.org/abs/2404.12347</link>
      <description><![CDATA[剪贴画是一种预制的图形艺术形式，提供了一种方便有效的方式来说明视觉内容。将静态剪贴画图像转换为运动序列的传统工作流程既费力又耗时，涉及许多复杂的步骤，如绑定、关键动画和中间处理。文本到视频生成的最新进展在解决这个问题方面具有巨大的潜力。然而，直接应用文本到视频生成模型通常很难保留剪贴画图像的视觉特征或生成卡通风格的动作，导致动画结果不令人满意。在本文中，我们介绍了 AniClipart，这是一个将静态剪贴画图像转换为由文本到视频先验引导的高质量运动序列的系统。为了生成卡通风格且平滑的运动，我们首先定义剪贴画图像关键点上的 B\&#39;{e}zier 曲线作为运动正则化的一种形式。然后，我们通过优化视频分数蒸馏采样（VSDS）损失，将关键点的运动轨迹与提供的文本提示对齐，该损失在预训练的文本到视频扩散模型中编码了足够的自然运动知识。通过可微分的 As-Rigid-As-Possible 形状变形算法，我们的方法可以在保持变形刚度的同时进行端到端优化。实验结果表明，所提出的 AniClipart 在文本视频对齐、视觉身份保留和运动一致性方面始终优于现有的图像到视频生成模型。此外，我们还展示了 AniClipart 的多功能性，使其能够生成更广泛的动画格式，例如允许拓扑变化的分层动画。]]></description>
      <guid>https://arxiv.org/abs/2404.12347</guid>
      <pubDate>Fri, 19 Apr 2024 01:03:39 GMT</pubDate>
    </item>
    </channel>
</rss>