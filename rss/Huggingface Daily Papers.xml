<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>拥抱日报</title>
    <link>https://huggingface.co/papers</link>
    <description>Huggingface Daily Papers - 由 RSSHub 用爱制作（https://github.com/DIYgod/RSSHub）</description>
    <lastBuildDate>Sun, 24 Mar 2024 11:35:35 GMT</lastBuildDate>
    <item>
      <title>GRM：用于高效 3D 重建和生成的大型高斯重建模型</title>
      <link>https://arxiv.org/abs/2403.14621</link>
      <description><![CDATA[我们引入了 GRM，一种大型重建器，能够在 0.1 秒左右的时间内从稀疏视图图像中恢复 3D 资源。 GRM 是一种基于前馈变压器的模型，它有效地结合多视图信息，将输入像素转换为像素对齐的高斯分布，这些高斯分布未经投影，以创建一组代表场景的密集分布的 3D 高斯分布。我们的 Transformer 架构和 3D Gaussians 的使用共同解锁了一个可扩展且高效的重建框架。大量的实验结果证明了我们的方法在重建质量和效率方面优于其他方法。我们还通过将 GRM 与现有的多视图扩散模型集成，展示了 GRM 在生成任务（即文本到 3D 和图像到 3D）中的潜力。我们的项目网站位于：https://justimyhxu.github.io/projects/grm/。]]></description>
      <guid>https://arxiv.org/abs/2403.14621</guid>
      <pubDate>Fri, 22 Mar 2024 05:11:19 GMT</pubDate>
    </item>
    <item>
      <title>ReNoise：通过迭代噪声进行真实图像反转</title>
      <link>https://arxiv.org/abs/2403.14602</link>
      <description><![CDATA[文本引导扩散模型的最新进展释放了强大的图像处理功能。然而，将这些方法应用于真实图像需要将图像反演到预训练扩散模型的域中。实现忠实的反演仍然是一个挑战，特别是对于经过训练以通过少量去噪步骤生成图像的最新模型而言。在这项工作中，我们引入了一种具有高质量操作比的反演方法，在不增加操作数量的情况下提高了重建精度。基于反转扩散采样过程，我们的方法在每个反演采样步骤中采用迭代去噪机制。该机制通过迭代应用预训练的扩散模型并对这些预测进行平均，细化了沿前向扩散轨迹的预测点的近似值。我们使用各种采样算法和模型（包括最近的加速扩散模型）评估 ReNoise 技术的性能。通过综合评估和比较，我们展示了其在准确性和速度方面的有效性。此外，我们通过在真实图像上演示文本驱动的图像编辑来确认我们的方法保留了可编辑性。]]></description>
      <guid>https://arxiv.org/abs/2403.14602</guid>
      <pubDate>Fri, 22 Mar 2024 05:00:21 GMT</pubDate>
    </item>
    <item>
      <title>MyVLM：针对特定于用户的查询个性化 VLM</title>
      <link>https://arxiv.org/abs/2403.14599</link>
      <description><![CDATA[最近的大规模视觉语言模型（VLM）在理解和生成视觉内容的文本描述方面表现出了卓越的能力。然而，这些模型缺乏对用户特定概念的理解。在这项工作中，我们向 VLM 个性化迈出了第一步，使它们能够学习和推理用户提供的概念。例如，我们探索这些模型是否可以学习在图像中识别您并传达您正在做的事情，从而定制模型以反映您的个人经历和关系。为了有效地识别各种用户特定的概念，我们用外部概念头来增强 VLM，这些概念头充当模型的切换开关，使 VLM 能够识别给定图像中特定目标概念的存在。认识到这个概念后，我们学习了一个嵌入 VLM 中间特征空间的新概念。这种嵌入的任务是引导语言模型将目标概念自然地集成到其生成的响应中。我们将我们的技术应用于 BLIP-2 和 LLaVA 进行个性化图像字幕，并进一步展示其在个性化视觉问答中的适用性。我们的实验证明了我们能够泛化到所学概念的未见图像，同时保留不相关输入的模型行为。]]></description>
      <guid>https://arxiv.org/abs/2403.14599</guid>
      <pubDate>Fri, 22 Mar 2024 04:40:03 GMT</pubDate>
    </item>
    <item>
      <title>Cobra：将 Mamba 扩展到多模态大型语言模型以实现高效推理</title>
      <link>https://arxiv.org/abs/2403.14520</link>
      <description><![CDATA[近年来，多模态大语言模型（MLLM）在各个领域的应用取得了令人瞩目的成功。然而，作为许多下游任务的基础模型，当前的 MLLM 由著名的 Transformer 网络组成，其二次计算复杂度效率较低。为了提高此类基本模型的效率，我们提出了 Cobra，一种线性计算复杂度 MLLM。具体来说，Cobra 将高效的 Mamba 语言模型集成到视觉模态中。此外，我们探索和研究各种模态融合方案，以创建有效的多模态 Mamba。大量实验表明，(1) Cobra 通过当前计算高效的最先进方法（例如 LLaVA-Phi、TinyLLaVA 和 MobileVLM v2）实现了极具竞争力的性能，并且由于 Cobra 的线性顺序建模而具有更快的速度。 （2）有趣的是，封闭集挑战性预测基准的结果表明，Cobra 在克服视觉错觉和空间关系判断方面表现良好。 (3) 值得注意的是，Cobra 甚至以约 43% 的参数数量实现了与 LLaVA 相当的性能。我们将开源Cobra的所有代码，并希望所提出的方法能够促进未来MLLM复杂性问题的研究。我们的项目页面位于：https://sites.google.com/view/cobravlm。]]></description>
      <guid>https://arxiv.org/abs/2403.14520</guid>
      <pubDate>Fri, 22 Mar 2024 04:34:09 GMT</pubDate>
    </item>
    <item>
      <title>回收资源：与生成语言模型聊天</title>
      <link>https://arxiv.org/abs/2403.14467</link>
      <description><![CDATA[在客户服务、信息检索和内容生成等环境中，研究人员和开发人员越来越依赖毒性评分来调节生成语言模型的输出。然而，毒性评分可能会使相关信息难以获取、僵化或“价值锁定”文化规范，并阻碍语言回收过程，特别是对于边缘化群体而言。在这项工作中，我们将算法资源的概念扩展到生成语言模型：我们为用户提供了一种新颖的机制，通过动态设置毒性过滤的阈值来实现他们所需的预测。因此，相对于与基线系统的交互，用户可以行使更大的代理权。一项试点研究（n = 30）支持我们提出的追索机制的潜力，表明与模型输出的固定阈值毒性过滤相比，可用性有所提高。未来的工作应该探索毒性评分、模型可控性、用户代理和语言回收过程的交叉点——特别是关于许多社区在与生成语言模型交互时遇到的偏见。]]></description>
      <guid>https://arxiv.org/abs/2403.14467</guid>
      <pubDate>Fri, 22 Mar 2024 04:01:47 GMT</pubDate>
    </item>
    <item>
      <title>MathVerse：您的多模式法学硕士能否真正看到视觉数学问题中的图表？</title>
      <link>https://arxiv.org/abs/2403.14624</link>
      <description><![CDATA[多模态大语言模型（MLLM）的显着进步因其在视觉环境中的卓越性能而引起了无与伦比的关注。然而，他们解决视觉数学问题的能力仍然没有得到充分的评估和理解。我们研究了当前的基准，将过多的视觉内容纳入文本问题中，这可能有助于 MLLM 在不真正解释输入图的情况下推导出答案。为此，我们推出了 MathVerse，这是一个全方位的可视化数学基准，旨在对 MLLM 进行公平和深入的评估。我们从公开来源精心收集了 2,612 个高质量、多学科的数学问题以及图表。然后，每个问题都由人类注释者转换为六个不同的版本，每个版本以多模态方式提供不同程度的信息内容，总共贡献了 15K 个测试样本。这种方法使 MathVerse 能够全面评估 MLLM 是否能够以及在多大程度上能够真正理解数学推理的可视化图表。此外，我们提出了一种思想链（CoT）评估策略，用于对输出答案进行细粒度评估。我们没有天真地判断 True 或 False，而是采用 GPT-4(V) 自适应地提取关键推理步骤，然后通过详细的错误分析对每个步骤进行评分，这可以揭示 MLLM 的中间 CoT 推理质量。我们希望 MathVerse 基准能够提供独特的见解来指导 MLLM 的未来发展。项目页面：https://mathverse-cuhk.github.io]]></description>
      <guid>https://arxiv.org/abs/2403.14624</guid>
      <pubDate>Fri, 22 Mar 2024 03:38:21 GMT</pubDate>
    </item>
    <item>
      <title>高斯磨砂：具有实时渲染的可编辑复杂辐射场</title>
      <link>https://arxiv.org/abs/2403.14554</link>
      <description><![CDATA[我们提出了 Gaussian Frosting，这是一种新颖的基于网格的表示，用于实时高质量渲染和编辑复杂的 3D 效果。我们的方法建立在最近的 3D 高斯分布框架的基础上，该框架优化了一组 3D 高斯以近似图像的辐射场。我们建议首先在优化期间从高斯中提取基础网格，然后在网格周围构建和细化具有可变厚度的自适应高斯层，以更好地捕捉表面附近的精细细节和体积效果，例如头发或草。我们将这一层称为高斯糖霜，因为它类似于蛋糕上的糖霜涂层。材料越模糊，糖霜就越厚。我们还引入了高斯参数化，以强制它们留在磨砂层内，并在变形、重新缩放、编辑或动画网格时自动调整其参数。我们的表示允许使用高斯喷射进行高效渲染，以及通过修改基础网格进行编辑和动画。我们证明了我们的方法在各种合成和真实场景上的有效性，并表明它优于现有的基于表面的方法。我们将发布我们的代码和基于网络的查看器作为额外的贡献。我们的项目页面如下：https://anttwo.github.io/frosting/]]></description>
      <guid>https://arxiv.org/abs/2403.14554</guid>
      <pubDate>Fri, 22 Mar 2024 03:04:30 GMT</pubDate>
    </item>
    <item>
      <title>DreamReward：根据人类偏好生成文本到 3D</title>
      <link>https://arxiv.org/abs/2403.14613</link>
      <description><![CDATA[最近，通过文本提示创建 3D 内容取得了显着的成功。然而，当前的文本转 3D 方法通常会生成与人类偏好不太相符的 3D 结果。在本文中，我们提出了一个名为 DreamReward 的综合框架，用于从人类偏好反馈中学习和改进文本到 3D 模型。首先，我们根据系统注释管道（包括评级和排名）收集 25,000 个专家比较。然后，我们构建了 Reward3D——第一个通用文本到 3D 人类偏好奖励模型，以有效编码人类偏好。基于 3D 奖励模型，我们最终进行理论分析并提出 Reward3D 反馈学习 (DreamFL)，这是一种直接调整算法，可通过重新定义的评分器来优化多视图扩散模型。以理论证明和广泛的实验比较为基础，我们的 DreamReward 成功生成了高保真度和 3D 一致的结果，并显着促进了与人类意图的迅速一致。我们的结果证明了从人类反馈中学习来改进文本到 3D 模型的巨大潜力。]]></description>
      <guid>https://arxiv.org/abs/2403.14613</guid>
      <pubDate>Fri, 22 Mar 2024 02:50:05 GMT</pubDate>
    </item>
    <item>
      <title>AnyV2V：适用于任何视频到视频编辑任务的即插即用框架</title>
      <link>https://arxiv.org/abs/2403.14468</link>
      <description><![CDATA[视频到视频编辑涉及编辑源视频以及附加控件（例如文本提示、主题或样式），以生成与源视频和提供的控件一致的新视频。传统方法仅限于某些编辑类型，限制了其满足广泛用户需求的能力。在本文中，我们介绍了 AnyV2V，这是一种新颖的免训练框架，旨在将视频编辑简化为两个主要步骤：（1）采用现成的图像编辑模型（例如 InstructPix2Pix、InstantID 等）来修改第一帧，（2）利用现有的图像到视频生成模型（例如 I2VGen-XL）进行 DDIM 反演和特征注入。在第一阶段，AnyV2V可以插入任何现有的图像编辑工具来支持广泛的视频编辑任务。除了传统的基于提示的编辑方法之外，AnyV2V 还可以支持新颖的视频编辑任务，包括基于参考的风格转换、主题驱动的编辑和身份操作，这是以前的方法无法实现的。在第二阶段，AnyV2V可以插入任何现有的图像到视频模型来执行DDIM反演和中间特征注入，以保持与源视频的外观和运动一致性。在基于提示的编辑方面，我们表明 AnyV2V 在提示对齐方面比之前的最佳方法高出 35%，在人类偏好方面比之前的最佳方法高出 25%。在这三个新颖的任务上，我们表明 AnyV2V 也取得了很高的成功率。我们相信 AnyV2V 将继续蓬勃发展，因为它能够无缝集成快速发展的图像编辑方法。这种兼容性可以帮助AnyV2V增加其多功能性，以满足不同的用户需求。]]></description>
      <guid>https://arxiv.org/abs/2403.14468</guid>
      <pubDate>Fri, 22 Mar 2024 02:44:20 GMT</pubDate>
    </item>
    <item>
      <title>时间与空间的探索</title>
      <link>https://arxiv.org/abs/2403.14611</link>
      <description><![CDATA[我们引入有界生成作为一种通用任务来控制视频生成，以仅基于给定的开始和结束帧来合成任意相机和主体运动。我们的目标是充分利用图像到视频模型固有的泛化能力，而无需对原始模型进行额外的训练或微调。这是通过提出的新采样策略实现的，我们将其称为时间反转融合，该策略融合分别以开始帧和结束帧为条件的时间前向和后向去噪路径。融合路径产生的视频可以平滑地连接两个帧，从而产生忠实的主体运动的中间、静态场景的新颖视图以及当两个边界帧相同时的无缝视频循环。我们整理了图像对的多样化评估数据集，并与最接近的现有方法进行比较。我们发现时间反转融合在所有子任务上都优于相关工作，表现出生成复杂运动和由有界框架引导的 3D 一致视图的能力。请参阅项目页面 https://time-reversal.github.io。]]></description>
      <guid>https://arxiv.org/abs/2403.14611</guid>
      <pubDate>Fri, 22 Mar 2024 02:31:01 GMT</pubDate>
    </item>
    </channel>
</rss>