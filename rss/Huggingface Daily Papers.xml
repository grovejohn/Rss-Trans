<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>拥抱日报</title>
    <link>https://huggingface.co/papers</link>
    <description>Huggingface Daily Papers - 由 RSSHub 用爱制作（https://github.com/DIYgod/RSSHub）</description>
    <lastBuildDate>Wed, 27 Mar 2024 06:16:12 GMT</lastBuildDate>
    <item>
      <title>用于几何精确辐射场的二维高斯溅射</title>
      <link>https://arxiv.org/abs/2403.17888</link>
      <description><![CDATA[3D 高斯溅射 (3DGS) 最近彻底改变了辐射场重建，无需烘焙即可实现高质量的新颖视图合成和快速渲染速度。然而，由于 3D 高斯的多视图不一致性质，3DGS 无法准确表示表面。我们提出了 2D 高斯分布 (2DGS)，这是一种从多视图图像中建模和重建几何精确辐射场的新方法。我们的关键思想是将 3D 体积折叠成一组 2D 定向平面高斯圆盘。与 3D 高斯不同，2D 高斯在本质上对表面进行建模的同时提供视图一致的几何形状。为了准确地恢复薄表面并实现稳定的优化，我们引入了利用射线溅射相交和光栅化的透视精确的 2D 溅射过程。此外，我们还结合了深度失真和正态一致性项，以进一步提高重建的质量。我们证明，我们的可微渲染器允许无噪声和详细的几何重建，同时保持有竞争力的外观质量、快速训练速度和实时渲染。我们的代码将公开。]]></description>
      <guid>https://arxiv.org/abs/2403.17888</guid>
      <pubDate>Wed, 27 Mar 2024 04:09:54 GMT</pubDate>
    </item>
    <item>
      <title>Octree-GS：利用 LOD 结构的 3D 高斯实现一致的实时渲染</title>
      <link>https://arxiv.org/abs/2403.17898</link>
      <description><![CDATA[与基于 NeRF 的神经场景表示相比，最近的 3D 高斯泼溅 (3D-GS) 显示出卓越的渲染保真度和效率。在展示实时渲染潜力的同时，3D-GS 在具有复杂细节的大型场景中遇到了渲染瓶颈，因为视锥体中存在过多的高斯图元。这种限制在缩小视图中尤其明显，并且可能导致细节变化的场景中渲染速度不一致。此外，它经常难以通过其启发式密度控制操作来捕获不同尺度的相应细节级别。受细节层次 (LOD) 技术的启发，我们引入了 Octree-GS，它采用 LOD 结构的 3D 高斯方法，支持场景表示的细节层次分解，从而有助于最终渲染结果。我们的模型从多分辨率锚点集中动态选择适当的级别，通过自适应 LOD 调整确保一致的渲染性能，同时保持高保真渲染结果。]]></description>
      <guid>https://arxiv.org/abs/2403.17898</guid>
      <pubDate>Wed, 27 Mar 2024 03:58:18 GMT</pubDate>
    </item>
    <item>
      <title>DreamPolisher：通过几何扩散实现高质量文本到 3D 生成</title>
      <link>https://arxiv.org/abs/2403.17237</link>
      <description><![CDATA[我们提出了 DreamPolisher，一种新颖的基于高斯泼溅的方法，具有几何指导，专门用于从文本描述中学习跨视图一致性和复杂的细节。虽然文本到 3D 生成方法的最新进展令人鼓舞，但流行的方法往往无法确保视图一致性和纹理丰富性。对于仅使用文本输入的方法来说，这个问题变得尤其明显。为了解决这个问题，我们提出了一种基于两阶段高斯泼溅的方法，该方法强制视图之间的几何一致性。最初，粗略的 3D 生成通过几何优化进行细化。随后，我们使用 ControlNet 驱动的细化器以及几何一致性项来提高生成的 3D 资源的纹理保真度和整体一致性。对涵盖各种对象类别的不同文本提示的实证评估证明了 DreamPolisher 在生成一致且逼真的 3D 对象方面的功效，与文本指令的语义紧密结合。]]></description>
      <guid>https://arxiv.org/abs/2403.17237</guid>
      <pubDate>Wed, 27 Mar 2024 03:53:19 GMT</pubDate>
    </item>
    <item>
      <title>实习生LM2技术报告</title>
      <link>https://arxiv.org/abs/2403.17297</link>
      <description><![CDATA[ChatGPT 和 GPT-4 等大型语言模型 (LLM) 的发展引发了关于通用人工智能 (AGI) 出现的讨论。然而，在开源模型中复制这些进步一直具有挑战性。本文介绍了 InternLM2，这是一个开源法学硕士，通过创新的预训练和优化技术，它在 6 个维度和 30 个基准的综合评估、长上下文建模和开放式主观评估方面优于其前辈。 InternLM2的预训练过程非常细致，突出了文本、代码、长上下文数据等多种数据类型的准备。 InternLM2 有效地捕获了长期依赖关系，最初在 4k token 上进行训练，然后在预训练和微调阶段升级到 32k token，在 200k 的“大海捞针”测试中表现出了出色的性能。InternLM2 进一步对齐使用监督微调 (SFT) 和新颖的人类反馈条件在线强化学习 (COOL RLHF) 策略来解决人类偏好冲突和奖励黑客行为。通过发布不同训练阶段和模型大小的 InternLM2 模型，我们为社区提供了见解进入模型的演变。]]></description>
      <guid>https://arxiv.org/abs/2403.17297</guid>
      <pubDate>Wed, 27 Mar 2024 03:46:57 GMT</pubDate>
    </item>
    <item>
      <title>通过自动提示优化提高文本到图像的一致性</title>
      <link>https://arxiv.org/abs/2403.17804</link>
      <description><![CDATA[文本到图像（T2I）生成模型取得了令人印象深刻的进步，产生了大量高性能模型，这些模型能够生成美观、逼真的图像。尽管取得了进展，这些模型仍然难以生成与输入提示一致的图像，常常无法正确捕获对象的数量、关系和属性。现有的提高提示图像一致性的解决方案面临以下挑战：（1）它们经常需要模型微调，（2）它们只关注附近的提示样本，（3）它们受到图像之间不利权衡的影响质量、表现多样性和提示图像一致性。在本文中，我们解决了这些挑战，并介绍了一种 T2I 提示优化框架 OPT2I，该框架利用大型语言模型 (LLM) 来提高 T2I 模型中提示图像的一致性。我们的框架从用户提示开始，迭代生成修改后的提示，目标是最大化一致性分数。我们对 MSCOCO 和 PartiPrompts 这两个数据集进行的广泛验证表明，OPT2I 可以将 DSG 分数的初始一致性分数提高高达 24.9%，同时保留 FID 并提高生成数据和真实数据之间的召回率。我们的工作为利用法学硕士的力量构建更可靠、更强大的 T2I 系统铺平了道路。]]></description>
      <guid>https://arxiv.org/abs/2403.17804</guid>
      <pubDate>Wed, 27 Mar 2024 03:39:46 GMT</pubDate>
    </item>
    <item>
      <title>英特尔数据中心 GPU 上完全融合的多层感知器</title>
      <link>https://arxiv.org/abs/2403.17607</link>
      <description><![CDATA[本文介绍了多层感知器 (MLP) 的 SYCL 实现，该实现以英特尔数据中心 GPU Max 1550 为目标并进行了优化。为了提高性能，我们的实现通过最大化通用内存中的数据重用来最小化缓慢的全局内存访问。通过融合 MLP 各层中的操作来实现寄存器文件和共享本地内存。我们通过一个简单的屋顶线模型表明，这会导致算术强度显着增加，从而提高性能，尤其是推理方面。我们将我们的方法与 MLP 的类似 CUDA 实现进行比较，结果表明，我们在 Intel 数据中心 GPU 上的实现在推理方面优于 Nvidia H100 GPU 上的 CUDA 实现，高出 2.84 倍，在训练中高出 1.75 倍。该论文还展示了我们的 SYCL 在三个重要领域实施的效率：图像压缩、神经辐射场和物理信息机器学习。在所有情况下，我们的实现比相同 Intel GPU 上现成的 Intel Extension for PyTorch (IPEX) 实现的性能高出 30 倍，比 Nvidia H100 GPU 上的 CUDA PyTorch 版本高出 19 倍。代码可以在 https://github.com/intel/tiny-dpcpp-nn 找到。]]></description>
      <guid>https://arxiv.org/abs/2403.17607</guid>
      <pubDate>Wed, 27 Mar 2024 03:35:09 GMT</pubDate>
    </item>
    <item>
      <title>更深层次的不合理无效</title>
      <link>https://arxiv.org/abs/2403.17887</link>
      <description><![CDATA[我们根据经验研究了一种针对流行的开放权重预训练 LLM 系列的简单层修剪策略，发现在删除大部分（最多一半）层之前，不同问答基准上的性能下降最小。为了修剪这些模型，我们通过考虑层之间的相似性来确定要修剪的最佳层块；然后，为了“治愈”损坏，我们进行少量微调。特别是，我们使用参数高效微调 (PEFT) 方法，特别是量化和低阶适配器 (QLoRA)，这样我们的每个实验都可以在单个 A100 GPU 上执行。从实践的角度来看，这些结果表明层剪枝方法可以补充其他 PEFT 策略，一方面进一步减少微调的计算资源，另一方面可以改善推理的内存和延迟。从科学的角度来看，这些 LLM 对删除层的鲁棒性意味着当前的预训练方法没有正确利用网络深层中的参数，或者浅层在存储知识方面发挥着关键作用。]]></description>
      <guid>https://arxiv.org/abs/2403.17887</guid>
      <pubDate>Wed, 27 Mar 2024 03:28:32 GMT</pubDate>
    </item>
    <item>
      <title>TC4D：轨迹条件文本到 4D 生成</title>
      <link>https://arxiv.org/abs/2403.17920</link>
      <description><![CDATA[最近的文本到 4D 生成技术使用预先训练的文本到视频模型的监督来合成动态 3D 场景。然而，现有的运动表示，例如变形模型或时间相关的神经表示，它们可以生成的运动量受到限制——它们无法合成远远超出用于体渲染的边界框的运动。缺乏更灵活的运动模型导致了 4D 生成方法与最近的近乎真实感视频生成模型之间的真实感差距。在这里，我们提出 TC4D：轨迹条件文本到 4D 生成，它将运动分解为全局和局部组件。我们使用沿样条参数化轨迹的刚性变换来表示场景边界框的全局运动。我们使用文本到视频模型的监督来学习符合全局轨迹的局部变形。我们的方法能够合成沿任意轨迹动画的场景、合成场景生成，并显着改进生成运动的真实性和数量，我们通过用户研究对其进行定性评估。视频结果可以在我们的网站上查看：https://sherwinbahmani.github.io/tc4d。]]></description>
      <guid>https://arxiv.org/abs/2403.17920</guid>
      <pubDate>Wed, 27 Mar 2024 03:20:55 GMT</pubDate>
    </item>
    </channel>
</rss>