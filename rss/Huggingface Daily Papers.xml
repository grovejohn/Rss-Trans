<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>拥抱日报</title>
    <link>https://huggingface.co/papers</link>
    <description>Huggingface Daily Papers - RSSHub 用爱制作的(https://github.com/DIYgod/RSSHub)</description>
    <lastBuildDate>Tue, 19 Mar 2024 11:11:10 GMT</lastBuildDate>
    <item>
      <title>PERL：根据人类反馈进行参数高效强化学习</title>
      <link>https://arxiv.org/abs/2403.10704</link>
      <description><![CDATA[人类反馈强化学习 (RLHF) 已被证明是一种使预训练大型语言模型 (LLM) 与人类偏好保持一致的强大方法。但使用 RLHF 训练模型的计算成本很高，而且整个过程很复杂。在这项工作中，我们研究了 RLHF，其中底层模型使用 Hu 等人引入的低秩适应（LoRA）参数有效方法进行训练。 [2021]。我们研究了“参数高效强化学习”(PERL) 的设置，其中我们使用 LoRA 执行奖励模型训练和强化学习。我们将 PERL 与传统的微调（全面调整）在 7 个基准的各种配置上进行比较，其中包括奖励建模和强化学习的 2 个新颖数据集。我们发现 PERL 的性能与传统的 RLHF 设置相当，同时训练速度更快，内存更少。这实现了 RLHF 的高性能，同时减少了限制其作为大型语言模型对齐技术采用的计算负担。我们还发布了 2 个新颖的点赞/反对偏好数据集：“Taskmaster Coffee”和“Taskmaster Ticketing”，以促进围绕 RLHF 的研究。]]></description>
      <guid>https://arxiv.org/abs/2403.10704</guid>
      <pubDate>Tue, 19 Mar 2024 05:05:26 GMT</pubDate>
    </item>
    <item>
      <title>DiPaCo：分布式路径组合</title>
      <link>https://arxiv.org/abs/2403.10616</link>
      <description><![CDATA[扩展神经网络模型推动了机器学习 (ML) 的进步。这种扩展是通过更加英勇的工程壮举实现的，这对于适应需要并行工作的设备之间的高带宽通信的机器学习方法是必要的。在这项工作中，我们提出了一种针对 ML 模型的共同设计的模块化架构和训练方法，称为分布式 PAth COmposition (DiPaCo)。在训练期间，DiPaCo 通过一组共享模块按路径分配计算。结合受本地 SGD 启发的优化 (DiLoCo)，使模块保持同步并大幅减少通信，我们的方法有助于对连接不良和异构工作人员进行培训，其设计可确保对工作人员故障和抢占的鲁棒性。在推理时，每个输入只需执行一条路径，无需任何模型压缩。我们认为这种方法是迈向大规模学习新范式的第一个原型，这种范式不太同步，而且更加模块化。我们在广泛使用的 C4 基准测试上进行的实验表明，对于相同数量的训练步骤但更少的挂钟时间，DiPaCo 通过选择 256 条可能路径之一（每条路径都有一个1.5亿个参数的大小。]]></description>
      <guid>https://arxiv.org/abs/2403.10616</guid>
      <pubDate>Tue, 19 Mar 2024 04:57:02 GMT</pubDate>
    </item>
    <item>
      <title>VFusion3D：从视频扩散模型学习可扩展的 3D 生成模型</title>
      <link>https://arxiv.org/abs/2403.12034</link>
      <description><![CDATA[本文提出了一种利用预先训练的视频扩散模型构建可扩展 3D 生成模型的新颖范例。开发基础 3D 生成模型的主要障碍是 3D 数据的可用性有限。与图像、文本或视频不同，3D 数据不易访问且难以获取。与大量其他类型的数据相比，这导致规模上存在显着差异。为了解决这个问题，我们建议使用经过大量文本、图像和视频训练的视频扩散模型作为 3D 数据的知识源。通过微调解锁其多视图生成能力，我们生成了大规模合成多视图数据集来训练前馈 3D 生成模型。所提出的模型 VFusion3D 经过近 3M 合成多视图数据的训练，可以在几秒钟内从单个图像生成 3D 资产，与当前的 SOTA 前馈 3D 生成模型相比，具有卓越的性能，用户对我们的结果的偏好超过 70 ％ 的时间。]]></description>
      <guid>https://arxiv.org/abs/2403.12034</guid>
      <pubDate>Tue, 19 Mar 2024 04:51:53 GMT</pubDate>
    </item>
    <item>
      <title>Larimar：具有情景记忆控制的大型语言模型</title>
      <link>https://arxiv.org/abs/2403.11901</link>
      <description><![CDATA[高效、准确地更新大型语言模型 (LLM) 中存储的知识是当今最紧迫的研究挑战之一。本文介绍了 Larimar——一种新颖的、受大脑启发的架构，用于通过分布式情景记忆增强法学硕士。 Larimar 的内存允许动态、一次性更新知识，而不需要计算成本高昂的重新训练或微调。多个事实编辑基准的实验结果表明，即使在具有挑战性的顺序编辑设置中，Larimar 也能达到与最具竞争力的基线相当的准确性，而且在速度方面也表现出色 - 根据基础 LLM 的不同，可实现 4-10 倍的加速 - 以及灵活性由于所提出的架构简单、与法学硕士无关，因此具有通用性。我们进一步提供了 Larimar 选择性事实遗忘和输入上下文长度泛化的机制，并展示了它们的有效性。]]></description>
      <guid>https://arxiv.org/abs/2403.11901</guid>
      <pubDate>Tue, 19 Mar 2024 04:49:16 GMT</pubDate>
    </item>
    <item>
      <title>LLaVA-UHD：感知任何长宽比和高分辨率图像的 LMM</title>
      <link>https://arxiv.org/abs/2403.11703</link>
      <description><![CDATA[视觉编码构成了理解视觉世界的大型多模态模型（LMM）的基础。传统的 LMM 处理固定尺寸和有限分辨率的图像，而最近在这个方向上的探索在适应性、效率甚至正确性方面都受到限制。在这项工作中，我们首先以 GPT-4V 和 LLaVA-1.5 为代表，并揭示了其视觉编码策略的系统缺陷。为了应对这些挑战，我们提出了 LLaVA-UHD，这是一种大型多模态模型，可以有效地感知任何长宽比和高分辨率的图像。 LLaVA-UHD 包括三个关键组件：(1) 图像模块化策略，将原始分辨率图像划分为较小的可变大小切片，以实现高效和可扩展的编码；(2) 压缩模块，进一步压缩来自视觉编码器的图像标记；以及 ( 3) 为 LLM 组织切片标记的空间模式。综合实验表明，LLaVA-UHD 的性能优于在 9 个基准上使用多 2-3 个数量级数据进行训练的已建立的 LMM。值得注意的是，我们基于 LLaVA-1.5 336x336 构建的模型仅使用 94% 的推理计算即可支持 6 倍大（即 672x1088）分辨率的图像，并且在 TextVQA 上实现了 6.4 的精度提升。此外，该模型可以在学术环境中在 8 个 A100 GPU 上在 23 小时内进行高效训练（而 LLaVA-1.5 则需要 26 小时）。我们在 https://github.com/thunlp/LLaVA-UHD 上公开提供数据和代码。]]></description>
      <guid>https://arxiv.org/abs/2403.11703</guid>
      <pubDate>Tue, 19 Mar 2024 04:46:49 GMT</pubDate>
    </item>
    <item>
      <title>MindEye2：共享受试者模型可通过 1 小时的数据实现 fMRI 转图像</title>
      <link>https://arxiv.org/abs/2403.11207</link>
      <description><![CDATA[通过大脑活动重建视觉感知已经有了极大的改善，但这种方法的实际用途却受到限制。这是因为此类模型是针对每个受试者独立训练的，每个受试者需要数十小时昂贵的功能磁共振成像训练数据才能获得高质量的结果。目前的工作仅使用 1 小时的 fMRI 训练数据就展示了高质量的重建。我们对 7 个受试者的模型进行预训练，然后根据新受试者的最少数据进行微调。我们新颖的功能对齐程序将所有大脑数据线性映射到共享主体潜在空间，然后共享非线性映射到 CLIP 图像空间。然后，我们通过微调 Stable Diffusion XL 将 CLIP 空间映射到像素空间，以接受 CLIP 潜在变量作为输入而不是文本。与单受试者方法相比，这种方法利用有限的训练数据提高了受试者外泛化能力，并且还获得了最先进的图像检索和重建指标。 MindEye2 展示了如何通过单次访问 MRI 设施来准确重建感知。所有代码均可在 GitHub 上获取。]]></description>
      <guid>https://arxiv.org/abs/2403.11207</guid>
      <pubDate>Tue, 19 Mar 2024 04:28:15 GMT</pubDate>
    </item>
    <item>
      <title>VideoAgent：用于视频理解的内存增强多模态代理</title>
      <link>https://arxiv.org/abs/2403.11481</link>
      <description><![CDATA[我们探索如何将多个基础模型（大型语言模型和视觉语言模型）与新颖的统一记忆机制相协调来解决具有挑战性的视频理解问题，特别是捕获长视频中的长期时间关系。特别是，所提出的多模态代理 VideoAgent： 1）构造一个结构化存储器来存储视频的通用时间事件描述和以对象为中心的跟踪状态； 2）给定一个输入任务查询，它利用包括视频片段定位和对象内存查询在内的工具以及其他视觉基础模型来交互式地解决任务，利用法学硕士的零样本工具使用能力。 VideoAgent 在多个长视野视频理解基准测试中表现出了令人印象深刻的性能，NExT-QA 上的平均性能提高了 6.6%，EgoSchema 上的性能提高了 26.0%，缩小了开源模型与包括 Gemini 1.5 Pro 在内的私有模型之间的差距。]]></description>
      <guid>https://arxiv.org/abs/2403.11481</guid>
      <pubDate>Tue, 19 Mar 2024 04:23:39 GMT</pubDate>
    </item>
    <item>
      <title>使用受控多视图编辑的通用 3D 扩散适配器</title>
      <link>https://arxiv.org/abs/2403.12032</link>
      <description><![CDATA[由于数据有限和计算复杂度较高，开放域 3D 对象合成一直落后于图像合成。为了弥补这一差距，最近的工作研究了多视图扩散，但在 3D 一致性、视觉质量或效率方面往往达不到要求。本文提出了 MVEdit，它作为 SDEdit 的 3D 对应物，采用祖先采样对多视图图像进行联合去噪并输出高质量的纹理网格。 MVEdit 基于现成的 2D 扩散模型构建，通过免训练的 3D 适配器实现 3D 一致性，该适配器将上一个时间步的 2D 视图提升为连贯的 3D 表示，然后使用渲染视图调节下一个时间步的 2D 视图，而不影响视觉质量。该框架的推理时间仅为 2-5 分钟，与分数蒸馏相比，在质量和速度之间实现了更好的权衡。 MVEdit 具有高度通用性和可扩展性，具有广泛的应用范围，包括文本/图像到 3D 生成、3D 到 3D 编辑和高质量纹理合成。特别是，评估展示了图像转 3D 和文本引导纹理生成任务中最先进的性能。此外，我们引入了一种在资源有限的小型 3D 数据集上微调 2D 潜在扩散模型的方法，从而实现快速的低分辨率文本到 3D 初始化。]]></description>
      <guid>https://arxiv.org/abs/2403.12032</guid>
      <pubDate>Tue, 19 Mar 2024 04:06:14 GMT</pubDate>
    </item>
    <item>
      <title>LN3Diff：可扩展的潜在神经场扩散以实现快速 3D 生成</title>
      <link>https://arxiv.org/abs/2403.12019</link>
      <description><![CDATA[随着生成模型和可微渲染技术的进步，神经渲染领域取得了重大进展。尽管 2D 扩散已经取得了成功，但统一的 3D 扩散管道仍然悬而未决。本文介绍了一种名为 LN3Diff 的新颖框架来解决这一差距，并实现快速、高质量和通用的条件 3D 生成。我们的方法利用 3D 感知架构和变分自动编码器 (VAE) 将输入图像编码到结构化、紧凑的 3D 潜在空间中。潜在信息由基于 Transformer 的解码器解码为高容量 3D 神经场。通过在这个 3D 感知潜在空间上训练扩散模型，我们的方法在 ShapeNet 上实现了最先进的 3D 生成性能，并在各种数据集的单目 3D 重建和条件 3D 生成中展示了卓越的性能。此外，它在推理速度方面超越了现有的 3D 扩散方法，无需针对每个实例进行优化。我们提出的 LN3Diff 代表了 3D 生成建模的重大进步，并为 3D 视觉和图形任务中的各种应用带来了希望。]]></description>
      <guid>https://arxiv.org/abs/2403.12019</guid>
      <pubDate>Tue, 19 Mar 2024 04:00:37 GMT</pubDate>
    </item>
    <item>
      <title>SV3D：使用潜在视频扩散从单个图像进行新颖的多视图合成和 3D 生成</title>
      <link>https://arxiv.org/abs/2403.12008</link>
      <description><![CDATA[我们提出了稳定视频 3D (SV3D)——一种潜在视频扩散模型，用于围绕 3D 对象生成高分辨率、图像到多视图的轨道视频。最近关于 3D 生成的工作提出了调整 2D 生成模型以实现新视图合成 (NVS) 和 3D 优化的技术。然而，由于视图有限或 NVS 不一致，这些方法存在一些缺点，从而影响 3D 对象生成的性能。在这项工作中，我们提出了 SV3D，它采用图像到视频扩散模型来实现新颖的多视图合成和 3D 生成，从而利用视频模型的泛化和多视图一致性，同时进一步为 NVS 添加显式摄像机控制。我们还提出了改进的 3D 优化技术，以使用 SV3D 及其 NVS 输出进行图像到 3D 的生成。对具有 2D 和 3D 指标的多个数据集的广泛实验结果以及用户研究证明，与之前的作品相比，SV3D 在 NVS 以及 3D 重建方面具有最先进的性能。]]></description>
      <guid>https://arxiv.org/abs/2403.12008</guid>
      <pubDate>Tue, 19 Mar 2024 03:46:47 GMT</pubDate>
    </item>
    </channel>
</rss>