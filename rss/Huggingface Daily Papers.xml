<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>拥抱日报</title>
    <link>https://huggingface.co/papers</link>
    <description>Huggingface Daily Papers - RSSHub 用爱制作的(https://github.com/DIYgod/RSSHub)</description>
    <lastBuildDate>Fri, 29 Mar 2024 13:13:08 GMT</lastBuildDate>
    <item>
      <title>sDPO：不要一次性使用所有数据</title>
      <link>https://arxiv.org/abs/2403.19270</link>
      <description><![CDATA[随着大型语言模型 (LLM) 开发的进展，使其与人类偏好保持一致变得越来越重要。我们提出了逐步 DPO (sDPO)，这是最近流行的用于对齐调整的直接偏好优化 (DPO) 的扩展。这种方法涉及划分可用的偏好数据集并逐步使用它们，而不是一次性使用它们。我们证明，这种方法有助于在 DPO 培训框架内使用更精确对齐的参考模型。此外，sDPO 训练的最终模型性能更高，甚至优于其他具有更多参数的流行法学硕士。]]></description>
      <guid>https://arxiv.org/abs/2403.19270</guid>
      <pubDate>Fri, 29 Mar 2024 04:40:11 GMT</pubDate>
    </item>
    <item>
      <title>LITA：语言指导的时间本地化助手</title>
      <link>https://arxiv.org/abs/2403.19046</link>
      <description><![CDATA[多模式大语言模型 (LLM) 取得了巨大进展。最近的工作已将这些模型扩展到视频输入，并具有良好的指令跟随能力。然而，一个重要的缺失部分是时间定位。这些模型无法准确回答“何时？”问题。我们确定了限制其时间定位能力的三个关键方面：（i）时间表示，（ii）架构和（iii）数据。我们通过提出具有以下功能的语言指导时间本地化助手（LITA）来解决这些缺点：（1）我们引入了时间标记，对相对于视频长度的时间戳进行编码，以更好地表示视频中的时间。 (2) 我们在架构中引入 SlowFast 令牌，以精细的时间分辨率捕获时间信息。 (3) 我们强调 LITA 的时间定位数据。除了利用带有时间戳的现有视频数据集之外，我们还提出了一项新任务：推理时间定位（RTL）以及数据集 ActivityNet-RTL，用于学习和评估该任务。推理时间本地化需要视频 LLM 的推理和时间本地化。 LITA 在这项具有挑战性的任务中表现出了出色的性能，几乎使基线的时间平均交并集 (mIoU) 提高了一倍。此外，我们表明，与现有的视频法学硕士相比，我们对时间本地化的重视也大大改善了基于视频的文本生成，包括时间理解相对提高了 36%。代码位于：https://github.com/NVlabs/LITA]]></description>
      <guid>https://arxiv.org/abs/2403.19046</guid>
      <pubDate>Fri, 29 Mar 2024 04:36:23 GMT</pubDate>
    </item>
    <item>
      <title>GaussianCube：使用 3D 生成建模的最佳传输构建高斯泼溅</title>
      <link>https://arxiv.org/abs/2403.19655</link>
      <description><![CDATA[3D 高斯分布 (GS) 在 3D 拟合保真度和渲染速度方面比神经辐射场取得了相当大的改进。然而，这种具有分散高斯分布的非结构化表示对生成建模提出了重大挑战。为了解决这个问题，我们引入了 GaussianCube，一种结构化的 GS 表示形式，对于生成建模来说既强大又高效。我们通过首先提出一种改进的致密化约束 GS 拟合算法来实现这一目标，该算法可以使用固定数量的自由高斯函数产生高质量的拟合结果，然后通过最佳传输将高斯函数重新排列到预定义的体素网格中。结构化网格表示允许我们使用标准 3D U-Net 作为扩散生成建模的骨干，而无需精心设计。在 ShapeNet 和 OmniObject3D 上进行的大量实验表明，我们的模型在定性和定量上都实现了最先进的生成结果，强调了 GaussianCube 作为强大且多功能的 3D 表示的潜力。]]></description>
      <guid>https://arxiv.org/abs/2403.19655</guid>
      <pubDate>Fri, 29 Mar 2024 04:25:54 GMT</pubDate>
    </item>
    <item>
      <title>Mesh2NeRF：用于神经辐射场表示和生成的直接网格监督</title>
      <link>https://arxiv.org/abs/2403.19319</link>
      <description><![CDATA[我们提出了 Mesh2NeRF，一种从用于 3D 生成任务的纹理网格导出地面实况辐射场的方法。许多 3D 生成方法将 3D 场景表示为用于训练的辐射场。它们的地面实况辐射场通常根据大规模合成 3D 数据集的多视图渲染进行拟合，这通常会因遮挡或拟合不足问题而产生伪影。在 Mesh2NeRF 中，我们提出了一种解析解决方案，可以直接从 3D 网格获取地面实况辐射场，使用具有定义表面厚度的占用函数来表征密度场，并通过考虑网格和环境的反射函​​数确定与视图相关的颜色灯光。 Mesh2NeRF 提取准确的辐射场，为训练生成 NeRF 和单场景表示提供直接监督。我们验证了 Mesh2NeRF 在各种任务中的有效性，在 ABO 数据集上的单场景表示视图合成中实现了 3.12dB 的 PSNR 显着改进，在 ShapeNet Cars 的单视图条件生成中实现了 0.69 PSNR 增强，并且显着改进了网格提取来自 NeRF 的 Objaverse Mugs 无条件生成。]]></description>
      <guid>https://arxiv.org/abs/2403.19319</guid>
      <pubDate>Fri, 29 Mar 2024 04:01:00 GMT</pubDate>
    </item>
    <item>
      <title>TextCrraftor：您的文本编码器可以是图像质量控制器</title>
      <link>https://arxiv.org/abs/2403.18978</link>
      <description><![CDATA[基于扩散的文本到图像生成模型，例如稳定扩散，彻底改变了内容生成领域，使图像编辑和视频合成等领域取得了重大进步。尽管这些模型具有强大的功能，但也有其局限性。合成与输入文本良好对齐的图像仍然具有挑战性，并且需要使用精心设计的提示进行多次运行才能获得满意的结果。为了减轻这些限制，许多研究都试图利用各种技术来微调预先训练的扩散模型，即 UNet。然而，在这些努力中，文本到图像扩散模型训练的一个关键问题在很大程度上仍未得到探索：微调文本编码器以提高文本到图像扩散模型的性能是否可能且可行？我们的研究结果表明，我们可以通过我们提出的微调方法 TextCrraftor 来增强它，从而在定量基准和人类评估方面取得实质性改进，而不是用其他大型语言模型替换稳定扩散中使用的 CLIP 文本编码器。有趣的是，我们的技术还通过不同文本编码器的插值来实现可控图像生成，并通过各种奖励进行微调。我们还证明 TextCraftor 与 UNet 微调是正交的，并且可以结合起来进一步提高生成质量。]]></description>
      <guid>https://arxiv.org/abs/2403.18978</guid>
      <pubDate>Fri, 29 Mar 2024 03:48:11 GMT</pubDate>
    </item>
    </channel>
</rss>