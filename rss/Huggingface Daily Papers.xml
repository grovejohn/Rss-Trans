<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>拥抱日报</title>
    <link>https://huggingface.co/papers</link>
    <description>Huggingface Daily Papers - RSSHub 用爱制作的(https://github.com/DIYgod/RSSHub)</description>
    <lastBuildDate>Mon, 18 Mar 2024 07:35:59 GMT</lastBuildDate>
    <item>
      <title>VideoAgent：以大语言模型为代理的长格式视频理解</title>
      <link>https://arxiv.org/abs/2403.10517</link>
      <description><![CDATA[长视频理解是计算机视觉领域的一项重大挑战，需要一个能够对长多模态序列进行推理的模型。受人类对长视频理解的认知过程的推动，我们强调交互式推理和规划，而不是处理冗长的视觉输入的能力。我们引入了一种新颖的基于代理的系统 VideoAgent，它采用大型语言模型作为中央代理来迭代地识别和编译关键信息来回答问题，并以视觉语言基础模型作为翻译和检索视觉信息的工具。根据具有挑战性的 EgoSchema 和 NExT-QA 基准进行评估，VideoAgent 实现了 54.1% 和 71.3% 的零镜头准确率，平均仅使用 8.4 和 8.2 帧。这些结果证明了我们的方法比当前最先进的方法具有卓越的有效性和效率，凸显了基于代理的方法在推进长视频理解方面的潜力。]]></description>
      <guid>https://arxiv.org/abs/2403.10517</guid>
      <pubDate>Mon, 18 Mar 2024 03:16:29 GMT</pubDate>
    </item>
    <item>
      <title>Uni-SMART：通用科学多模态分析和研究转换器</title>
      <link>https://arxiv.org/abs/2403.10301</link>
      <description><![CDATA[在科学研究及其应用中，科学文献分析至关重要，因为它使研究人员能够以他人的工作为基础。然而，科学知识的快速增长导致学术文章大量增加，使得深入的文献分析变得越来越具有挑战性和耗时。大型语言模型（LLM）的出现为应对这一挑战提供了一种新方法。法学硕士以其强大的文本总结能力而闻名，被视为改进科学文献分析的潜在工具。然而，现有的法学硕士有其自身的局限性。科学文献通常包含广泛的多模式元素，例如分子结构、表格和图表，这些对于以文本为中心的法学硕士来说很难理解和分析。这个问题表明迫切需要能够充分理解和分析科学文献中多模态内容的新解决方案。为了满足这一需求，我们推出了 Uni-SMART（Universal Science Multimodal Analysis and Research Transformer），这是一种专为深入理解多模态科学文献而设计的创新模型。通过跨多个领域的严格定量评估，Uni-SMART 表现出了优于领先的以文本为中心的法学硕士的性能。此外，我们的探索延伸到实际应用，包括专利侵权检测和图表的细致分析。这些应用不仅凸显了 Uni-SMART 的适应性，而且还凸显了它彻底改变我们与科学文献互动方式的潜力。]]></description>
      <guid>https://arxiv.org/abs/2403.10301</guid>
      <pubDate>Mon, 18 Mar 2024 03:11:39 GMT</pubDate>
    </item>
    <item>
      <title>RAFT：使语言模型适应特定领域的 RAG</title>
      <link>https://arxiv.org/abs/2403.10131</link>
      <description><![CDATA[在大型文本数据语料库上预训练大型语言模型 (LLM) 现在已成为一种标准范例。当将这些 LLM 用于许多下游应用程序时，通常会通过基于 RAG 的提示或微调将新知识（例如，时间关键的新闻或私有领域知识）额外融入到预训练模型中。然而，模型获得此类新知识的最佳方法仍然是一个悬而未决的问题。在本文中，我们提出了检索增强微调（RAFT），这是一种训练方法，可以提高模型在“开放书籍”域内设置中回答问题的能力。在 RAFT 中，给定一个问题和一组检索到的文档，我们训练模型忽略那些无助于回答问题的文档，我们称之为干扰文档。 RAFT 通过逐字引用相关文档中有助于回答问题的正确序列来实现这一点。这与 RAFT 的思维链式响应相结合，有助于提高模型的推理能力。在特定领域的 RAG 中，RAFT 持续改进了 PubMed、HotpotQA 和 Gorilla 数据集中的模型性能，提供了一种训练后配方，可将预训练的 LLM 改进为域内 RAG。 RAFT 的代码和演示在 github.com/ShishirPatil/gorilla 上开源。]]></description>
      <guid>https://arxiv.org/abs/2403.10131</guid>
      <pubDate>Mon, 18 Mar 2024 03:07:04 GMT</pubDate>
    </item>
    <item>
      <title>EfficientVMamba：用于轻量级 Visual Mamba 的 Atrous 选择性扫描</title>
      <link>https://arxiv.org/abs/2403.09977</link>
      <description><![CDATA[先前在轻量级模型开发方面的努力主要集中在基于 CNN 和 Transformer 的设计上，但面临着持续的挑战。擅长局部特征提取的 CNN 会影响分辨率，而 Transformer 则可提供全局覆盖，但会增加计算需求 O(N^2)。准确性和效率之间持续的权衡仍然是一个重大障碍。最近，Mamba等状态空间模型（SSM）在语言建模和计算机视觉等各种任务中表现出了出色的性能和竞争力，同时将全局信息提取的时间复杂度降低到O(N)。受此启发，这项工作提出探索视觉状态空间模型在轻量级模型设计中的潜力，并引入一种名为 EfficientVMamba 的新型高效模型变体。具体来说，我们的 EfficientVMamba 通过高效的跳跃采样集成了基于孔的选择性扫描方法，构成了旨在利用全局和局部表征特征的构建块。此外，我们研究了 SSM 块和卷积之间的集成，并引入了高效的视觉状态空间块与附加的卷积分支相结合，进一步提高了模型性能。实验结果表明，EfficientVMamba 降低了计算复杂性，同时在各种视觉任务中产生了有竞争力的结果。例如，我们具有 1.3G FLOP 的 EfficientVMamba-S 在 ImageNet 上将具有 1.5G FLOP 的 Vim-Ti 的准确率大幅提高了 5.6%。代码位于：https://github.com/TerryPei/EfficientVMamba。]]></description>
      <guid>https://arxiv.org/abs/2403.09977</guid>
      <pubDate>Mon, 18 Mar 2024 02:55:14 GMT</pubDate>
    </item>
    <item>
      <title>用于大型语言模型中快速推测解码的循环起草器</title>
      <link>https://arxiv.org/abs/2403.09919</link>
      <description><![CDATA[在本文中，我们介绍了一种改进的推测解码方法，旨在提高服务大型语言模型的效率。我们的方法利用了两种既定技术的优势：经典的双模型推测解码方法和更新的单模型方法 Medusa。受到美杜莎的启发，我们的方法采用单模型策略进行推测解码。然而，我们的方法的独特之处在于采用了具有循环依赖设计的单个轻量级草稿头，本质上类似于经典推测解码中使用的小型草稿模型，但没有完整变压器架构的复杂性。并且由于反复出现的依赖关系，我们可以使用集束搜索（beam search）来通过草稿头快速过滤掉不需要的候选人。结果是一种结合了单模型设计的简单性的方法，并且避免了创建仅用于 Medusa 推理的数据依赖树注意结构的需要。我们凭经验证明了所提出的方法在几种流行的开源语言模型上的有效性，并对采用这种方法所涉及的权衡进行了全面分析。]]></description>
      <guid>https://arxiv.org/abs/2403.09919</guid>
      <pubDate>Mon, 18 Mar 2024 02:21:20 GMT</pubDate>
    </item>
    <item>
      <title>Alignment Studio：使大型语言模型与特定上下文规则保持一致</title>
      <link>https://arxiv.org/abs/2403.09704</link>
      <description><![CDATA[大型语言模型的对齐通常由模型提供者完成，以添加或控制跨用例和上下文常见或普遍理解的行为。相比之下，在本文中，我们提出了一种方法和架构，使应用程序开发人员能够根据其特定价值观、社会规范、法律和其他法规调整模型，并在上下文中协调潜在冲突的需求。我们列出了这种 Alignment Studio 架构的三个主要组件：Framers、Instructors 和 Auditors，它们协同工作来控制语言模型的行为。我们通过一个运行示例来说明这种方法，该示例将公司面向内部的企业聊天机器人与其业务行为准则保持一致。]]></description>
      <guid>https://arxiv.org/abs/2403.09704</guid>
      <pubDate>Mon, 18 Mar 2024 02:17:58 GMT</pubDate>
    </item>
    <item>
      <title>MusicHiFi：快速高保真立体声声码</title>
      <link>https://arxiv.org/abs/2403.10493</link>
      <description><![CDATA[基于扩散的音频和音乐生成模型通常通过构建音频的图像表示（例如梅尔频谱图）然后使用相位重建模型或声码器将其转换为音频来生成音乐。然而，典型的声码器以较低的分辨率（例如 16-24 kHz）产生单声道音频，这限制了它们的有效性。我们提出 MusicHiFi——一种高效的高保真立体声声码器。我们的方法采用级联的三个生成对抗网络（GAN），将低分辨率梅尔频谱图转换为音频，通过带宽扩展上采样为高分辨率音频，并上混为立体声音频。与之前的工作相比，我们提出了 1）一个统一的基于 GAN 的生成器和鉴别器架构以及级联每个阶段的训练程序，2）一个新的快速、接近下采样兼容的带宽扩展模块，以及 3）一个新的快速下混 -兼容的单声道到立体声上混音器，确保输出中保留单声道内容。我们使用客观和主观听力测试来评估我们的方法，发现与过去的工作相比，我们的方法产生了可比或更好的音频质量、更好的空间化控制以及明显更快的推理速度。声音示例位于 https://MusicHiFi.github.io/web/。]]></description>
      <guid>https://arxiv.org/abs/2403.10493</guid>
      <pubDate>Mon, 18 Mar 2024 02:03:36 GMT</pubDate>
    </item>
    <item>
      <title>Isotropic3D：基于单个 CLIP 嵌入的图像到 3D 生成</title>
      <link>https://arxiv.org/abs/2403.10395</link>
      <description><![CDATA[受预训练 2D 扩散模型不断增加的鼓舞，利用分数蒸馏采样 (SDS) 生成图像到 3D 正在取得显着进展。大多数现有方法结合了二维扩散模型的新颖视图提升，通常以参考图像为条件，同时在参考视图上应用硬 L2 图像监督。然而，过度依赖图像很容易破坏 2D 扩散模型的归纳知识，导致 3D 生成经常平坦或扭曲。在这项工作中，我们以新颖的视角重新审视图像到 3D 的过程，并提出了 Isotropic3D，这是一种仅将图像 CLIP 嵌入作为输入的图像到 3D 生成流程。 Isotropic3D 允许优化是各向同性的。方位角仅依赖于 SDS 损耗。我们框架的核心在于两阶段扩散模型微调。首先，我们对文本到3D扩散模型进行微调，用图像编码器代替文本编码器，从而使模型初步获得图像到图像的能力。其次，我们使用显式多视图注意（EMA）进行微调，它将有噪声的多视图图像与无噪声参考图像结合起来作为显式条件。 CLIP嵌入在整个过程中被发送到扩散模型，而参考图像在微调后被丢弃一次。因此，通过单张图像CLIP嵌入，Isotropic3D能够生成多视图相互一致的图像，并且与现有图像相比，内容更加对称整齐、几何形状比例匀称、色彩纹理丰富、畸变更少的3D模型-to-3D 方法，同时仍然在很大程度上保留与参考图像的相似性。该项目页面位于 https://isotropic3d.github.io/。代码和模型可在 https://github.com/pkunliu/Isotropic3D 获取。]]></description>
      <guid>https://arxiv.org/abs/2403.10395</guid>
      <pubDate>Mon, 18 Mar 2024 01:56:46 GMT</pubDate>
    </item>
    <item>
      <title>通过表面对齐高斯溅射实现可控文本到 3D 生成</title>
      <link>https://arxiv.org/abs/2403.09981</link>
      <description><![CDATA[虽然文本到 3D 和图像到 3D 生成任务受到了相当多的关注，但它们之间的一个重要但尚未充分探索的领域是可控文本到 3D 生成，我们在这项工作中主要关注这一点。为了解决这个任务，1）我们引入了多视图控制网络（MVControl），这是一种新颖的神经网络架构，旨在通过集成额外的输入条件（例如边缘、深度、法线和涂鸦）来增强现有的预训练多视图扩散模型地图。我们的创新在于引入了一个调节模块，该模块使用局部和全局嵌入来控制基础扩散模型，这些嵌入是根据输入条件图像和相机姿势计算得出的。经过训练后，MVControl 能够为基于优化的 3D 生成提供 3D 扩散指导。并且，2）我们提出了一种高效的多阶段 3D 生成流程，该流程利用了最新大型重建模型和分数蒸馏算法的优势。基于我们的 MVControl 架构，我们采用独特的混合扩散引导方法来指导优化过程。为了追求效率，我们采用 3D 高斯作为表示，而不是常用的隐式表示。我们还率先使用 SuGaR，这是一种将高斯函数绑定到网格三角形面的混合表示形式。这种方法缓解了 3D 高斯中几何形状较差的问题，并能够在网格上直接雕刻细粒度几何形状。大量实验表明，我们的方法实现了稳健的泛化，并能够可控地生成高质量的 3D 内容。]]></description>
      <guid>https://arxiv.org/abs/2403.09981</guid>
      <pubDate>Mon, 18 Mar 2024 01:50:52 GMT</pubDate>
    </item>
    </channel>
</rss>