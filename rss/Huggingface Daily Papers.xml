<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>拥抱日报</title>
    <link>https://huggingface.co/papers</link>
    <description>Huggingface Daily Papers - RSSHub 用爱制作的(https://github.com/DIYgod/RSSHub)</description>
    <lastBuildDate>Tue, 02 Apr 2024 16:16:31 GMT</lastBuildDate>
    <item>
      <title>测量扩散模型中的风格相似度</title>
      <link>https://arxiv.org/abs/2404.01292</link>
      <description><![CDATA[生成模型现在被图形设计师和艺术家广泛使用。先前的研究表明，这些模型会在生成过程中记住并经常复制训练数据中的内容。因此，随着图像数量的增加，每次在生成的图像用于专业目的之前，执行数据库搜索以确定图像的属性是否可归因于特定的训练数据变得非常重要。用于此目的的现有工具侧重于检索相似语义内容的图像。与此同时，许多艺术家关心文本到图像模型中的风格复制。我们提出了一个用于理解和从图像中提取风格描述符的框架。我们的框架包含一个新的数据集，该数据集使用以下见解来策划：风格是图像的主观属性，它捕获复杂但有意义的因素之间的相互作用，包括但不限于颜色、纹理、形状等。我们还提出了一种提取风格描述符的方法，可用于将生成图像的样式归因于文本到图像模型的训练数据集中使用的图像。我们展示了各种风格检索任务中令人鼓舞的结果。我们还定量和定性分析稳定扩散模型中的风格归因和匹配。代码和工件可在 https://github.com/learn2phoenix/CSD 获取。]]></description>
      <guid>https://arxiv.org/abs/2404.01292</guid>
      <pubDate>Tue, 02 Apr 2024 15:16:00 GMT</pubDate>
    </item>
    <item>
      <title>ST-LLM：大型语言模型是有效的时间学习者</title>
      <link>https://arxiv.org/abs/2404.00308</link>
      <description><![CDATA[大型语言模型 (LLM) 在文本理解和生成方面展示了令人印象深刻的能力，推动了视频 LLM 的研究工作，以促进视频级别的人机交互。然而，如何在基于视频的对话系统中有效地编码和理解视频仍有待解决。在本文中，我们研究了一个简单但尚未探索的问题：我们能否将所有时空标记输入到法学硕士中，从而将视频序列建模的任务委托给法学硕士？令人惊讶的是，这种简单的方法显着提高了视频理解能力。基于此，我们提出了 ST-LLM，一种有效的视频-LLM 基线，在 LLM 内进行时空序列建模。此外，为了解决法学硕士内未压缩视频令牌带来的开销和稳定性问题，我们开发了一种具有定制培训目标的动态屏蔽策略。对于特别长的视频，我们还设计了全局-本地输入模块来平衡效率和效果。因此，我们利用法学硕士进行熟练的时空建模，同时保持效率和稳定性。大量的实验结果证明了我们方法的有效性。通过更简洁的模型和训练流程，ST-LLM 在 VideoChatGPT-Bench 和 MVBench 上建立了新的最先进结果。代码已在 https://github.com/TencentARC/ST-LLM 提供。]]></description>
      <guid>https://arxiv.org/abs/2404.00308</guid>
      <pubDate>Tue, 02 Apr 2024 15:10:47 GMT</pubDate>
    </item>
    <item>
      <title>流式传输密集视频字幕</title>
      <link>https://arxiv.org/abs/2404.01297</link>
      <description><![CDATA[密集视频字幕的理想模型（预测视频中临时定位的字幕）应该能够处理长输入视频，预测丰富、详细的文本描述，并能够在处理整个视频之前生成输出。然而，当前最先进的模型处理固定数量的下采样帧，并在观看整个视频后进行单个完整预测。我们提出了一种流式密集视频字幕模型，该模型由两个新颖的组件组成：首先，我们提出了一种基于聚类传入令牌的新内存模块，由于内存大小固定，因此可以处理任意长的视频。其次，我们开发了一种流式解码算法，使我们的模型能够在整个视频处理之前进行预测。我们的模型实现了这种流媒体能力，并显着提高了三个密集视频字幕基准测试的最新水平：ActivityNet、YouCook2 和 ViTT。我们的代码发布于 https://github.com/google-research/scenic。]]></description>
      <guid>https://arxiv.org/abs/2404.01297</guid>
      <pubDate>Tue, 02 Apr 2024 14:59:46 GMT</pubDate>
    </item>
    <item>
      <title>CosmicMan：人类的文本到图像基础模型</title>
      <link>https://arxiv.org/abs/2404.01294</link>
      <description><![CDATA[我们提出了 CosmicMan，一种专门用于生成高保真人类图像的文本到图像基础模型。与当前通用基础模型陷入质量低劣和人类文本图像错位的困境不同，CosmicMan能够生成外观细致、结构合理、文本图像对齐精确、描述详细密集的逼真人类图像。 CosmicMan 成功的核心是对数据和模型的新反思和视角：（1）我们发现数据质量和可扩展的数据生产流程对于训练模型的最终结果至关重要。因此，我们提出了一种新的数据生产范式，Annotate Anybody，它作为永久的数据飞轮，随着时间的推移产生具有准确且具有成本效益的注释的高质量数据。在此基础上，我们构建了一个大规模数据集 CosmicMan-HQ 1.0，其中包含 600 万张平均分辨率为 1488x1255 的高质量真实人类图像，并附有来自 1.15 亿个不同粒度属性的精确文本注释。 （2）我们认为，专门针对人类的文本到图像基础模型必须是务实的——易于集成到下游任务中，同时有效地生成高质量的人类图像。因此，我们建议以分解的方式对密集文本描述和图像像素之间的关系进行建模，并提出分解注意力重新聚焦（Daring）训练框架。它无缝分解现有文本到图像扩散模型中的交叉注意力特征，并在不添加额外模块的情况下强制注意力重新聚焦。通过 Daring，我们表明，将连续文本空间显式离散为与人体结构对齐的几个基本组是轻松解决错位问题的关键。]]></description>
      <guid>https://arxiv.org/abs/2404.01294</guid>
      <pubDate>Tue, 02 Apr 2024 14:55:59 GMT</pubDate>
    </item>
    <item>
      <title>正确做法：提高文本到图像模型的空间一致性</title>
      <link>https://arxiv.org/abs/2404.01197</link>
      <description><![CDATA[当前文本到图像（T2I）模型的主要缺点之一是它们无法一致地生成忠实遵循文本提示中指定的空间关系的图像。在本文中，我们对这一限制进行了全面的研究，同时还开发了实现最先进性能的数据集和方法。首先，我们发现当前的视觉语言数据集不能很好地表示空间关系；为了缓解这一瓶颈，我们通过重新描述来自 4 个广泛使用的视觉数据集的 600 万张图像，创建了 SPRIGHT，这是第一个以空间为中心的大型数据集。通过三重评估和分析流程，我们发现 SPRIGHT 在捕获空间关系方面极大地改进了现有数据集。为了证明其功效，我们仅利用约 0.25% 的 SPRIGHT，在生成空间精确图像方面实现了 22% 的改进，同时还提高了 FID 和 CMMD 分数。其次，我们发现对包含大量对象的图像进行训练可以显着提高空间一致性。值得注意的是，通过对 &lt;500 张图像进行微调，我们在 T2I-CompBench 上达到了最先进的水平，空间得分为 0.2133。最后，通过一组受控实验和消融，我们记录了多项发现，我们相信这些发现将增强对影响文本到图像模型空间一致性的因素的理解。我们公开发布我们的数据集和模型，以促进该领域的进一步研究。]]></description>
      <guid>https://arxiv.org/abs/2404.01197</guid>
      <pubDate>Tue, 02 Apr 2024 05:41:30 GMT</pubDate>
    </item>
    <item>
      <title>FlexiDreamer：使用 FlexiCube 生成单一图像到 3D</title>
      <link>https://arxiv.org/abs/2404.00987</link>
      <description><![CDATA[最近，通过文本提示或单个图像生成 3D 内容在质量和速度方面取得了显着进步。其主要范例之一涉及生成一致的多视图图像，然后进行稀疏视图重建。然而，由于直接变形网格表示以接近目标拓扑的挑战，大多数方法在稀疏视图重建期间学习隐式表示（例如 NeRF），并通过后处理提取来获取目标网格。尽管隐式表示可以有效地对丰富的 3D 信息进行建模，但其训练通常需要较长的收敛时间。此外，从隐式场中进行的后提取操作也会导致不良的视觉伪影。在本文中，我们提出了 FlexiDreamer，一种新颖的单图像到 3D 生成框架，它以端到端的方式重建目标网格。通过利用灵活的基于梯度的提取（称为 FlexiCubes），我们的方法避免了后处理带来的缺陷，并有助于直接获取目标网格。此外，我们采用了多分辨率哈希网格编码方案，该方案逐步将编码级别激活到 FlexiCube 中的隐式字段中，以帮助捕获每步优化的几何细节。值得注意的是，FlexiDreamer 在单个 NVIDIA A100 GPU 上大约 1 分钟内从单视图图像中恢复密集的 3D 结构，大大优于以前的方法。]]></description>
      <guid>https://arxiv.org/abs/2404.00987</guid>
      <pubDate>Tue, 02 Apr 2024 05:22:13 GMT</pubDate>
    </item>
    <item>
      <title>来自语言模型奖励的视频大型多模态模型的直接偏好优化</title>
      <link>https://arxiv.org/abs/2404.01258</link>
      <description><![CDATA[偏好建模技术，例如直接偏好优化（DPO），在增强大语言模型（LLM）的泛化能力方面已被证明是有效的。然而，在涉及遵循视频指令的任务中，提供信息反馈，尤其是检测生成的响应中的幻觉，仍然是一个重大挑战。先前的研究已经探索使用大型多模态模型（LMM）作为奖励模型来指导偏好建模，但它们准确评估生成的响应与相应视频相比的真实性的能力尚未最终确定。本文介绍了一种新颖的框架，该框架利用详细的视频字幕作为视频内容的代理，使语言模型能够将此信息合并为视频问答（QA）预测评分的支持证据。我们的方法展示了与 OpenAI GPT-4V 模型的奖励机制的稳健一致性，该机制直接将视频帧作为输入。此外，我们还表明，通过 DPO 应用这种定制奖励可以显着提高视频 LMM 在视频 QA 任务上的性能。]]></description>
      <guid>https://arxiv.org/abs/2404.01258</guid>
      <pubDate>Tue, 02 Apr 2024 05:17:22 GMT</pubDate>
    </item>
    <item>
      <title>用于受控图像生成的条件感知神经网络</title>
      <link>https://arxiv.org/abs/2404.01143</link>
      <description><![CDATA[我们提出了条件感知神经网络（CAN），这是一种向图像生成模型添加控制的新方法。与现有的条件控制方法并行，CAN 通过动态操纵神经网络的权重来控制图像生成过程。这是通过引入条件感知权重生成模块来实现的，该模块根据输入条件为卷积/线性层生成条件权重。我们在 ImageNet 上测试 CAN 的类条件图像生成，在 COCO 上测试文本到图像生成。 CAN 始终如一地为扩散变压器模型（包括 DiT 和 UViT）提供显着改进。特别是，CAN 与 EfficientViT (CaT) 相结合，在 ImageNet 512x512 上实现了 2.78 FID，超越了 DiT-XL/2，同时每个采样步骤所需的 MAC 数量减少了 52 倍。]]></description>
      <guid>https://arxiv.org/abs/2404.01143</guid>
      <pubDate>Tue, 02 Apr 2024 05:13:44 GMT</pubDate>
    </item>
    <item>
      <title>MaGRITTe：从图像、俯视图和文本中进行操作和生成 3D 实现</title>
      <link>https://arxiv.org/abs/2404.00345</link>
      <description><![CDATA[根据用户指定的条件生成 3D 场景为减轻 3D 应用中的制作负担提供了一条有前途的途径。由于控制条件有限，之前的研究需要付出巨大的努力才能实现所需的场景。我们提出了一种使用部分图像、顶视图中表示的布局信息和文本提示在多模态条件下控制和生成 3D 场景的方法。结合这些条件来生成 3D 场景涉及以下重大困难：(1) 大型数据集的创建，(2) 反映多模态条件的交互，以及 (3) 布局条件的域依赖性。我们将 3D 场景生成过程分解为根据给定条件生成 2D 图像和根据 2D 图像生成 3D 场景。 2D 图像生成是通过使用部分图像和布局的小型人工数据集微调预训练的文本到图像模型来实现的，3D 场景生成是通过布局条件深度估计和神经辐射场 (NeRF) 来实现的，从而避免了大型数据集的创建。使用 360 度图像的空间信息的通用表示允许考虑多模态条件交互并减少布局控制的域依赖性。实验结果定性和定量地表明，所提出的方法可以根据多模态条件生成从室内到室外的不同领域的 3D 场景。]]></description>
      <guid>https://arxiv.org/abs/2404.00345</guid>
      <pubDate>Tue, 02 Apr 2024 05:10:10 GMT</pubDate>
    </item>
    </channel>
</rss>