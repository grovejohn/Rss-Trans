<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>拥抱日报</title>
    <link>https://huggingface.co/papers</link>
    <description>Huggingface Daily Papers - RSSHub 用爱制作的(https://github.com/DIYgod/RSSHub)</description>
    <lastBuildDate>Tue, 09 Apr 2024 05:12:05 GMT</lastBuildDate>
    <item>
      <title>Koala：关键帧调节长视频-LLM</title>
      <link>https://arxiv.org/abs/2404.04346</link>
      <description><![CDATA[长视频问答是一项具有挑战性的任务，涉及识别短期活动并推理其细粒度关系。最先进的视频大语言模型（vLLM）因其在新任务上展示的新兴能力而有望成为可行的解决方案。然而，尽管 vLLM 接受了数百万个长达数秒的短视频的训练，但仍无法理解长达数分钟的视频并准确回答有关它们的问题。为了解决这个限制，我们提出了一种轻量级的自监督方法，即关键帧调节的长视频 LLM (Koala)，它引入了可学习的时空查询来调整预训练的 vLLM 以推广到更长的视频。我们的方法引入了两个新的标记器，它们以从稀疏视频关键帧计算出的视觉标记为条件，以理解短视频时刻和长视频时刻。我们在 HowTo100M 上训练我们提出的方法，并在零样本长视频理解基准上展示其有效性，在所有任务中，它的绝对准确度比最先进的大型模型高 3 - 6%。令人惊讶的是，我们还凭经验表明，我们的方法不仅有助于预训练的 vLLM 理解长视频，而且还提高了其短期动作识别的准确性。]]></description>
      <guid>https://arxiv.org/abs/2404.04346</guid>
      <pubDate>Tue, 09 Apr 2024 03:59:08 GMT</pubDate>
    </item>
    <item>
      <title>Ferret-UI：基于多模式法学硕士的移动 UI 理解</title>
      <link>https://arxiv.org/abs/2404.05719</link>
      <description><![CDATA[多模态大语言模型 (MLLM) 的最新进展值得注意，但是，这些通用域 MLLM 往往缺乏理解用户界面 (UI) 屏幕并与其有效交互的能力。在本文中，我们提出了 Ferret-UI，这是一种新的 MLLM，专为增强对移动 UI 屏幕的理解而定制，配备了引用、接地和推理功能。鉴于 UI 屏幕通常表现出比自然图像更拉长的纵横比，并且包含更小的感兴趣对象（例如图标、文本），我们在 Ferret 之上合并“任何分辨率”，以放大细节并利用增强的视觉功能。具体地，每个屏幕根据原始宽高比被划分为2个子图像（即，纵向屏幕横向划分，横向屏幕纵向划分）。两个子图像在发送到 LLM 之前都会单独编码。我们从广泛的基本 UI 任务中精心收集训练样本，例如图标识别、查找文本和小部件列表。这些样本经过格式化以遵循指令并带有区域注释，以方便精确的参考和基础。为了增强模型的推理能力，我们进一步编译了用于高级任务的数据集，包括详细描述、感知/交互对话和函数推理。在对精选数据集进行训练后，Ferret-UI 展现出了对 UI 屏幕的出色理解能力以及执行开放式指令的能力。对于模型评估，我们建立了涵盖所有上述任务的综合基准。 Ferret-UI 不仅优于大多数开源 UI MLLM，而且在所有基本 UI 任务上也超过了 GPT-4V。]]></description>
      <guid>https://arxiv.org/abs/2404.05719</guid>
      <pubDate>Tue, 09 Apr 2024 03:42:14 GMT</pubDate>
    </item>
    <item>
      <title>MA-LMM：用于长期视频理解的记忆增强大型多模态模型</title>
      <link>https://arxiv.org/abs/2404.05726</link>
      <description><![CDATA[随着大型语言模型（LLM）的成功，将视觉模型集成到 LLM 中以构建视觉语言基础模型最近引起了越来越多的兴趣。然而，现有的基于LLM的大型多模态模型（例如Video-LLaMA、VideoChat）只能接受有限数量的帧来理解短视频。在这项研究中，我们主要致力于设计一个高效且有效的长期视频理解模型。我们建议以在线方式处理视频并将过去的视频信息存储在内存库中，而不是像大多数现有工作那样尝试同时处理更多帧。这使得我们的模型能够参考历史视频内容进行长期分析，而不会超出法学硕士的上下文长度限制或 GPU 内存限制。我们的记忆库可以以现成的方式无缝集成到当前的多模式法学硕士中。我们对各种视频理解任务进行了广泛的实验，例如长视频理解、视频问答和视频字幕，我们的模型可以在多个数据集上实现最先进的性能。代码可在 https://boheumd.github.io/MA-LMM/ 获取。]]></description>
      <guid>https://arxiv.org/abs/2404.05726</guid>
      <pubDate>Tue, 09 Apr 2024 03:30:07 GMT</pubDate>
    </item>
    <item>
      <title>Diffusion-RWKV：为扩散模型扩展类似 RWKV 的架构</title>
      <link>https://arxiv.org/abs/2404.04478</link>
      <description><![CDATA[Transformers 促进了计算机视觉和自然语言处理 (NLP) 领域的进步。然而，巨大的计算复杂性限制了它们在长上下文任务（例如高分辨率图像生成）中的应用。本文介绍了一系列改编自 NLP 中使用的 RWKV 模型的架构，并针对应用于图像生成任务的扩散模型进行了必要的修改，称为 Diffusion-RWKV。与 Transformers 的扩散类似，我们的模型旨在有效地处理具有额外条件的序列中的补丁输入，同时还可以有效地扩展，适应大规模参数和广泛的数据集。其独特的优势体现在降低了空间聚合复杂性，使其特别擅长处理高分辨率图像，从而消除了窗口或组缓存操作的必要性。条件和非条件图像生成任务的实验结果表明，Diffison-RWKV 在 FID 和 IS 指标中实现了与现有 CNN 或基于 Transformer 的扩散模型相当或超过的性能，同时显着降低了总计算 FLOP 使用量。]]></description>
      <guid>https://arxiv.org/abs/2404.04478</guid>
      <pubDate>Tue, 09 Apr 2024 03:28:10 GMT</pubDate>
    </item>
    <item>
      <title>MagicTime：作为变形模拟器的延时视频生成模型</title>
      <link>https://arxiv.org/abs/2404.05014</link>
      <description><![CDATA[文本到视频生成（T2V）的最新进展在从文本描述合成高质量通用视频方面取得了显着的成功。 T2V 中一个很大程度上被忽视的问题是现有模型没有充分编码现实世界的物理知识，因此生成的视频往往运动有限且变化较差。在本文中，我们提出了MagicTime，一种变质延时视频生成模型，它从延时视频中学习现实世界的物理知识并实现变质生成。首先，我们设计了一个MagicAdapter方案来解耦空间和时间训练，从变质视频中编码更多物理知识，并转换预训练的T2V模型以生成变质视频。其次，我们引入了动态帧提取策略来适应变质延时视频，其变化范围更广，涵盖了戏剧性的物体变质过程，因此比一般视频体现了更多的物理知识。最后，我们引入了魔术文本编码器来提高对变质视频提示的理解。此外，我们创建了一个名为 ChronoMagic 的延时视频文本数据集，专门用于解锁变质视频生成能力。大量实验证明了 MagicTime 在生成高质量和动态变形视频方面的优越性和有效性，表明延时视频生成是构建物理世界变形模拟器的一条有前途的途径。]]></description>
      <guid>https://arxiv.org/abs/2404.05014</guid>
      <pubDate>Tue, 09 Apr 2024 03:24:36 GMT</pubDate>
    </item>
    <item>
      <title>UniFL：通过统一反馈学习提高稳定扩散</title>
      <link>https://arxiv.org/abs/2404.05595</link>
      <description><![CDATA[扩散模型彻底改变了图像生成领域，导致高质量模型和多样化下游应用的激增。然而，尽管取得了这些重大进步，当前的竞争解决方案仍然存在一些局限性，包括视觉质量较差、缺乏审美吸引力、推理效率低下，而且还没有全面的解决方案。为了应对这些挑战，我们提出了 UniFL，这是一个利用反馈学习来全面增强扩散模型的统一框架。 UniFL 是一种通用、有效且可推广的解决方案，适用于各种扩散模型，例如 SD1.5 和 SDXL。值得注意的是，UniFL 包含三个关键组成部分：感知反馈学习，提高视觉质量；解耦反馈学习，提高审美吸引力；以及对抗性反馈学习，可优化推理速度。深入的实验和广泛的用户研究验证了我们提出的方法在提高生成模型的质量及其加速方面的卓越性能。例如，UniFL 在生成质量方面超过 ImageReward 17% 用户偏好，并在 4 步推理中比 LCM 和 SDXL Turbo 分别高出 57% 和 20%。此外，我们还验证了我们的方法在下游任务中的有效性，包括 Lora、ControlNet 和 AnimateDiff。]]></description>
      <guid>https://arxiv.org/abs/2404.05595</guid>
      <pubDate>Tue, 09 Apr 2024 03:18:40 GMT</pubDate>
    </item>
    </channel>
</rss>