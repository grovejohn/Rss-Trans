<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>拥抱日报</title>
    <link>https://huggingface.co/papers</link>
    <description>Huggingface Daily Papers - 由 RSSHub 用爱制作（https://github.com/DIYgod/RSSHub）</description>
    <lastBuildDate>Tue, 09 Apr 2024 14:11:54 GMT</lastBuildDate>
    <item>
      <title>SpatialTracker：跟踪 3D 空间中的任何 2D 像素</title>
      <link>https://arxiv.org/abs/2404.04319</link>
      <description><![CDATA[恢复视频中密集且长距离的像素运动是一个具有挑战性的问题。部分困难源自 3D 到 2D 的投影过程，导致 2D 运动域中的遮挡和不连续性。虽然 2D 运动可能很复杂，但我们认为底层的 3D 运动通常可以是简单且低维的。在这项工作中，我们建议估计 3D 空间中的点轨迹，以减轻图像投影引起的问题。我们的方法名为 SpatialTracker，使用单目深度估计器将 2D 像素提升为 3D，使用三平面表示有效地表示每个帧的 3D 内容，并使用转换器执行迭代更新以估计 3D 轨迹。 3D 跟踪使我们能够利用尽可能刚性 (ARAP) 约束，同时学习将像素聚集到不同刚性部分的刚性嵌入。广泛的评估表明，我们的方法在定性和定量上都实现了最先进的跟踪性能，特别是在平面外旋转等具有挑战性的场景中。]]></description>
      <guid>https://arxiv.org/abs/2404.04319</guid>
      <pubDate>Tue, 09 Apr 2024 05:00:20 GMT</pubDate>
    </item>
    <item>
      <title>PhysAvatar：从视觉观察中学习着装 3D 化身的物理原理</title>
      <link>https://arxiv.org/abs/2404.04421</link>
      <description><![CDATA[建模和渲染逼真的化身在许多应用中至关重要。然而，现有的通过视觉观察构建 3D 化身的方法很难重建穿着衣服的人类。我们介绍了 PhysAvatar，这是一种新颖的框架，它将逆向渲染与逆向物理相结合，可以根据多视图视频数据以及衣服面料的物理参数自动估计人类的形状和外观。为此，我们采用网格对齐 4D 高斯技术进行时空网格跟踪，并采用基于物理的逆渲染器来估计内在材料属性。 PhysAvatar 集成了物理模拟器，以原则性的方式使用基于梯度的优化来估计服装的物理参数。这些新颖的功能使 PhysAvatar 能够在训练数据中未见的运动和照明条件下，为穿着宽松衣服的化身创建高质量的新颖视图渲染。这标志着使用基于物理的逆渲染和循环物理来建模逼真的数字人类的重大进步。我们的项目网站位于：https://qingqing-zhao.github.io/PhysAvatar]]></description>
      <guid>https://arxiv.org/abs/2404.04421</guid>
      <pubDate>Tue, 09 Apr 2024 04:55:46 GMT</pubDate>
    </item>
    <item>
      <title>MoMA：用于快速生成个性化图像的多模态 LLM 适配器</title>
      <link>https://arxiv.org/abs/2404.05674</link>
      <description><![CDATA[在本文中，我们提出了 MoMA：一种开放词汇、免训练的个性化图像模型，具有灵活的零样本功能。随着基础文本到图像模型的快速发展，对强大的图像到图像转换的需求不断增长。为了满足这一需求，MoMA 专门研究主题驱动的个性化图像生成。利用开源的多模态大语言模型 (MLLM)，我们训练 MoMA 充当特征提取器和生成器的双重角色。该方法有效地协同参考图像和文本提示信息以产生有价值的图像特征，从而促进图像扩散模型。为了更好地利用生成的特征，我们进一步引入了一种新颖的自注意力捷径方法，该方法可以有效地将图像特征转移到图像扩散模型，从而提高生成图像中目标对象的相似度。值得注意的是，作为一个免调整的即插即用模块，我们的模型仅需要单个参考图像，并且在生成具有高细节保真度、增强的身份保留和即时忠实度的图像方面优于现有方法。我们的工作是开源的，从而使人们能够普遍获得这些进步。]]></description>
      <guid>https://arxiv.org/abs/2404.05674</guid>
      <pubDate>Tue, 09 Apr 2024 04:50:33 GMT</pubDate>
    </item>
    <item>
      <title>YaART：另一种 ART 渲染技术</title>
      <link>https://arxiv.org/abs/2404.05666</link>
      <description><![CDATA[在快速发展的生成模型领域，高效、高保真文本到图像扩散系统的开发代表了一个重要的前沿领域。本研究介绍了 YaART，这是一种新颖的生产级文本到图像级联扩散模型，使用人类反馈强化学习 (RLHF) 来符合人类偏好。在YaART的开发过程中，我们特别关注模型的选择和训练数据集大小，这些方面以前没有对文本到图像级联扩散模型进行系统研究。特别是，我们全面分析了这些选择如何影响训练过程的效率和生成图像的质量，这在实践中非常重要。此外，我们证明，在较小的高质量图像数据集上训练的模型可以成功地与在较大数据集上训练的模型竞争，从而建立更有效的扩散模型训练场景。从质量角度来看，与许多现有的最先进模型相比，YaART 始终受到用户的青睐。]]></description>
      <guid>https://arxiv.org/abs/2404.05666</guid>
      <pubDate>Tue, 09 Apr 2024 04:47:44 GMT</pubDate>
    </item>
    <item>
      <title>ByteEdit：增强、合规和加速生成图像编辑</title>
      <link>https://arxiv.org/abs/2404.04860</link>
      <description><![CDATA[基于扩散的生成图像编辑的最新进展引发了一场深刻的革命，重塑了图像绘制和修复任务的格局。尽管取得了这些进步，该领域仍面临着固有的挑战，包括：i）质量低劣； ii) 一致性差； iii) 未充分遵守指令； iv) 发电效率次优。为了解决这些障碍，我们推出了 ByteEdit，这是一种创新的反馈学习框架，经过精心设计，旨在增强、遵守和加速生成图像编辑任务。 ByteEdit 无缝集成了致力于增强美观和图像文本对齐的图像奖励模型，同时还引入了专为促进输出一致性而定制的密集像素级奖励模型。此外，我们提出了一种开创性的对抗性和渐进式反馈学习策略，以加快模型的推理速度。通过广泛的大规模用户评估，我们证明ByteEdit在生成质量和一致性方面都超越了领先的生成图像编辑产品，包括Adobe、Canva和MeiTu。与基线模型相比，ByteEdit-Outpainting 的质量和一致性分别显着提高了 388% 和 135%。实验还验证了我们的加速模型在质量和一致性方面保持了优异的性能结果。]]></description>
      <guid>https://arxiv.org/abs/2404.04860</guid>
      <pubDate>Tue, 09 Apr 2024 04:42:00 GMT</pubDate>
    </item>
    <item>
      <title>BeyondScene：通过预训练扩散生成更高分辨率的以人为中心的场景</title>
      <link>https://arxiv.org/abs/2404.04544</link>
      <description><![CDATA[生成具有细节和控制的更高分辨率的以人为中心的场景仍然是现有文本到图像扩散模型的挑战。这一挑战源于有限的训练图像大小、文本编码器容量（有限的标记）以及生成涉及多人的复杂场景的固有困难。虽然当前的方法仅尝试解决训练大小限制，但它们通常会产生具有严重伪影的以人为中心的场景。我们提出了 BeyondScene，这是一个克服先前限制的新颖框架，使用现有的预训练扩散模型生成精致的高分辨率（超过 8K）以人为中心的场景，具有出色的文本图像对应性和自然度。 BeyondScene采用分阶段和分层的方法，首先生成详细的基础图像，重点关注多人实例创建的关键元素以及超出扩散模型令牌限制的详细描述，然后将基础图像无缝转换为更高分辨率的输出，超过通过我们新颖的实例感知分层放大过程训练图像大小并合并感知文本和实例的细节，该过程由我们提出的高频注入前向扩散和自适应联合扩散组成。 BeyondScene 在与详细文本描述的对应性和自然性方面超越了现有方法，为更高分辨率的以人为中心的场景创建的高级应用铺平了道路，超越了预训练扩散模型的能力，而无需昂贵的再训练。项目页面：https://janeyeon.github.io/beyond-scene。]]></description>
      <guid>https://arxiv.org/abs/2404.04544</guid>
      <pubDate>Tue, 09 Apr 2024 04:35:50 GMT</pubDate>
    </item>
    <item>
      <title>通过优化人类效用来调整扩散模型</title>
      <link>https://arxiv.org/abs/2404.04465</link>
      <description><![CDATA[我们提出了 Diffusion-KTO，这是一种通过将对齐目标制定为预期人类效用的最大化来对齐文本到图像扩散模型的新颖方法。由于该目标独立适用于每一代，因此 Diffusion-KTO 不需要收集昂贵的成对偏好数据，也不需要训练复杂的奖励模型。相反，我们的目标需要简单的每图像二进制反馈信号，例如喜欢或不喜欢，这是很容易获得的。在使用 Diffusion-KTO 进行微调后，文本到图像的扩散模型与现有技术（包括监督微调和 Diffusion-DPO）相比，无论是在人类判断还是 PickScore 和 ImageReward 等自动评估指标方面都表现出了优越的性能。总体而言，Diffusion-KTO 释放了利用现成的每图像二进制信号的潜力，并扩大了将文本到图像扩散模型与人类偏好对齐的适用性。]]></description>
      <guid>https://arxiv.org/abs/2404.04465</guid>
      <pubDate>Tue, 09 Apr 2024 04:31:47 GMT</pubDate>
    </item>
    <item>
      <title>DATENeRF：NeRF 的深度感知文本编辑</title>
      <link>https://arxiv.org/abs/2404.04526</link>
      <description><![CDATA[扩散模型的最新进展显示出在根据文本提示编辑 2D 图像方面的出色熟练程度。然而，扩展这些技术来编辑神经辐射场 (NeRF) 中的场景很复杂，因为编辑单个 2D 帧可能会导致多个视图之间的不一致。我们的重要见解是 NeRF 场景的几何形状可以充当集成这些 2D 编辑的桥梁。利用这种几何结构，我们采用深度调节的 ControlNet 来增强每个 2D 图像修改的一致性。此外，我们引入了一种修复方法，该方法利用 NeRF 场景的深度信息在不同图像之间分配 2D 编辑，确保针对错误和重采样挑战的鲁棒性。我们的结果表明，与现有的文本驱动 NeRF 场景编辑领先方法相比，这种方法实现了更加一致、逼真和详细的编辑。]]></description>
      <guid>https://arxiv.org/abs/2404.04526</guid>
      <pubDate>Tue, 09 Apr 2024 04:23:02 GMT</pubDate>
    </item>
    <item>
      <title>SwapAnything：在个性化可视化编辑中启用任意对象交换</title>
      <link>https://arxiv.org/abs/2404.05717</link>
      <description><![CDATA[个人内容的有效编辑在使个人表达创造力、在视觉故事中编织引人入胜的叙述以及提升视觉内容的整体质量和影响力方面发挥着关键作用。因此，在这项工作中，我们引入了 SwapAnything，这是一个新颖的框架，可以用参考给出的个性化概念交换图像中的任何对象，同时保持上下文不变。与现有的个性化主题交换方法相比，SwapAnything具有三个独特的优势：（1）精确控制任意对象和部分而不是主要主题，（2）更忠实地保存上下文像素，（3）更好地适应个性化概念到图像。首先，我们提出有针对性的变量交换，以对潜在特征图应用区域控制，并交换屏蔽变量，以实现忠实的上下文保存和初始语义概念交换。然后，我们引入外观适应，在图像生成过程中将语义概念在目标位置、形状、风格和内容方面无缝地适应原始图像。人工和自动评估的广泛结果表明，我们的方法相对于个性化交换的基线方法有显着改进。此外，SwapAnything 在单个对象、多个对象、部分对象和跨域交换任务中显示了其精确和忠实的交换能力。 SwapAnything 还在基于文本的交换和交换之外的任务（例如对象插入）上实现了出色的性能。]]></description>
      <guid>https://arxiv.org/abs/2404.05717</guid>
      <pubDate>Tue, 09 Apr 2024 04:14:59 GMT</pubDate>
    </item>
    <item>
      <title>Koala：关键帧调节长视频-LLM</title>
      <link>https://arxiv.org/abs/2404.04346</link>
      <description><![CDATA[长视频问答是一项具有挑战性的任务，涉及识别短期活动并推理其细粒度关系。最先进的视频大语言模型 (vLLM) 因其在新任务上展示的新兴功能而有望成为可行的解决方案。然而，尽管 vLLM 接受了数百万个长达数秒的短视频的训练，但仍无法理解长达数分钟的视频并准确回答有关它们的问题。为了解决这个限制，我们提出了一种轻量级的自监督方法，即关键帧调节的长视频 LLM (Koala)，它引入了可学习的时空查询来调整预训练的 vLLM 以推广到更长的视频。我们的方法引入了两个新的标记器，它们以从稀疏视频关键帧计算出的视觉标记为条件，以理解短视频时刻和长视频时刻。我们在 HowTo100M 上训练我们提出的方法，并在零镜头长视频理解基准上展示其有效性，在所有任务中，它的绝对准确度比最先进的大型模型高出 3 - 6%。令人惊讶的是，我们还凭经验表明，我们的方法不仅有助于预训练的 vLLM 理解长视频，而且还提高了其短期动作识别的准确性。]]></description>
      <guid>https://arxiv.org/abs/2404.04346</guid>
      <pubDate>Tue, 09 Apr 2024 03:59:08 GMT</pubDate>
    </item>
    </channel>
</rss>