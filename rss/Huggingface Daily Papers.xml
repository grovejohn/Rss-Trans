<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>拥抱日报</title>
    <link>https://huggingface.co/papers</link>
    <description>Huggingface Daily Papers - RSSHub 用爱制作的(https://github.com/DIYgod/RSSHub)</description>
    <lastBuildDate>Tue, 02 Apr 2024 07:13:39 GMT</lastBuildDate>
    <item>
      <title>用于受控图像生成的条件感知神经网络</title>
      <link>https://arxiv.org/abs/2404.01143</link>
      <description><![CDATA[我们提出了条件感知神经网络（CAN），这是一种向图像生成模型添加控制的新方法。与现有的条件控制方法并行，CAN 通过动态操纵神经网络的权重来控制图像生成过程。这是通过引入条件感知权重生成模块来实现的，该模块根据输入条件为卷积/线性层生成条件权重。我们在 ImageNet 上测试 CAN 的类条件图像生成，在 COCO 上测试文本到图像生成。 CAN 始终如一地为扩散变压器模型（包括 DiT 和 UViT）提供显着改进。特别是，CAN 与 EfficientViT (CaT) 相结合，在 ImageNet 512x512 上实现了 2.78 FID，超越了 DiT-XL/2，同时每个采样步骤所需的 MAC 数量减少了 52 倍。]]></description>
      <guid>https://arxiv.org/abs/2404.01143</guid>
      <pubDate>Tue, 02 Apr 2024 05:13:44 GMT</pubDate>
    </item>
    <item>
      <title>MaGRITTe：从图像、俯视图和文本中进行操作和生成 3D 实现</title>
      <link>https://arxiv.org/abs/2404.00345</link>
      <description><![CDATA[根据用户指定的条件生成 3D 场景为减轻 3D 应用中的制作负担提供了一条有前途的途径。由于控制条件有限，之前的研究需要付出巨大的努力才能实现所需的场景。我们提出了一种使用部分图像、顶视图中表示的布局信息和文本提示在多模态条件下控制和生成 3D 场景的方法。结合这些条件来生成 3D 场景涉及以下重大困难：(1) 大型数据集的创建，(2) 反映多模态条件的交互，以及 (3) 布局条件的域依赖性。我们将 3D 场景生成过程分解为根据给定条件生成 2D 图像和根据 2D 图像生成 3D 场景。 2D 图像生成是通过使用部分图像和布局的小型人工数据集微调预训练的文本到图像模型来实现的，3D 场景生成是通过布局条件深度估计和神经辐射场 (NeRF) 来实现的，从而避免了大型数据集的创建。使用 360 度图像的空间信息的通用表示允许考虑多模态条件交互并减少布局控制的域依赖性。实验结果定性和定量地表明，所提出的方法可以根据多模态条件生成从室内到室外的不同领域的 3D 场景。]]></description>
      <guid>https://arxiv.org/abs/2404.00345</guid>
      <pubDate>Tue, 02 Apr 2024 05:10:10 GMT</pubDate>
    </item>
    </channel>
</rss>