<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>拥抱日报</title>
    <link>https://huggingface.co/papers</link>
    <description>Huggingface Daily Papers - RSSHub 用爱制作的(https://github.com/DIYgod/RSSHub)</description>
    <lastBuildDate>Sun, 28 Apr 2024 15:12:57 GMT</lastBuildDate>
    <item>
      <title>重新审视 Gecko 的文本到图像评估：关于指标、提示和人类评级</title>
      <link>https://arxiv.org/abs/2404.16820</link>
      <description><![CDATA[虽然文本到图像（T2I）生成模型已经变得无处不在，但它们不一定生成与给定提示相符的图像。虽然之前的工作通过提出用于收集人类判断的指标、基准和模板来评估 T2I 一致性，但这些组件的质量并未得到系统测量。人工评分的提示集通常很小，并且不会评估评分的可靠性以及用于比较模型的提示集。我们通过评估自动评估指标和人工模板进行广泛的研究来解决这一差距。我们提供了三个主要贡献：（1）我们引入了一个基于技能的综合基准，可以区分不同人类模板的模型。这种基于技能的基准将提示分类为子技能，使从业者不仅可以查明哪些技能具有挑战性，还可以查明该技能在何种复杂程度下变得具有挑战性。 (2)我们收集了四个模板和四个 T2I 模型的人类评分，总共超过 100K 个注释。这使我们能够了解由于提示中固有的模糊性而产生的差异以及由于指标和模型质量的差异而产生的差异。 (3) 最后，我们引入了一种新的基于 QA 的自动评估指标，与新数据集、跨不同人工模板以及 TIFA160 上的现有指标相比，该指标与人工评分的相关性更好。]]></description>
      <guid>https://arxiv.org/abs/2404.16820</guid>
      <pubDate>Fri, 26 Apr 2024 04:37:04 GMT</pubDate>
    </item>
    <item>
      <title>逐一列出项目：多模式法学硕士的新数据源和学习范式</title>
      <link>https://arxiv.org/abs/2404.16375</link>
      <description><![CDATA[标记集 (SoM) 提示通过使模型能够将视觉对象与图像上插入的标签相关联，释放 GPT-4V 的视觉基础功能。这些标签用字母数字标记，可以通过文本标记进行索引，以便于参考。尽管 GPT-4V 具有非凡的性能，但我们观察到其他多模态大型语言模型 (MLLM) 很难理解这些视觉标签。为了促进开源模型的 SoM 学习，我们提出了一种新的学习范式：“逐一列出项目”，它要求模型按照标签的字母数字顺序枚举和描述放置在图像上的所有视觉标签。通过将我们策划的数据集与其他视觉指令调整数据集集成，我们能够为现有的 MLLM 配备 SoM 提示能力。此外，我们还在五个 MLLM 基准上评估了经过微调的 SoM 模型。我们发现这个新的数据集，即使是相对较小的数据集（带有标签的 10k-30k 图像），也能显着增强视觉推理能力并减少 MLLM 的幻觉。也许令人惊讶的是，即使在推理过程中从输入图像中省略视觉标签，这些改进仍然存在。这表明“逐一列出项目”作为训练 MLLM 的新范例的潜力，它通过在训练阶段使用视觉标签来加强对象-文本对齐。最后，我们通过探索训练好的模型进行分析，以了解 SoM 的工作机制。我们的代码和数据可在 https://github.com/zzxslp/SoM-LLaVA 获取。]]></description>
      <guid>https://arxiv.org/abs/2404.16375</guid>
      <pubDate>Fri, 26 Apr 2024 04:27:21 GMT</pubDate>
    </item>
    <item>
      <title>NeRF-XL：使用多个 GPU 扩展 NeRF</title>
      <link>https://arxiv.org/abs/2404.16221</link>
      <description><![CDATA[我们提出了 NeRF-XL，这是一种在多个 GPU 上分布神经辐射场 (NeRF) 的原理方法，从而能够以任意大的容量训练和渲染 NeRF。我们首先重新审视现有的多 GPU 方法，这些方法将大场景分解为多个独立训练的 NeRF，并确定这些方法的几个基本问​​题，这些问题阻碍了重建质量的提高，因为在训练中使用了额外的计算资源 (GPU)。 NeRF-XL 解决了这些问题，只需使用更多硬件即可使用任意数量的参数来训练和渲染 NeRF。我们方法的核心在于一种新颖的分布式训练和渲染公式，它在数学上等同于经典的单 GPU 情况，并最大限度地减少了 GPU 之间的通信。通过解锁具有任意大参数数量的 NeRF，我们的方法是第一个揭示 NeRF 的多 GPU 缩放定律的方法，显示出通过更大的参数数量可以提高重建质量，并通过更多 GPU 来提高速度。我们展示了 NeRF-XL 在各种数据集上的有效性，包括迄今为止最大的开源数据集 MatrixCity，其中包含覆盖 25km^2 城市区域的 258K 图像。]]></description>
      <guid>https://arxiv.org/abs/2404.16221</guid>
      <pubDate>Fri, 26 Apr 2024 04:11:25 GMT</pubDate>
    </item>
    <item>
      <title>ConsolidatedID：具有多模式细粒度身份保护的肖像生成</title>
      <link>https://arxiv.org/abs/2404.16771</link>
      <description><![CDATA[基于扩散的技术已经取得了重大进展，特别是在个性化和定制的面部生成方面。然而，现有方法在实现高保真和详细的身份（ID）一致性方面面临挑战，这主要是由于对面部区域的细粒度控制不足，并且缺乏充分考虑复杂的面部细节和整体面部的全面的身份保存策略。为了解决这些限制，我们引入了 ConcientID，这是一种创新方法，专门用于在细粒度多模态面部提示下生成多样化身份保留的肖像，仅利用单个参考图像。 ConsolidatedID 包含两个关键组件：一个多模态面部提示生成器，它结合了面部特征、相应的面部描述和整体面部上下文，以提高面部细节的精确度；以及通过面部注意力定位策略优化的 ID 保存网络，旨在保持 ID 一致性在面部区域。这些组件通过引入来自面部区域的细粒度多模态 ID 信息，显着提高了 ID 保存的准确性。为了促进ConstantID的训练，我们提出了一个细粒度的肖像数据集FGID，其中包含超过500,000张面部图像，比现有的公共面部数据集提供了更大的多样性和全面性。 % 例如 LAION-Face、CelebA、FFHQ 和 SFHQ。实验结果证实，我们的 ConcientID 在个性化面部生成方面实现了卓越的精度和多样性，超越了 MyStyle 数据集中的现有方法。此外，虽然ConstantID引入了更多的多模态ID信息，但它在生成过程中保持了快速的推理速度。]]></description>
      <guid>https://arxiv.org/abs/2404.16771</guid>
      <pubDate>Fri, 26 Apr 2024 03:34:58 GMT</pubDate>
    </item>
    <item>
      <title>让你的法学硕士充分利用环境</title>
      <link>https://arxiv.org/abs/2404.16811</link>
      <description><![CDATA[尽管许多当代大型语言模型（LLM）可以处理冗长的输入，但它们仍然难以充分利用长上下文中的信息，这被称为“迷失在中间的挑战”。我们假设这是由于在长上下文训练过程中缺乏明确的监督，没有强调长上下文中的任何位置都可以保存关键信息。基于这种直觉，我们的研究提出了信息密集型（IN2）训练，这是一种纯粹的数据驱动解决方案，可以克服中间迷失的问题。具体来说，IN2 训练利用合成的长上下文问答数据集，其中答案需要 (1) 对合成的长上下文（4K-32K 标记）内的短片段（~128 个标记）进行细粒度的信息感知，并且（ 2）对两个或多个短片段的信息进行整合和推理。通过在 Mistral-7B 上应用这种信息密集型训练，我们提出了 FILM-7B（FILl-in-the-Middle）。为了彻底评估 FILM-7B 利用长上下文的能力，我们设计了三个探测任务，涵盖各种上下文样式（文档、代码和结构化数据上下文）和信息检索模式（前向、后向和双向检索） 。探测结果表明 FILM-7B 可以从其 32K 上下文窗口中的不同位置稳健地检索信息。除了这些探测任务之外，FILM-7B 还显着提高了现实世界长上下文任务的性能（例如，NarrativeQA 上的 F1 分数为 23.5-&gt;26.9），同时在短上下文任务上保持了可比较的性能（例如，59.3-&gt; ；MMLU 精度为 59.2）。 Github 链接：https://github.com/microsoft/FILM。]]></description>
      <guid>https://arxiv.org/abs/2404.16811</guid>
      <pubDate>Fri, 26 Apr 2024 03:20:06 GMT</pubDate>
    </item>
    <item>
      <title>层跳跃：启用提前退出推理和自推测解码</title>
      <link>https://arxiv.org/abs/2404.16710</link>
      <description><![CDATA[我们推出了 LayerSkip，这是一种用于加速大型语言模型 (LLM) 推理的端到端解决方案。首先，在训练期间，我们应用层丢失，早期层的丢失率较低，后面层的丢失率较高，以及所有变压器层共享相同出口的早期退出损失。其次，在推理过程中，我们表明该训练方法提高了早期层提前退出的准确性，而无需向模型添加任何辅助层或模块。第三，我们提出了一种新颖的自推测解码解决方案，我们在早期层退出并验证和纠正模型的其余层。我们提出的自推测解码方法比其他推测解码方法具有更少的内存占用，并且受益于草稿和验证阶段的共享计算和激活。我们在不同类型的训练中对不同大小的 Llama 模型进行实验：从头开始预训练、持续预训练、特定数据域的微调以及特定任务的微调。我们实现了推理解决方案，并在 CNN/DM 文档的摘要方面显示了高达 2.16 倍的加速，在编码方面加速了 1.82 倍，在 TOPv2 语义解析任务上加速了 2.0 倍。]]></description>
      <guid>https://arxiv.org/abs/2404.16710</guid>
      <pubDate>Fri, 26 Apr 2024 02:58:11 GMT</pubDate>
    </item>
    <item>
      <title>Tele-FLM 技术报告</title>
      <link>https://arxiv.org/abs/2404.16645</link>
      <description><![CDATA[大型语言模型（LLM）展示了语言理解和生成方面的深厚能力，促进了广泛的应用。然而，如何以最小的试错成本和计算资源有效地将 LLM 扩展到超过 500 亿个参数，详细的开源方法明显缺乏。在本报告中，我们介绍了 Tele-FLM（又名 FLM-2），这是一个 52B 开源多语言大语言模型，具有稳定、高效的预训练范式和增强的事实判断能力。 Tele-FLM 展示了卓越的多语言语言建模能力，通过 BPB 在文本语料库上进行测量。此外，在英文和中文基础模型评估中，它可以与预训练 FLOP 较大的强大开源模型（如 Llama2-70B 和 DeepSeek-67B）相媲美。除了模型权重之外，我们还分享核心设计、工程实践和培训细节，希望学术界和工业界都能受益。]]></description>
      <guid>https://arxiv.org/abs/2404.16645</guid>
      <pubDate>Fri, 26 Apr 2024 02:56:14 GMT</pubDate>
    </item>
    <item>
      <title>我们距离 GPT-4V 还有多远？利用开源套件缩小与商业多模态模型的差距</title>
      <link>https://arxiv.org/abs/2404.16821</link>
      <description><![CDATA[在本报告中，我们介绍了 InternVL 1.5，这是一种开源多模态大语言模型 (MLLM)，旨在弥合开源模型和专有商业模型在多模态理解方面的能力差距。我们介绍三个简单的改进：（1）强视觉编码器：我们为大规模视觉基础模型——InternViT-6B探索了一种持续学习策略，提高了其视觉理解能力，并使其可以在不同的LLM中迁移和重用。 (2)动态高分辨率：根据输入图像的长宽比和分辨率，将图像划分为1到40个448×448像素的图块，最高支持4K分辨率输入。 （3）高质量的双语数据集：我们精心收集了高质量的双语数据集，涵盖常见场景、文档图像，并用英文和中文问答对对其进行注释，显着提高了 OCR 和中文相关任务的性能。我们通过一系列基准测试和比较研究来评估 InternVL 1.5。与开源和专有模型相比，InternVL 1.5 显示出具有竞争力的性能，在 18 个基准测试中的 8 个中取得了最先进的结果。代码已发布于 https://github.com/OpenGVLab/InternVL。]]></description>
      <guid>https://arxiv.org/abs/2404.16821</guid>
      <pubDate>Fri, 26 Apr 2024 02:48:55 GMT</pubDate>
    </item>
    <item>
      <title>SEED-Bench-2-Plus：使用富文本视觉理解对多模式大型语言模型进行基准测试</title>
      <link>https://arxiv.org/abs/2404.16790</link>
      <description><![CDATA[理解富含文本的视觉内容对于多模态大语言模型（MLLM）的实际应用至关重要，因为丰富文本的场景在现实世界中无处不在，其特点是图像中嵌入了大量文本。最近，具有令人印象深刻的多功能性的 MLLM 的出现提高了我们对 MLLM 的期望标准。然而，他们在文本丰富的场景中的熟练程度尚未得到全面、客观的评估，因为当前的 MLLM 基准主要侧重于评估一般视觉理解能力。在这项工作中，我们引入了 SEED-Bench-2-Plus，这是一个专门为评估 MLLM 的丰富文本视觉理解能力而设计的基准。我们的基准测试包括 2.3K 个带有精确人工注释的多项选择题，涵盖三大类：图表、地图和网络，每一类都涵盖了现实世界中各种文本丰富的场景。这些类别由于其固有的复杂性和多样性，有效地模拟了现实世界中文本丰富的环境。我们进一步对 34 个著名的 MLLM（包括 GPT-4V、Gemini-Pro-Vision 和 Claude-3-Opus）进行了全面评估，并强调了 MLLM 目前在文本丰富的视觉理解方面的局限性。我们希望我们的工作能够成为现有 MLLM 基准的有价值的补充，提供富有洞察力的观察结果并启发在 MLLM 的文本丰富的视觉理解领域进行进一步的研究。数据集和评估代码可以在 https://github.com/AILab-CVC/SEED-Bench 访问。]]></description>
      <guid>https://arxiv.org/abs/2404.16790</guid>
      <pubDate>Fri, 26 Apr 2024 02:35:35 GMT</pubDate>
    </item>
    <item>
      <title>Interactive3D：通过交互式 3D 生成创建您想要的内容</title>
      <link>https://arxiv.org/abs/2404.16510</link>
      <description><![CDATA[3D 对象生成取得了显着进步，产生了高质量的结果。然而，无法实现精确的用户控制，通常会产生与用户期望不符的结果，从而限制了它们的适用性。由于交互能力有限，用户想象的 3D 对象生成在使用当前的生成模型实现其概念时面临着重大挑战。现有方法主要提供两种方法：（i）解释具有受限可控性的文本指令，或（ii）从 2D 图像重建 3D 对象。它们都将定制限制在 2D 参考的范围内，并可能在 3D 提升过程中引入不良伪影，从而限制了直接和通用 3D 修改的范围。在这项工作中，我们介绍了 Interactive3D，这是一种用于交互式 3D 生成的创新框架，可让用户通过广泛的 3D 交互功能精确控制生成过程。 Interactive3D 通过两个级联阶段构建，利用不同的 3D 表示。第一阶段采用高斯溅射进行直接用户交互，允许在任何中间步骤修改和指导生成方向，通过（i）添加和删除组件，（ii）可变形和刚性拖动，（iii）几何变换，以及（iv）语义编辑。随后，高斯图被转换为 InstantNGP。我们引入了一种新颖的 (v) 交互式哈希细化模块，以在第二阶段进一步添加细节并提取几何图形。我们的实验表明，Interactive3D 显着提高了 3D 生成的可控性和质量。我们的项目网页位于 https://interactive-3d.github.io/。]]></description>
      <guid>https://arxiv.org/abs/2404.16510</guid>
      <pubDate>Fri, 26 Apr 2024 01:51:11 GMT</pubDate>
    </item>
    </channel>
</rss>