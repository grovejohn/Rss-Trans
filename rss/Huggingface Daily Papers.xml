<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>拥抱日报</title>
    <link>https://huggingface.co/papers</link>
    <description>Huggingface Daily Papers - RSSHub 用爱制作的(https://github.com/DIYgod/RSSHub)</description>
    <lastBuildDate>Sun, 07 Apr 2024 06:14:24 GMT</lastBuildDate>
    <item>
      <title>RALL-E：具有文本转语音合成思想链提示的鲁棒编解码器语言建模</title>
      <link>https://arxiv.org/abs/2404.03204</link>
      <description><![CDATA[我们提出了 RALL-E，一种用于文本到语音 (TTS) 合成的强大语言建模方法。虽然之前基于大语言模型 (LLM) 的工作在零样本 TTS 上表现出了令人印象深刻的性能，但此类方法通常稳健性较差，例如韵律不稳定（怪异的音高和节奏/持续时间）和较高的单词错误率 (WER)，由于语言模型的自回归预测风格。 RALL-E背后的核心思想是思想链（CoT）提示，它将任务分解为更简单的步骤，以增强基于LLM的TTS的稳健性。为了实现这个想法，RALL-E 首先预测输入文本的韵律特征（音高和持续时间），并将它们用作中间条件来预测 CoT 风格的语音标记。其次，RALL-E利用预测的持续时间提示来指导Transformer中自注意力权重的计算，以强制模型在预测语音标记时关注相应的音素和韵律特征。综合客观和主观评估的结果表明，与强大的基线方法 VALL-E 相比，RALL-E 显着提高了零样本 TTS 的 WER，从 6.3%（没有重新排序）和 2.1%（有重新排序）提高到 2.8%，并且分别为 1.0%。此外，我们证明 RALL-E 可以正确合成 VALL-E 难以理解的句子，并将错误率从 68% 降低到 4%。]]></description>
      <guid>https://arxiv.org/abs/2404.03204</guid>
      <pubDate>Fri, 05 Apr 2024 03:00:34 GMT</pubDate>
    </item>
    <item>
      <title>MiniGPT4-Video：利用交错的视觉文本标记推进多模态法学硕士的视频理解</title>
      <link>https://arxiv.org/abs/2404.03413</link>
      <description><![CDATA[本文介绍了 MiniGPT4-Video，这是一种专为视频理解而设计的多模态大语言模型 (LLM)。该模型能够处理时间视觉和文本数据，使其擅长理解视频的复杂性。 MiniGPT-v2 擅长将视觉特征转化为单个图像的 LLM 空间，并在各种图像文本基准上取得了令人印象深刻的结果，基于 MiniGPT-v2 的成功，本文扩展了模型处理帧序列的能力，使其能够理解视频。 MiniGPT4-video 不仅考虑视觉内容，还包含文本对话，使模型能够有效地回答涉及视觉和文本组件的查询。所提出的模型优于现有的最先进方法，在 MSVD、MSRVTT、TGIF 和 TVQA 基准上分别获得了 4.22%、1.13%、20.82% 和 13.1% 的增益。我们的模型和代码已在此处公开发布 https://vision-cair.github.io/MiniGPT4-video/]]></description>
      <guid>https://arxiv.org/abs/2404.03413</guid>
      <pubDate>Fri, 05 Apr 2024 02:56:58 GMT</pubDate>
    </item>
    <item>
      <title>LVLM-Intrepret：大型视觉语言模型的可解释性工具</title>
      <link>https://arxiv.org/abs/2404.03118</link>
      <description><![CDATA[在快速发展的人工智能领域，多模态大语言模型正在成为一个重要的关注领域。这些结合了各种形式的数据输入的模型正变得越来越流行。然而，了解其内部机制仍然是一项复杂的任务。可解释性工具和机制领域已经取得了许多进展，但仍有很多值得探索的地方。在这项工作中，我们提出了一种新颖的交互式应用程序，旨在理解大型视觉语言模型的内部机制。我们的界面旨在增强图像块的可解释性，这有助于生成答案，并评估语言模型将其输出基于图像的有效性。通过我们的应用程序，用户可以系统地研究模型并发现系统限制，为增强系统功能铺平道路。最后，我们提出了一个案例研究，说明我们的应用程序如何帮助理解流行的大型多模态模型中的故障机制：LLaVA。]]></description>
      <guid>https://arxiv.org/abs/2404.03118</guid>
      <pubDate>Fri, 05 Apr 2024 02:52:23 GMT</pubDate>
    </item>
    <item>
      <title>PointInfinity：分辨率不变点扩散模型</title>
      <link>https://arxiv.org/abs/2404.03566</link>
      <description><![CDATA[我们提出了 PointInfinity，一个高效的点云扩散模型系列。我们的核心思想是使用基于转换器的架构，具有固定大小、分辨率不变的潜在表示。这使得能够使用低分辨率点云进行高效训练，同时允许在推理过程中生成高分辨率点云。更重要的是，我们表明将测试时间分辨率扩展到训练分辨率之外可以提高生成的点云和表面的保真度。我们分析了这种现象，并与扩散模型中常用的无分类器指导建立了联系，证明两者都允许在推理过程中权衡保真度和可变性。 CO3D 上的实验表明，PointInfinity 可以高效生成具有最先进质量的高分辨率点云（最多 131k 点，是 Point-E 的 31 倍）。]]></description>
      <guid>https://arxiv.org/abs/2404.03566</guid>
      <pubDate>Fri, 05 Apr 2024 02:48:21 GMT</pubDate>
    </item>
    <item>
      <title>ReFT：语言模型的表示微调</title>
      <link>https://arxiv.org/abs/2404.03592</link>
      <description><![CDATA[参数高效微调（PEFT）方法寻求通过更新少量权重来适应大型模型。然而，许多先前的可解释性工作表明，表示编码了丰富的语义信息，这表明编辑表示可能是一种更强大的替代方案。在这里，我们通过开发一系列表示微调（ReFT）方法来追求这一假设。 ReFT 方法在冻结的基础模型上运行，并学习对隐藏表示的特定任务干预。我们定义了 ReFT 系列的一个强大实例，即低秩线性子空间 ReFT (LoReFT)。 LoReFT 是现有 PEFT 的直接替代品，其学习干预措施的参数效率比之前最先进的 PEFT 高 10 至 50 倍。我们展示了 LoReFT 在八个常识推理任务、四个算术推理任务、Alpaca-Eval v1.0 和 GLUE 上的表现。在所有这些评估中，LoReFT 提供了效率和性能的最佳平衡，并且几乎总是优于最先进的 PEFT。我们在 https://github.com/stanfordnlp/pyreft 公开发布了通用 ReFT 训练库。]]></description>
      <guid>https://arxiv.org/abs/2404.03592</guid>
      <pubDate>Fri, 05 Apr 2024 02:45:06 GMT</pubDate>
    </item>
    <item>
      <title>通过神经压缩文本训练法学硕士</title>
      <link>https://arxiv.org/abs/2404.03626</link>
      <description><![CDATA[在本文中，我们探讨了在高度压缩的文本上训练大型语言模型（LLM）的想法。虽然标准子词标记器以较小的因子压缩文本，但神经文本压缩器可以实现更高的压缩率。如果可以直接在神经压缩文本上训练法学硕士，这将在训练和服务效率方面带来优势，并且更容易处理长文本跨度。这一目标的主要障碍是强压缩往往会产生不透明的输出，不太适合学习。特别是，我们发现法学硕士不容易学习通过算术编码自然压缩的文本。为了克服这个问题，我们提出了 Equal-Info Windows，这是一种新颖的压缩技术，将文本分割成块，每个块压缩到相同的位使用这种方法，我们展示了对神经压缩文本的有效学习，该文本随着规模的扩大而提高，并且在困惑度和推理速度基准上大大优于字节级基线。虽然我们的方法为使用相同的参数数量，它的优点是序列长度更短。更短的序列长度需要更少的自回归生成步骤，并减少延迟。最后，我们对有助于可学习性的属性进行了广泛的分析，并为如何进一步提高可学习性提供了具体建议高压缩分词器的性能。]]></description>
      <guid>https://arxiv.org/abs/2404.03626</guid>
      <pubDate>Fri, 05 Apr 2024 02:42:33 GMT</pubDate>
    </item>
    <item>
      <title>AutoWebGLM：引导和强化基于大型语言模型的 Web 导航代理</title>
      <link>https://arxiv.org/abs/2404.03648</link>
      <description><![CDATA[大型语言模型 (LLM) 推动了许多智能代理任务，例如网络导航，但由于以下三个因素，大多数现有代理在现实网页中的表现远远不能令人满意：(1) 网页上操作的多功能性，(2) HTML 文本超出模型处理能力，以及 (3) 由于 Web 的开放域性质而导致决策的复杂性。鉴于这一挑战，我们开发了 AutoWebGLM，这是一种基于 ChatGLM3-6B 构建的、性能优于 GPT-4 的自动 Web 导航代理。受人类浏览模式的启发，我们设计了一种 HTML 简化算法来表示网页，简洁地保留重要信息。我们采用混合人类人工智能方法来构建用于课程培训的网络浏览数据。然后，我们通过强化学习和拒绝采样来引导模型，以进一步促进网页理解、浏览器操作和高效的任务分解。为了进行测试，我们为现实世界的网页浏览任务建立了一个双语基准——AutoWebBench。我们通过不同的网络导航基准评估 AutoWebGLM，揭示其改进，但也揭示了应对真实环境的潜在挑战。相关代码、模型和数据将在https://github.com/THUDM/AutoWebGLM发布。]]></description>
      <guid>https://arxiv.org/abs/2404.03648</guid>
      <pubDate>Fri, 05 Apr 2024 02:39:20 GMT</pubDate>
    </item>
    <item>
      <title>红队 GPT-4V：GPT-4V 能否安全抵御单/多模式越狱攻击？</title>
      <link>https://arxiv.org/abs/2404.03411</link>
      <description><![CDATA[人们对大型语言模型 (LLM) 进行了各种越狱攻击，并揭示了 LLM 的脆弱保护措施。此外，一些方法不限于文本模态，通过扰乱视觉输入将越狱攻击扩展到多模态大型语言模型（MLLM）。然而，缺乏通用的评估基准使性能再现和公平比较变得复杂。此外，缺乏对闭源最先进（SOTA）模型的全面评估，特别是MLLM，例如GPT-4V。为了解决这些问题，这项工作首先构建了一个全面的越狱评估数据集，其中包含涵盖 11 种不同安全政策的 1445 个有害问题。基于此数据集，对 11 个不同的 LLM 和 MLLM 进行了广泛的红队实验，包括 SOTA 专有模型和开源模型。然后，我们对评估结果进行深入分析，发现（1）与开源 LLM 和 MLLM 相比，GPT4 和 GPT-4V 表现出更好的抵御越狱攻击的鲁棒性。 (2) 与其他开源模型相比，Llama2 和 Qwen-VL-Chat 更加稳健。 (3)与文本越狱方法相比，视觉越狱方法的可移植性相对有限。数据集和代码可以在这里找到 https://anonymous.4open.science/r/red_teaming_gpt4-C1CE/README.md 。]]></description>
      <guid>https://arxiv.org/abs/2404.03411</guid>
      <pubDate>Fri, 05 Apr 2024 02:35:20 GMT</pubDate>
    </item>
    <item>
      <title>CodeEditorBench：评估大型语言模型的代码编辑能力</title>
      <link>https://arxiv.org/abs/2404.03543</link>
      <description><![CDATA[代码的大型语言模型 (LLM) 正在迅速发展，代码编辑逐渐成为一项关键功能。我们推出了 CodeEditorBench，这是一个评估框架，旨在严格评估法学硕士在代码编辑任务中的表现，包括调试、翻译、润色和需求切换。与仅关注代码生成的现有基准测试不同，CodeEditorBench 强调软件开发的真实场景和实际方面。我们从五个来源策划各种编码挑战和场景，涵盖各种编程语言、复杂程度和编辑任务。对 19 个法学硕士的评估表明，闭源模型（特别是 Gemini-Ultra 和 GPT-4）在 CodeEditorBench 中的表现优于开源模型，突出了基于问题类型和提示敏感性的模型性能差异。 CodeEditorBench 旨在通过提供一个用于评估代码编辑能力的强大平台来促进法学硕士的进步。我们将发布所有提示和数据集，以使社区能够扩展数据集并对新兴法学硕士进行基准测试。通过引入 CodeEditorBench，我们为法学硕士在代码编辑方面的进步做出了贡献，并为研究人员和从业者提供了宝贵的资源。]]></description>
      <guid>https://arxiv.org/abs/2404.03543</guid>
      <pubDate>Fri, 05 Apr 2024 02:32:28 GMT</pubDate>
    </item>
    <item>
      <title>CoMat：将文本到图像扩散模型与图像到文本概念匹配对齐</title>
      <link>https://arxiv.org/abs/2404.03653</link>
      <description><![CDATA[扩散模型在文本到图像生成领域取得了巨大成功。然而，减轻文本提示和图像之间的错位仍然具有挑战性。未对准背后的根本原因尚未得到广泛调查。我们观察到这种错位是由于令牌注意力激活不足引起的。我们进一步将这种现象归因于扩散模型的条件利用不足，这是由其训练范式造成的。为了解决这个问题，我们提出了 CoMat，一种具有图像到文本概念匹配机制的端到端扩散模型微调策略。我们利用图像字幕模型来测量图像到文本的对齐情况，并指导扩散模型重新访问被忽略的标记。还提出了一种新颖的属性集中模块来解决属性绑定问题。在没有任何图像或人类偏好数据的情况下，我们仅使用 20K 文本提示来微调 SDXL 以获得 CoMat-SDXL。大量实验表明，CoMat-SDXL 在两个文本到图像对齐基准测试中显着优于基线模型 SDXL，并实现了最先进的性能。]]></description>
      <guid>https://arxiv.org/abs/2404.03653</guid>
      <pubDate>Fri, 05 Apr 2024 02:25:54 GMT</pubDate>
    </item>
    </channel>
</rss>