<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>拥抱日报</title>
    <link>https://huggingface.co/papers</link>
    <description>Huggingface Daily Papers - 由 RSSHub 用爱制作（https://github.com/DIYgod/RSSHub）</description>
    <lastBuildDate>Mon, 01 Apr 2024 07:12:28 GMT</lastBuildDate>
    <item>
      <title>Jamba：混合 Transformer-Mamba 语言模型</title>
      <link>https://arxiv.org/abs/2403.19887</link>
      <description><![CDATA[我们推出了 Jamba，这是一种基于新型混合 Transformer-Mamba 专家混合 (MoE) 架构的新基础大型语言模型。具体来说，Jamba 将 Transformer 层和 Mamba 层的块交错在一起，享受这两个模型系列的优点。 MoE 添加到其中一些层中，以提高模型容量，同时保持活动参数使用的可控性。这种灵活的架构允许特定于资源和目标的配置。在我们实现的特定配置中，我们最终得到了一个适合单个 80GB GPU 的强大模型。与普通 Transformer 相比，Jamba 是大规模构建的，可提供高吞吐量和较小的内存占用，同时在标准语言模型基准测试和长上下文评估上具有最先进的性能。值得注意的是，该模型在高达 256K 令牌上下文长度的情况下呈现出强劲的结果。我们研究了各种架构决策，例如如何组合 Transformer 和 Mamba 层，以及如何混合专家，并表明其中一些决策对于大规模建模至关重要。我们还描述了 Jamba 的训练和评估所揭示的这些架构的几个有趣的属性，并计划从各种消融运行中发布检查点，以鼓励进一步探索这种新颖的架构。我们在许可下公开 Jamba 实施的权重。]]></description>
      <guid>https://arxiv.org/abs/2403.19887</guid>
      <pubDate>Mon, 01 Apr 2024 03:49:48 GMT</pubDate>
    </item>
    <item>
      <title>InstantSplat：40 秒内无界稀疏视图无姿势高斯泼溅</title>
      <link>https://arxiv.org/abs/2403.20309</link>
      <description><![CDATA[虽然新颖的视图合成 (NVS) 在 3D 计算机视觉领域取得了实质性进展，但它通常需要从密集视点对相机内部和外部进行初步估计。这种预处理通常通过运动结构 (SfM) 管道进行，该过程可能缓慢且不可靠，特别是在匹配特征不足以进行精确重建的稀疏视图场景中。在这项工作中，我们将基于点的表示（例如 3D Gaussian Splatting、3D-GS）的优势与端到端密集立体模型（DUSt3R）相结合，以解决无约束设置下 NVS 中复杂但尚未解决的问题，这包括无姿势和稀疏视图挑战。我们的框架 InstantSplat 将密集立体先验与 3D-GS 相结合，从稀疏视图和稀疏视图中构建大规模场景的 3D 高斯模型。不到 1 分钟即可获得无姿势图像。具体来说，InstantSplat 包含一个粗略几何初始化 (CGI) 模块，该模块利用从预训练的密集立体管道导出的全局对齐 3D 点图，快速建立所有训练视图中的初步场景结构和相机参数。接下来是快速 3D 高斯优化 (F-3DGO) 模块，该模块通过姿势正则化联合优化 3D 高斯属性和初始化姿势。在大型户外坦克和大型坦克上进行的实验Temples 数据集表明，InstantSplat 显着改进了 SSIM（提高了 32%），同时将绝对轨迹误差 (ATE) 降低了 80%。这些使 InstantSplat 成为涉及无姿势和稀疏视图条件的场景的可行解决方案。项目页面：instantsplat.github.io。]]></description>
      <guid>https://arxiv.org/abs/2403.20309</guid>
      <pubDate>Mon, 01 Apr 2024 03:38:01 GMT</pubDate>
    </item>
    <item>
      <title>Snap-it、Tap-it、Splat-it：用于重建具有挑战性的表面的触觉通知 3D 高斯喷射</title>
      <link>https://arxiv.org/abs/2403.20275</link>
      <description><![CDATA[触觉和视觉齐头并进，共同增强我们理解世界的能力。从研究的角度来看，混合触觉和视觉的问题尚未得到充分探索，并提出了有趣的挑战。为此，我们提出了触觉信息 3DGS，这是一种将触摸数据（局部深度图）与多视图视觉数据相结合的新颖方法，以实现表面重建和新颖的视图合成。我们的方法优化了 3D 高斯基元，以准确地对接触点处的物体几何形状进行建模。通过创建一个降低触摸位置透射率的框架，我们实现了精细的表面重建，确保了均匀平滑的深度图。在考虑非朗伯物体（例如闪亮或反射表面）时，触摸特别有用，因为当代方法往往无法以保真度镜面高光进行重建。通过将视觉和触觉传感相结合，我们用比以前的方法更少的图像实现了更准确的几何重建。我们对具有光泽和反射表面的物体进行评估，并证明我们方法的有效性，从而显着提高重建质量。]]></description>
      <guid>https://arxiv.org/abs/2403.20275</guid>
      <pubDate>Mon, 01 Apr 2024 03:29:47 GMT</pubDate>
    </item>
    <item>
      <title>无法解决的问题检测：评估视觉语言模型的可信度</title>
      <link>https://arxiv.org/abs/2403.20331</link>
      <description><![CDATA[本文介绍了视觉语言模型 (VLM) 的一项新颖且重大的挑战，称为不可解决问题检测 (UPD)。 UPD 检查 VLM 在视觉问答 (VQA) 任务中遇到无法解决的问题时保留答案的能力。 UPD 包含三种不同的设置：缺席答案检测 (AAD)、不兼容答案集检测 (IASD) 和不兼容视觉问题检测 (IVQD)。为了深入研究 UPD 问题，大量实验表明，大多数 VLM（包括 GPT-4V 和 LLaVA-Next-34B）都在不同程度上与我们的基准测试相悖，凸显了巨大的改进空间。为了解决 UPD 问题，我们探索了免培训和基于培训的解决方案，并对其有效性和局限性提供了新的见解。我们希望我们的见解以及未来在拟议的 UPD 设置中的努力将增强对更实用和可靠的 VLM 的更广泛的理解和开发。]]></description>
      <guid>https://arxiv.org/abs/2403.20331</guid>
      <pubDate>Mon, 01 Apr 2024 03:12:25 GMT</pubDate>
    </item>
    <item>
      <title>ReALM：参考解析作为语言建模</title>
      <link>https://arxiv.org/abs/2403.20329</link>
      <description><![CDATA[参考解析是一个重要的问题，对于理解和成功处理不同类型的上下文至关重要。此上下文包括先前的轮次和与非会话实体相关的上下文，例如用户屏幕上的实体或在后台运行的实体。虽然法学硕士已被证明对于各种任务都非常强大，但它们在参考解析中的使用，特别是对于非会话实体，仍然没有得到充分利用。本文通过展示如何将引用解析转换为语言建模问题，展示了如何使用法学硕士来创建一个极其有效的系统来解析各种类型的引用，尽管涉及到传统上不利于解决问题的屏幕上的实体形式。被简化为纯文本模式。我们展示了对不同类型参考具有类似功能的现有系统的巨大改进，我们最小的模型在屏幕参考上获得了超过 5% 的绝对增益。我们还针对 GPT-3.5 和 GPT-4 进行了基准测试，我们最小的模型实现了与 GPT-4 相当的性能，而我们较大的模型则远远优于 GPT-4。]]></description>
      <guid>https://arxiv.org/abs/2403.20329</guid>
      <pubDate>Mon, 01 Apr 2024 03:07:34 GMT</pubDate>
    </item>
    </channel>
</rss>