<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>拥抱日报</title>
    <link>https://huggingface.co/papers</link>
    <description>Huggingface Daily Papers - 由 RSSHub 用爱制作（https://github.com/DIYgod/RSSHub）</description>
    <lastBuildDate>Wed, 10 Apr 2024 03:35:54 GMT</lastBuildDate>
    <item>
      <title>OmniFusion 技术报告</title>
      <link>https://arxiv.org/abs/2404.06212</link>
      <description><![CDATA[去年，多模式架构引发了基于人工智能的方法和解决方案的革命，扩展了大型语言模型 (LLM) 的功能。我们提出了一个基于预训练的 LLM 和视觉模态适配器的 OmniFusion 模型。我们评估和比较了几种架构设计原则，以实现更好的文本和视觉数据耦合：MLP 和转换器适配器、各种基于 CLIP ViT 的编码器（SigLIP、InternVIT 等）及其融合方法、图像编码方法（整个图像或图块编码） ）和两个 7B 法学硕士（专有的和开源的 Mistral）。 8 个视觉语言基准测试的实验显示，与类似开源 LLaVA 的解决方案相比，最佳 OmniFusion 设置在不同的 VQA 任务方面得分最高：VizWiz、Pope、MM-Vet、ScienceQA、MMBench、TextVQA、VQAv2、MMMU 。我们还提出了各种情况，OmniFusion 在不同领域提供高度详细的答案：家政、观光、文化、医学、手写和扫描方程识别等。基于 Mistral 的 OmniFusion 模型是一个开源解决方案，具有权重、训练等功能和推理脚本可在 https://github.com/AIRI-Institute/OmniFusion 获取。]]></description>
      <guid>https://arxiv.org/abs/2404.06212</guid>
      <pubDate>Wed, 10 Apr 2024 02:20:20 GMT</pubDate>
    </item>
    <item>
      <title>大象永远不会忘记：大型语言模型中表格数据的记忆和学习</title>
      <link>https://arxiv.org/abs/2404.06209</link>
      <description><![CDATA[虽然许多人已经展示了如何将大型语言模型 (LLM) 应用于各种不同的任务，但数据污染和记忆的关键问题常常被忽视。在这项工作中，我们解决了表格数据的问题。具体来说，我们引入了各种不同的技术来评估语言模型在训练期间是否看到了表格数据集。这项调查表明，法学硕士已经逐字记住了许多流行的表格数据集。然后，我们将训练期间看到的 LLM 在数据集上的小样本学习性能与训练后发布的数据集上的性能进行比较。我们发现法学硕士在训练期间看到的数据集上表现更好，这表明记忆会导致过度拟合。与此同时，法学硕士在新颖的数据集上表现出非凡的性能，并且对数据转换具有惊人的鲁棒性。然后我们调查法学硕士的情境统计学习能力。如果不进行微调，我们会发现它们是有限的。这表明，在新颖数据集上的小样本性能很大程度上归功于法学硕士的世界知识。总的来说，我们的结果强调了测试法学硕士在预训练期间是否看到评估数据集的重要性。我们将我们开发的暴露测试作为 tabmemcheck Python 包提供，网址为 https://github.com/interpretml/LLM-Tabular-Memorization-Checker]]></description>
      <guid>https://arxiv.org/abs/2404.06209</guid>
      <pubDate>Wed, 10 Apr 2024 02:13:53 GMT</pubDate>
    </item>
    <item>
      <title>CodecLM：使语言模型与定制的合成数据保持一致</title>
      <link>https://arxiv.org/abs/2404.05875</link>
      <description><![CDATA[指令调优已成为将大型语言模型 (LLM) 与特定任务指令对齐的关键，从而减少下一个令牌预测目标与用户实际目标之间的差异。为了减少人类收集或注释数据的劳动力和时间成本，研究人员开始探索使用法学硕士来生成指令对齐的合成数据。最近的工作重点是生成多样化的指令并应用 LLM 来增加指令复杂性，通常忽略下游用例。目前尚不清楚如何定制高质量数据，以在不同的目标指令分布和法学硕士中获得更好的指令跟踪能力。为此，我们引入了 CodecLM，这是一个通用框架，用于自适应生成高质量合成数据，用于与不同下游指令分布和 LLM 进行 LLM 对齐。借鉴编码-解码原则，我们使用 LLM 作为编解码器来指导数据生成过程。我们首先将种子指令编码为元数据，元数据是即时生成的简洁关键字，用于捕获目标指令分布，然后解码元数据以创建定制指令。我们还在解码过程中引入了自量规和对比过滤，以定制数据高效的样本。根据基准对四个开放域指令进行的广泛实验验证了 CodecLM 相对于当前最先进技术的有效性。]]></description>
      <guid>https://arxiv.org/abs/2404.05875</guid>
      <pubDate>Wed, 10 Apr 2024 02:08:56 GMT</pubDate>
    </item>
    <item>
      <title>SambaLingo：教授大型语言模型新语言</title>
      <link>https://arxiv.org/abs/2404.05829</link>
      <description><![CDATA[尽管法学硕士广泛存在，但其跨不同语言的能力和可用性仍然存在很大差距。解决这些问题的一种方法是采用现有的预训练法学硕士并继续对其进行新语言的培训。虽然之前的工作已经尝试了语言适应，但尚未涵盖有关最佳实践和方法的许多问题。在本文中，我们对法学硕士对新语言的适应进行了全面的调查。我们的研究涵盖了这个过程中的关键组成部分，包括词汇扩展、直接偏好优化以及低资源语言中人类对齐的数据稀缺问题。我们将这些实验扩展到 9 种语言和 2 个参数尺度（7B 和 70B）。我们将我们的模型与 Llama 2、Aya-101、XGLM、BLOOM 和现有语言专家进行比较，优于所有先前发布的基线。此外，所有评估代码和检查点都是公开的，以方便未来的研究。]]></description>
      <guid>https://arxiv.org/abs/2404.05829</guid>
      <pubDate>Wed, 10 Apr 2024 02:03:46 GMT</pubDate>
    </item>
    <item>
      <title>LLM2Vec：大型语言模型是秘密强大的文本编码器</title>
      <link>https://arxiv.org/abs/2404.05961</link>
      <description><![CDATA[大型纯解码器语言模型 (LLM) 是当今大多数 NLP 任务和基准测试中最先进的模型。然而，社区只是慢慢地将这些模型用于文本嵌入任务，这需要丰富的上下文表示。在这项工作中，我们引入了 LLM2Vec，这是一种简单的无监督方法，可以将任何仅解码器的 LLM 转换为强大的文本编码器。 LLM2Vec 包含三个简单步骤：1）启用双向注意力，2）屏蔽下一个标记预测，3）无监督对比学习。我们通过将 LLM2Vec 应用于从 1.3B 到 7B 参数的 3 个流行的 LLM 来证明 LLM2Vec 的有效性，并评估英语单词和序列级任务的转换模型。我们在字级任务上远远优于仅编码器模型，并在大规模文本嵌入基准（MTEB）上达到了新的无监督最先进性能。此外，当将 LLM2Vec 与监督对比学习相结合时，我们在仅使用公开数据进行训练的模型中在 MTEB 上实现了最先进的性能。我们强有力的实证结果和广泛的分析表明，LLM 可以以参数有效的方式有效地转换为通用文本编码器，而不需要昂贵的适应或合成 GPT-4 生成的数据。]]></description>
      <guid>https://arxiv.org/abs/2404.05961</guid>
      <pubDate>Wed, 10 Apr 2024 01:56:06 GMT</pubDate>
    </item>
    <item>
      <title>InternLM-XComposer2-4KHD：开创性的大型视觉语言模型，可处理从 336 像素到 4K 高清的分辨率</title>
      <link>https://arxiv.org/abs/2404.06512</link>
      <description><![CDATA[大视觉语言模型（LVLM）领域已经取得了显着的进步，但由于分辨率有限，其发展受到了理解细粒度视觉内容方面的挑战的阻碍。最近的努力旨在增强 LVLM 的高分辨率理解能力，但它们的上限仍然约为 1500 x 1500 像素，并且分辨率范围相对较窄。本文介绍了 InternLM-XComposer2-4KHD，这是对将 LVLM 分辨率能力提升至 4K 高清 (3840 x 1600) 及以上的突破性探索。同时，考虑到并非所有场景都需要超高分辨率，它支持从336像素到4K标准的多种分辨率，大大拓宽了其适用范围。具体来说，这项研究通过引入一种新颖的扩展来推进补丁划分范式：具有自动补丁配置的动态分辨率。它保持训练图像的长宽比，同时根据预先训练的视觉变换器 (ViT) (336 x 336) 自动改变补丁数量和配置布局，从而实现从 336 像素到 4K 标准的动态训练分辨率。我们的研究表明，将训练分辨率扩展到 4K 高清可以带来一致的性能增强，而不会达到潜在改进的上限。 InternLM-XComposer2-4KHD 在 16 项基准测试中的 10 项中表现出卓越的性能，可与 GPT-4V 和 Gemini Pro 相媲美甚至超越。具有 7B 参数的 InternLM-XComposer2-4KHD 模型系列可在 https://github.com/InternLM/InternLM-XComposer 上公开获取。]]></description>
      <guid>https://arxiv.org/abs/2404.06512</guid>
      <pubDate>Wed, 10 Apr 2024 01:48:52 GMT</pubDate>
    </item>
    <item>
      <title>MuPT：生成符号音乐预训练变压器</title>
      <link>https://arxiv.org/abs/2404.06393</link>
      <description><![CDATA[在本文中，我们探讨了大型语言模型（LLM）在音乐预训练中的应用。虽然 MIDI 在音乐建模中的普遍使用已经得到证实，但我们的研究结果表明，LLM 本质上与 ABC 表示法更兼容，这与其设计和优势更紧密地结合在一起，从而增强了模型在音乐创作中的表现。为了解决生成过程中不同曲目的测量不一致所带来的挑战，我们建议开发同步多轨 ABC 表示法（SMT-ABC 表示法），其目的是保持多个音乐曲目之间的一致性。我们的贡献包括一系列能够处理多达 8192 个标记的模型，覆盖了我们训练集中 90% 的符号音乐数据。此外，我们还探讨了符号音乐缩放定律（SMS 定律）对模型性能的影响。结果表明音乐生成的未来研究有一个有前途的方向，通过我们的开源贡献为社区主导的研究提供广泛的资源。]]></description>
      <guid>https://arxiv.org/abs/2404.06393</guid>
      <pubDate>Wed, 10 Apr 2024 01:46:53 GMT</pubDate>
    </item>
    <item>
      <title>Magic-Boost：通过多视图条件扩散增强 3D 生成</title>
      <link>https://arxiv.org/abs/2404.06429</link>
      <description><![CDATA[受益于2D扩散模型的快速发展，3D内容创作最近取得了重大进展。一种有前景的解决方案是对预先训练的 2D 扩散模型进行微调，以利用其生成多视图图像的能力，然后通过快速 NeRF 或大型重建模型等方法将其提升为精确的 3D 模型。然而，由于不一致仍然存在并且生成的分辨率有限，此类方法的生成结果仍然缺乏复杂的纹理和复杂的几何形状。为了解决这个问题，我们提出了 Magic-Boost，这是一种多视图条件扩散模型，通过短暂的 SDS 优化 (sim15min) 显着细化粗略的生成结果。与之前基于文本或单图像的扩散模型相比，Magic-Boost 表现出强大的能力，可以从伪合成多视图图像生成高度一致性的图像。它提供了精确的 SDS 指导，与输入图像的身份很好地匹配，丰富了初始生成结果的几何和纹理的局部细节。大量实验表明，Magic-Boost 极大地增强了粗略输入，并生成具有丰富几何和纹理细节的高质量 3D 资源。 （项目页面：https://magic-research.github.io/magic-boost/）]]></description>
      <guid>https://arxiv.org/abs/2404.06429</guid>
      <pubDate>Wed, 10 Apr 2024 01:36:48 GMT</pubDate>
    </item>
    </channel>
</rss>