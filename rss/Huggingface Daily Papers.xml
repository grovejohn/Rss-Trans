<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>拥抱日报</title>
    <link>https://huggingface.co/papers</link>
    <description>Huggingface Daily Papers - RSSHub 用爱制作的(https://github.com/DIYgod/RSSHub)</description>
    <lastBuildDate>Mon, 08 Apr 2024 11:35:51 GMT</lastBuildDate>
    <item>
      <title>Sigma：用于多模态语义分割的暹罗曼巴网络</title>
      <link>https://arxiv.org/abs/2404.04256</link>
      <description><![CDATA[多模态语义分割显着增强了人工智能代理的感知和场景理解，特别是在弱光或过度曝光环境等不利条件下。利用热和深度等附加模态（X 模态）与传统 RGB 一起提供补充信息，从而实现更强大、更可靠的分割。在这项工作中，我们引入了 Sigma，这是一种利用选择性结构化状态空间模型 Mamba 进行多模态语义分割的 Siamese Mamba 网络。与依赖于局部感受野有限的 CNN 或视觉变换器 (ViT) 的传统方法不同，后者以二次复杂度为代价提供全局感受野，我们的模型以线性复杂度实现了全局感受野覆盖。通过采用 Siamese 编码器并创新 Mamba 融合机制，我们有效地从不同模态中选择重要信息。然后开发解码器来增强模型的通道建模能力。我们的方法 Sigma 在 RGB-Thermal 和 RGB-Depth 分割任务上进行了严格评估，证明了其优越性，并标志着状态空间模型（SSM）在多模态感知任务中的首次成功应用。代码可在 https://github.com/zifuwan/Sigma 获取。]]></description>
      <guid>https://arxiv.org/abs/2404.04256</guid>
      <pubDate>Mon, 08 Apr 2024 09:46:16 GMT</pubDate>
    </item>
    <item>
      <title>鲁棒高斯泼溅</title>
      <link>https://arxiv.org/abs/2404.04211</link>
      <description><![CDATA[在本文中，我们解决了 3D 高斯泼溅 (3DGS) 的常见错误源，包括模糊、不完美的相机姿势和颜色不一致，目的是提高其在手持手机捕获重建等实际应用中的鲁棒性。我们的主要贡献是将运动模糊建模为相机姿势的高斯分布，使我们能够以统一的方式解决相机姿势细化和运动模糊校正问题。此外，我们还提出了散焦模糊补偿机制，以及解决由环境光、阴影或由于相机相关因素（例如不同的白平衡设置）引起的颜色不一致问题。我们提出的解决方案以无缝方式与 3DGS 公式集成，同时保持其在训练效率和渲染速度方面的优势。我们通过实验验证了我们对相关基准数据集（包括 Scannet++ 和 Deblur-NeRF）的贡献，获得了最先进的结果，从而在相关基线上取得了一致的改进。]]></description>
      <guid>https://arxiv.org/abs/2404.04211</guid>
      <pubDate>Mon, 08 Apr 2024 05:35:50 GMT</pubDate>
    </item>
    <item>
      <title>大语言模型的社交技能训练</title>
      <link>https://arxiv.org/abs/2404.04204</link>
      <description><![CDATA[人们依靠解决冲突等社交技能来有效沟通并在工作和个人生活中蓬勃发展。然而，社交技能的练习环境对于大多数人来说通常是遥不可及的。我们如何才能使社交技能培训变得更容易获得、更容易获得、更有吸引力？这篇观点论文借鉴了传播学和心理学的跨学科研究，确定了进入专业领域的社交技能障碍。然后，我们提出了一个解决方案，通过通用框架利用大型语言模型进行社交技能培训。我们的 AI 合作伙伴 AI Mentor 框架将体验式学习与现实实践和定制反馈相结合。这项工作最终需要跨学科创新，以解决对劳动力发展和社会平等的更广泛影响。]]></description>
      <guid>https://arxiv.org/abs/2404.04204</guid>
      <pubDate>Mon, 08 Apr 2024 05:25:27 GMT</pubDate>
    </item>
    <item>
      <title>CantTalkAboutThis：调整语言模型以保持对话主题</title>
      <link>https://arxiv.org/abs/2404.03820</link>
      <description><![CDATA[指令调整数据集的最新进展主要集中在数学或逻辑推理等特定任务上。旨在调整语言模型以保持对话中主题相关性的数据存在显着差距——这是将聊天机器人部署到生产中的一个关键方面。我们引入 CantTalkAboutThis 数据集来帮助语言模型在面向任务的交互过程中保持关注当前的主题。它由来自不同领域的广泛对话主题的综合对话组成。这些对话中穿插着一些干扰性的对话，有意将聊天机器人从预定义的主题上转移开。与 GPT-4-turbo 和 Mixtral-Instruct 等通用指令调整的 LLM 相比，在此数据集上微调语言模型有助于使它们能够适应偏离分配的角色，并提高它们保持主题连贯性的能力。此外，初步观察表明，该数据集上的训练模型还可以提高其在细粒度指令跟踪任务中的性能。]]></description>
      <guid>https://arxiv.org/abs/2404.03820</guid>
      <pubDate>Mon, 08 Apr 2024 04:20:43 GMT</pubDate>
    </item>
    <item>
      <title>AutoWebGLM：引导和强化基于大型语言模型的 Web 导航代理</title>
      <link>https://arxiv.org/abs/2404.03648</link>
      <description><![CDATA[大型语言模型 (LLM) 推动了许多智能代理任务，例如网络导航，但由于以下三个因素，大多数现有代理在现实网页中的表现远不能令人满意：(1) 网页上操作的多功能性，(2) HTML 文本超出模型处理能力，以及 (3) 由于 Web 的开放域性质而导致决策的复杂性。鉴于这一挑战，我们开发了 AutoWebGLM，这是一种基于 ChatGLM3-6B 构建的、性能优于 GPT-4 的自动 Web 导航代理。受人类浏览模式的启发，我们设计了一种 HTML 简化算法来表示网页，简洁地保留重要信息。我们采用混合人类人工智能方法来构建用于课程培训的网络浏览数据。然后，我们通过强化学习和拒绝采样来引导模型，以进一步促进网页理解、浏览器操作和高效的任务分解。为了进行测试，我们为现实世界的网页浏览任务建立了一个双语基准——AutoWebBench。我们通过不同的网络导航基准评估 AutoWebGLM，揭示其改进，但也揭示了应对真实环境的潜在挑战。相关代码、模型和数据将在https://github.com/THUDM/AutoWebGLM发布。]]></description>
      <guid>https://arxiv.org/abs/2404.03648</guid>
      <pubDate>Mon, 08 Apr 2024 04:15:05 GMT</pubDate>
    </item>
    <item>
      <title>中文小型法学硕士：预训练以中文为中心的大语言模型</title>
      <link>https://arxiv.org/abs/2404.04167</link>
      <description><![CDATA[在本研究中，我们介绍了 CT-LLM，这是一种 2B 大语言模型 (LLM)，它说明了在开发 LLM 时向优先考虑中文的关键转变。 CT-LLM独特地从零开始，与传统方法不同，主要纳入中文文本数据，利用12000亿个令牌的广泛语料库，其中8000亿个中文令牌、3000亿个英文令牌和1000亿个代码令牌。这种战略组合促进了模型在理解和处理中文方面的卓越能力，通过对齐技术进一步增强了这种能力。 CT-LLM在CHC-Bench上表现出色，在中文任务中表现出色，并通过SFT展示了其对英语的熟练程度。这项研究挑战了主要以英语语料库培训法学硕士，然后将其适应其他语言的主流模式，拓宽了法学硕士培训方法的视野。通过开源中文法学硕士培养的全流程，包括使用获得的海量合适预训练中文语料库（MAP-CC）、精心挑选的多学科中文硬案例基准（CHC-Bench）和2B规模的中文Tiny LLM（CT-LLM），我们的目标是促进学术界和工业界的进一步探索和创新，为更具包容性和多功能的语言模型铺平道路。]]></description>
      <guid>https://arxiv.org/abs/2404.04167</guid>
      <pubDate>Mon, 08 Apr 2024 04:11:33 GMT</pubDate>
    </item>
    <item>
      <title>没有指数数据就没有“零射击”：预训练概念频率决定多模态模型性能</title>
      <link>https://arxiv.org/abs/2404.04125</link>
      <description><![CDATA[网络爬取的预训练数据集是多模态模型令人印象深刻的“零样本”评估性能的基础，例如用于分类/检索的 CLIP 和用于图像生成的稳定扩散。然而，尚不清楚“零样本”泛化的概念对于此类多模态模型有多大意义，因为尚不清楚其预训练数据集在多大程度上包含“零样本”评估期间针对的下游概念。在这项工作中，我们问：预训练数据集中这些概念的频率如何影响多模态模型在下游概念上的性能？我们在 34 个模型和 5 个标准预训练数据集（CC-3M、CC-12M、YFCC-15M、LAION-400M、LAION-Aesthetics）中全面研究了这个问题，生成了超过 300GB 的数据工件。我们一致发现，多模态模型远非表现出“零样本”泛化，而是需要指数级更多的数据来实现下游“零样本”性能的线性改进，遵循样本低效对数线性缩放趋势。即使在控制预训练和下游数据集之间的样本级相似性以及对纯合成数据分布进行测试时，这种趋势仍然存在。此外，根据我们的分析对采样的长尾数据进行基准测试模型，我们证明多模态模型整体表现不佳。我们将此长尾测试集贡献为“Let it Wag!”为进一步研究该方向奠定了基础。综上所述，我们的研究揭示了对训练数据的指数级需求，这意味着大规模训练范式下“零样本”泛化能力的关键仍有待找到。]]></description>
      <guid>https://arxiv.org/abs/2404.04125</guid>
      <pubDate>Mon, 08 Apr 2024 04:01:49 GMT</pubDate>
    </item>
    <item>
      <title>搜索流（SoS）：学习用语言搜索</title>
      <link>https://arxiv.org/abs/2404.03683</link>
      <description><![CDATA[语言模型在训练时很少出现富有成果的错误。然后，他们努力去超越下一个标记，遭受滚雪球般的错误的困扰，并努力预测他们的行为的后果。在本文中，我们展示了如何通过将语言中的搜索过程表示为扁平化字符串——搜索流（SoS）来训练语言模型进行搜索。我们提出了一种统一的搜索语言，它捕获了一系列不同的符号搜索策略。我们使用简单但困难的倒计时游戏演示了我们的方法，其目标是将输入数字与算术运算相结合以达到目标数字。我们在启发式求解器生成的搜索流数据集上从头开始预训练基于变压器的语言模型。我们发现，与仅预测最佳搜索轨迹的模型相比，SoS 预训练的搜索精度提高了 25%。我们通过两种策略改进方法进一步微调该模型：优势诱导策略调整（APA）和自学推理器（STaR）。经过微调的 SoS 模型解决了 36% 以前未解决的问题，包括任何启发式求解器都无法解决的问题。我们的结果表明，语言模型可以学习通过搜索解决问题，自我改进以灵活使用不同的搜索策略，并有可能发现新的策略。]]></description>
      <guid>https://arxiv.org/abs/2404.03683</guid>
      <pubDate>Mon, 08 Apr 2024 02:38:04 GMT</pubDate>
    </item>
    <item>
      <title>直接纳什优化：教授语言模型根据一般偏好进行自我改进</title>
      <link>https://arxiv.org/abs/2404.03715</link>
      <description><![CDATA[本文研究了训练后大型语言模型 (LLM)，使用来自强大预言机的偏好反馈来帮助模型迭代地改进自身。培训后法学硕士的典型方法涉及人类反馈强化学习（RLHF），传统上它将奖励学习和后续策略优化分开。然而，这种奖励最大化方法受到“逐点”奖励（例如 Bradley-Terry 模型）性质的限制，无法表达复杂的不及物或循环偏好关系。虽然 RLHF 的进展表明奖励学习和策略优化可以合并为一个单一的对比目标以实现稳定性，但它们仍然受到奖励最大化框架的束缚。最近，新一波研究回避了奖励最大化假设，转而直接优化“成对”或一般偏好。在本文中，我们介绍了直接纳什优化（DNO），这是一种可证明且可扩展的算法，它将对比学习的简单性和稳定性与优化一般偏好的理论通用性结合起来。由于 DNO 是一种使用基于回归的目标的批处理策略算法，因此其实现简单且高效。此外，DNO 在迭代中享有单调改进，这有助于它甚至超越强大的教师（例如 GPT-4）。在我们的实验中，由 DNO 对齐的 7B 参数 Orca-2.5 模型在 AlpacaEval 2.0 上实现了针对 GPT-4-Turbo 的最先进的 33% 胜率（即使在控制响应长度之后），绝对的与初始化模型相比，增益提高了 26%（7% 至 33%）。它的性能优于具有更多参数的模型，包括 Mistral Large、Self-Rewarding LM（70B 参数）和旧版本的 GPT-4。]]></description>
      <guid>https://arxiv.org/abs/2404.03715</guid>
      <pubDate>Mon, 08 Apr 2024 02:03:29 GMT</pubDate>
    </item>
    <item>
      <title>一致性模型的强化学习：更快的奖励引导文本到图像生成</title>
      <link>https://arxiv.org/abs/2404.03673</link>
      <description><![CDATA[强化学习 (RL) 通过直接优化捕捉图像质量、美观和指令遵循能力的奖励，改进了扩散模型的引导图像生成。然而，由此产生的生成策略继承了导致生成缓慢的扩散模型的相同迭代采样过程。为了克服这一限制，一致性模型提出学习一类新的生成模型，将噪声直接映射到数据，从而产生一个可以在短短一次采样迭代中生成图像的模型。在这项工作中，为了优化特定任务奖励的文本到图像生成模型并实现快速训练和推理，我们提出了一个通过 RL 微调一致性模型的框架。我们的框架称为一致性模型强化学习 (RLCM)，将一致性模型的迭代推理过程构建为 RL 过程。 RLCM 在文本到图像生成功能上改进了 RL 微调扩散模型，并在推理时间内以计算量换取样本质量。通过实验，我们表明 RLCM 可以使文本到图像的一致性模型适应难以通过提示表达的目标，例如图像可压缩性，以及源自人类反馈的目标，例如美学质量。与 RL 微调扩散模型相比，RLCM 训练速度明显更快，提高了奖励目标下测量的生成质量，并通过仅需两个推理步骤生成高质量图像来加快推理过程。我们的代码可在 https://rlcm.owenoertell.com 获取]]></description>
      <guid>https://arxiv.org/abs/2404.03673</guid>
      <pubDate>Mon, 08 Apr 2024 01:55:33 GMT</pubDate>
    </item>
    </channel>
</rss>