<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>拥抱日报</title>
    <link>https://huggingface.co/papers</link>
    <description>Huggingface Daily Papers - 由 RSSHub 用爱制作（https://github.com/DIYgod/RSSHub）</description>
    <lastBuildDate>Thu, 21 Mar 2024 13:15:29 GMT</lastBuildDate>
    <item>
      <title>模型合并配方的进化优化</title>
      <link>https://arxiv.org/abs/2403.13187</link>
      <description><![CDATA[我们提出了进化算法的一种新颖应用，可以自动创建强大的基础模型。虽然模型合并因其成本效益而成为法学硕士开发的一种有前景的方法，但它目前依赖于人类直觉和领域知识，限制了其潜力。在这里，我们提出了一种进化方法，通过自动发现不同开源模型的有效组合，利用它们的集体智慧，而不需要大量的额外训练数据或计算来克服这一限制。我们的方法在参数空间和数据流空间中运行，允许优化而不仅仅是单个模型的权重。这种方法甚至促进了跨领域合并，生成像具有数学推理能力的日本法学硕士那样的模型。令人惊讶的是，我们的日本数学法学硕士在各种已建立的日本法学硕士基准上取得了最先进的表现，甚至超越了参数明显更多的模型，尽管没有针对此类任务进行明确的培训。此外，通过我们的方法生成的具有文化意识的日本 VLM 证明了其在描述日本文化特定内容方面的有效性，优于以前的日本 VLM。这项工作不仅为开源社区贡献了最先进的新模型，而且还引入了自动化模型组合的新范式，为探索基础模型开发的替代、高效方法铺平了道路。]]></description>
      <guid>https://arxiv.org/abs/2403.13187</guid>
      <pubDate>Thu, 21 Mar 2024 04:53:31 GMT</pubDate>
    </item>
    <item>
      <title>DepthFM：利用流量匹配进行快速单目深度估计</title>
      <link>https://arxiv.org/abs/2403.13788</link>
      <description><![CDATA[单目深度估计对于众多下游视觉任务和应用至关重要。目前解决这个问题的判别方法由于模糊伪像而受到限制，而最先进的生成方法由于其 SDE 性质而受到采样缓慢的困扰。我们不是从噪声开始，而是寻求从输入图像到深度图的直接映射。我们观察到，这可以使用流匹配来有效地构建，因为它通过解决方案空间的直线轨迹提供了效率和高质量。我们的研究表明，预先训练的图像扩散模型可以作为流匹配深度模型的充分先验，允许仅对合成数据进行有效训练以推广到真实图像。我们发现辅助表面法线损失进一步改善了深度估计。由于我们方法的生成性质，我们的模型可靠地预测了其深度估计的置信度。在复杂自然场景的标准基准上，我们的轻量级方法以有利的低计算成本展示了最先进的性能，尽管只接受了很少的合成数据的训练。]]></description>
      <guid>https://arxiv.org/abs/2403.13788</guid>
      <pubDate>Thu, 21 Mar 2024 04:43:26 GMT</pubDate>
    </item>
    <item>
      <title>成为你的外画师：通过特定于输入的适应掌握视频外画</title>
      <link>https://arxiv.org/abs/2403.13745</link>
      <description><![CDATA[视频绘制是一项具有挑战性的任务，旨在在输入视频的视口之外生成视频内容，同时保持帧间和帧内的一致性。现有方法在生成质量或灵活性方面都存在不足。我们引入了 MOTIA 通过特定于输入的适应来掌握视频外画，这是一种基于扩散的管道，它利用源视频的固有数据特定模式和图像/视频生成先验来进行有效的外画。 MOTIA 包括两个主要阶段：输入特定的适应和模式感知的外画。输入特定的适应阶段涉及对单镜头源视频进行高效且有效的伪外画学习。此过程鼓励模型识别和学习源视频中的模式，并弥合标准生成过程和绘制之间的差距。后续阶段是模式感知外画，致力于泛化这些学习模式以生成外画结果。提出了包括空间感知插入和噪声传播在内的其他策略，以更好地利用扩散模型的生成先验和从源视频获取的视频模式。广泛的评估强调了 MOTIA 的优越性，在广泛认可的基准中优于现有的最先进方法。值得注意的是，这些进步是在不需要进行广泛的、特定于任务的调整的情况下实现的。]]></description>
      <guid>https://arxiv.org/abs/2403.13745</guid>
      <pubDate>Thu, 21 Mar 2024 04:40:16 GMT</pubDate>
    </item>
    <item>
      <title>RadSplat：基于辐射场的高斯喷射，可实现 900+ FPS 的鲁棒实时渲染</title>
      <link>https://arxiv.org/abs/2403.13806</link>
      <description><![CDATA[视图合成和实时渲染方面的最新进展以令人印象深刻的渲染速度实现了逼真的质量。虽然基于辐射场的方法在野外捕捉和大型场景等具有挑战性的场景中实现了最先进的质量，但它们经常受到与体积渲染相关的过高的计算要求的困扰。另一方面，基于高斯溅射的方法依赖于光栅化并自然地实现实时渲染，但受到脆弱的优化启发式方法的影响，在更具挑战性的场景中表现不佳。在这项工作中，我们提出了 RadSplat，一种用于复杂场景的鲁棒实时渲染的轻量级方法。我们的主要贡献有三方面。首先，我们使用辐射场作为先验和监督信号来优化基于点的场景表示，从而提高质量和更稳健的优化。接下来，我们开发了一种新颖的修剪技术，在保持高质量的同时减少总体点数，从而获得更小、更紧凑的场景表示和更快的推理速度。最后，我们提出了一种新颖的测试时过滤方法，可以进一步加速渲染并允许扩展到更大的房屋大小的场景。我们发现我们的方法能够以 900+ FPS 的速度合成最先进的复杂捕获。]]></description>
      <guid>https://arxiv.org/abs/2403.13806</guid>
      <pubDate>Thu, 21 Mar 2024 04:28:47 GMT</pubDate>
    </item>
    <item>
      <title>ZigMa：之字形曼巴扩散模型</title>
      <link>https://arxiv.org/abs/2403.13802</link>
      <description><![CDATA[扩散模型长期以来一直受到可扩展性和二次复杂性问题的困扰，特别是在基于变压器的结构中。在本研究中，我们的目标是利用称为 Mamba 的状态空间模型的长序列建模功能，将其适用性扩展到视觉数据生成。首先，我们发现大多数当前基于 Mamba 的视觉方法存在一个关键的疏忽，即 Mamba 扫描方案中缺乏对空间连续性的考虑。其次，基于这一见解，我们引入了一种简单、即插即用、零参数的方法，名为 Zigzag Mamba，它的性能优于基于 Mamba 的基线，并且与基于 Transformer 的基线相比，速度和内存利用率得到了提高。最后，我们将 Zigzag Mamba 与随机插值框架集成，以研究模型在高分辨率视觉数据集上的可扩展性，例如 FacesHQ 1024times 1024 和 UCF101、MultiModal-CelebA-HQ 和 MS COCO 256times 256。代码将在https://taohu.me/zigma/]]></description>
      <guid>https://arxiv.org/abs/2403.13802</guid>
      <pubDate>Thu, 21 Mar 2024 04:26:48 GMT</pubDate>
    </item>
    <item>
      <title>VSTAR：用于更长动态视频合成的生成时间护理</title>
      <link>https://arxiv.org/abs/2403.13501</link>
      <description><![CDATA[尽管文本到视频 (T2V) 合成领域取得了巨大进步，但开源 T2V 扩散模型仍难以生成内容动态变化和演变的较长视频。他们倾向于合成准静态视频，忽略文本提示中隐含的必要的视觉随时间变化。与此同时，扩展这些模型以实现更长、更动态的视频合成通常在计算上仍然很困难。为了应对这一挑战，我们引入了生成时间护理（GTN）的概念，我们的目标是在推理过程中动态改变生成过程，以改善对时间动态的控制并生成更长的视频。我们提出了一种名为 VSTAR 的 GTN 方法，它由两个关键要素组成：1）视频概要提示（VSP）——利用 LLM 基于原始单一提示自动生成视频概要，为不同的视觉状态提供准确的文本指导2) 时间注意力正则化 (TAR) - 一种正则化技术，用于细化预训练 T2V 扩散模型的时间注意力单元，从而能够控制视频动态。我们通过实验展示了所提出的方法在生成更长、更具视觉吸引力的视频方面优于现有开源 T2V 模型。我们还分析了使用和不使用 VSTAR 实现的时间注意力图，证明了应用我们的方法来减轻对随时间推移所需视觉变化的忽视的重要性。]]></description>
      <guid>https://arxiv.org/abs/2403.13501</guid>
      <pubDate>Thu, 21 Mar 2024 04:18:13 GMT</pubDate>
    </item>
    <item>
      <title>HyperLLaVA：针对多模态大型语言模型的动态视觉和语言专家调整</title>
      <link>https://arxiv.org/abs/2403.13447</link>
      <description><![CDATA[最近的进展表明，扩展多模态大型语言模型（MLLM）可以有效提高下游多模态任务的性能。流行的 MLLM 范例，例如 LLaVA，使用静态视觉语言映射器将视觉特征转换为类似文本的标记，从而使静态 LLM 能够通过视觉指令调整来开发理解视觉信息的能力。虽然很有前途，但静态调优策略~静态调优是指使用静态参数训练模型。共享相同参数可能会限制不同下游多模式任务的性能。有鉴于此，我们引入了 HyperLLaVA，它涉及分别与动态视觉专家和语言专家一起自适应调整投影仪和 LLM 参数。这些专家源自 HyperNetworks，它通过视觉和语言指导生成自适应参数变化，从而在两阶段训练中实现动态投影仪和 LLM 建模。我们的实验表明，我们的解决方案在现有 MLLM 基准（包括 MME、MMBench、SEED-Bench 和 LLaVA-Bench）上显着优于 LLaVA。 ~我们的项目可通过链接 https://github.com/DCDmllm/HyperLLaVA 获取。]]></description>
      <guid>https://arxiv.org/abs/2403.13447</guid>
      <pubDate>Thu, 21 Mar 2024 03:56:57 GMT</pubDate>
    </item>
    <item>
      <title>LlamaFactory：100+语言模型统一高效微调</title>
      <link>https://arxiv.org/abs/2403.13372</link>
      <description><![CDATA[高效的微调对于使大型语言模型 (LLM) 适应下游任务至关重要。然而，在不同的模型上实现这些方法需要付出很大的努力。我们推出了 LlamaFactory，一个集成了一套尖端高效训练方法的统一框架。它允许用户灵活地定制100多个LLM的微调，而无需通过内置的Web UI LlamaBoard进行编码。我们凭经验验证了我们的框架在语言建模和文本生成任务上的效率和有效性。它已在 https://github.com/hiyouga/LLaMA-Factory 发布，并已获得超过 13,000 个 star 和 1,600 个分叉。]]></description>
      <guid>https://arxiv.org/abs/2403.13372</guid>
      <pubDate>Thu, 21 Mar 2024 03:50:35 GMT</pubDate>
    </item>
    <item>
      <title>Compress3D：用于从单个图像生成 3D 的压缩潜在空间</title>
      <link>https://arxiv.org/abs/2403.13524</link>
      <description><![CDATA[3D 生成已取得显着进步，但从单个图像高效生成高质量 3D 资产仍然具有挑战性。在本文中，我们提出了一种三平面自动编码器，它将 3D 模型编码到紧凑的三平面潜在空间中，以有效地压缩 3D 几何和纹理信息。在自动编码器框架中，我们引入了一种 3D 感知交叉注意机制，该机制利用低分辨率潜在表示从高分辨率 3D 特征量中查询特征，从而增强潜在空间的表示能力。随后，我们在这个细化的潜在空间上训练扩散模型。与仅仅依靠图像嵌入进行 3D 生成相比，我们提出的方法主张同时利用图像嵌入和形状嵌入作为条件。具体来说，形状嵌入是通过以图像嵌入为条件的扩散先验模型来估计的。通过全面的实验，我们证明我们的方法优于最先进的算法，在需要更少的训练数据和时间的同时实现卓越的性能。我们的方法只需 7 秒即可在单个 A100 GPU 上生成高质量的 3D 资源。]]></description>
      <guid>https://arxiv.org/abs/2403.13524</guid>
      <pubDate>Thu, 21 Mar 2024 03:14:00 GMT</pubDate>
    </item>
    <item>
      <title>迈向语言模型中的 3D 分子文本解释</title>
      <link>https://arxiv.org/abs/2401.13923</link>
      <description><![CDATA[语言模型（LM）极大地影响了不同的领域。然而，它们在理解 3D 分子结构方面的固有局限性极大地限制了它们在生物分子领域的潜力。为了弥补这一差距，我们专注于 3D 分子文本解释，并提出 3D-MoLM：3D 分子语言建模。具体来说，3D-MoLM 通过为 LM 配备 3D 分子编码器，使 LM 能够解释和分析 3D 分子。这种集成是通过 3D 分子文本投影仪实现的，桥接 3D 分子编码器的表示空间和 LM 的输入空间。此外，为了增强3D-MoLM跨模态分子理解和指令跟踪的能力，我们精心策划了一个以3D分子为中心的指令调整数据集——3D-MoIT。通过3D分子文本对齐和以3D分子为中心的指令调整，3D-MoLM建立了3D分子编码器和LM的集成。它显着超越了下游任务的现有基线，包括分子文本检索、分子字幕和更具挑战性的开放文本分子 QA 任务，特别是关注 3D 相关属性。]]></description>
      <guid>https://arxiv.org/abs/2401.13923</guid>
      <pubDate>Thu, 21 Mar 2024 03:01:09 GMT</pubDate>
    </item>
    </channel>
</rss>