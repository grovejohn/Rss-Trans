<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>拥抱日报</title>
    <link>https://huggingface.co/papers</link>
    <description>Huggingface Daily Papers - RSSHub 用爱制作的(https://github.com/DIYgod/RSSHub)</description>
    <lastBuildDate>Fri, 22 Mar 2024 04:35:14 GMT</lastBuildDate>
    <item>
      <title>高斯磨砂：具有实时渲染的可编辑复杂辐射场</title>
      <link>https://arxiv.org/abs/2403.14554</link>
      <description><![CDATA[我们提出了 Gaussian Frosting，这是一种新颖的基于网格的表示，用于实时高质量渲染和编辑复杂的 3D 效果。我们的方法建立在最近的 3D 高斯分布框架的基础上，该框架优化了一组 3D 高斯以近似图像的辐射场。我们建议首先在优化过程中从高斯模型中提取基础网格，然后在网格周围构建和细化具有可变厚度的自适应高斯层，以更好地捕捉表面附近的精细细节和体积效果，例如头发或草。我们将这一层称为高斯糖霜，因为它类似于蛋糕上的糖霜涂层。材料越模糊，糖霜就越厚。我们还引入了高斯参数化，以强制它们留在磨砂层内，并在变形、重新缩放、编辑或动画网格时自动调整其参数。我们的表示允许使用高斯喷射进行高效渲染，以及通过修改基础网格进行编辑和动画。我们证明了我们的方法在各种合成和真实场景上的有效性，并表明它优于现有的基于表面的方法。我们将发布我们的代码和基于网络的查看器作为额外的贡献。我们的项目页面如下：https://anttwo.github.io/frosting/]]></description>
      <guid>https://arxiv.org/abs/2403.14554</guid>
      <pubDate>Fri, 22 Mar 2024 03:04:30 GMT</pubDate>
    </item>
    <item>
      <title>DreamReward：根据人类偏好生成文本到 3D</title>
      <link>https://arxiv.org/abs/2403.14613</link>
      <description><![CDATA[最近，通过文本提示创建 3D 内容取得了显着的成功。然而，当前的文本转 3D 方法通常会生成与人类偏好不太相符的 3D 结果。在本文中，我们提出了一个名为 DreamReward 的综合框架，用于从人类偏好反馈中学习和改进文本到 3D 模型。首先，我们根据系统注释管道（包括评级和排名）收集 25,000 个专家比较。然后，我们构建了 Reward3D——第一个通用文本到 3D 人类偏好奖励模型，以有效编码人类偏好。基于 3D 奖励模型，我们最终进行理论分析并提出 Reward3D 反馈学习 (DreamFL)，这是一种直接调整算法，可通过重新定义的评分器来优化多视图扩散模型。以理论证明和广泛的实验比较为基础，我们的 DreamReward 成功生成了高保真度和 3D 一致的结果，并显着促进了与人类意图的迅速一致。我们的结果证明了从人类反馈中学习来改进文本到 3D 模型的巨大潜力。]]></description>
      <guid>https://arxiv.org/abs/2403.14613</guid>
      <pubDate>Fri, 22 Mar 2024 02:50:05 GMT</pubDate>
    </item>
    <item>
      <title>AnyV2V：适用于任何视频到视频编辑任务的即插即用框架</title>
      <link>https://arxiv.org/abs/2403.14468</link>
      <description><![CDATA[视频到视频编辑涉及编辑源视频以及附加控件（例如文本提示、主题或样式），以生成与源视频和提供的控件一致的新视频。传统方法仅限于某些编辑类型，限制了其满足广泛用户需求的能力。在本文中，我们介绍了 AnyV2V，这是一种新颖的免训练框架，旨在将视频编辑简化为两个主要步骤：（1）采用现成的图像编辑模型（例如 InstructPix2Pix、InstantID 等）来修改第一帧，（2）利用现有的图像到视频生成模型（例如 I2VGen-XL）进行 DDIM 反演和特征注入。在第一阶段，AnyV2V可以插入任何现有的图像编辑工具来支持广泛的视频编辑任务。除了传统的基于提示的编辑方法之外，AnyV2V 还可以支持新颖的视频编辑任务，包括基于参考的风格转换、主题驱动的编辑和身份操作，这是以前的方法无法实现的。在第二阶段，AnyV2V可以插入任何现有的图像到视频模型来执行DDIM反演和中间特征注入，以保持与源视频的外观和运动一致性。在基于提示的编辑方面，我们表明 AnyV2V 在提示对齐方面比之前的最佳方法高出 35%，在人类偏好方面比之前的最佳方法高出 25%。在这三个新颖的任务上，我们表明 AnyV2V 也取得了很高的成功率。我们相信 AnyV2V 将继续蓬勃发展，因为它能够无缝集成快速发展的图像编辑方法。这种兼容性可以帮助AnyV2V增加其多功能性，以满足不同的用户需求。]]></description>
      <guid>https://arxiv.org/abs/2403.14468</guid>
      <pubDate>Fri, 22 Mar 2024 02:44:20 GMT</pubDate>
    </item>
    <item>
      <title>时间与空间的探索</title>
      <link>https://arxiv.org/abs/2403.14611</link>
      <description><![CDATA[我们引入有界生成作为一种通用任务来控制视频生成，以仅基于给定的开始和结束帧来合成任意相机和主体运动。我们的目标是充分利用图像到视频模型固有的泛化能力，而无需对原始模型进行额外的训练或微调。这是通过提出的新采样策略实现的，我们将其称为时间反转融合，该策略融合分别以开始帧和结束帧为条件的时间前向和后向去噪路径。融合路径产生的视频可以平滑地连接两个帧，从而产生忠实的主体运动的中间、静态场景的新颖视图以及当两个边界帧相同时的无缝视频循环。我们整理了图像对的多样化评估数据集，并与最接近的现有方法进行比较。我们发现时间反转融合在所有子任务上都优于相关工作，表现出生成复杂运动和由有界框架引导的 3D 一致视图的能力。请参阅项目页面 https://time-reversal.github.io。]]></description>
      <guid>https://arxiv.org/abs/2403.14611</guid>
      <pubDate>Fri, 22 Mar 2024 02:31:01 GMT</pubDate>
    </item>
    <item>
      <title>StyleCineGAN：使用预先训练的 StyleGAN 生成景观电影图片</title>
      <link>https://arxiv.org/abs/2403.14186</link>
      <description><![CDATA[我们提出了一种可以使用预先训练的 StyleGAN 从静态风景图像自动生成电影图的方法。受到最近无条件视频生成成功的启发，我们利用强大的预训练图像生成器来合成高质量的电影图像。与之前主要利用预训练 StyleGAN 的潜在空间的方法不同，我们的方法利用其深层特征空间进行 GAN 反演和电影图生成。具体来说，我们提出了多尺度深度特征扭曲（MSDFW），它以不同的分辨率扭曲预训练的 StyleGAN 的中间特征。通过使用 MSDFW，生成的电影图像具有高分辨率并表现出合理的循环动画。我们通过用户研究以及与最先进的电影图像生成方法和使用预训练 StyleGAN 的视频生成方法的定量比较，证明了我们方法的优越性。]]></description>
      <guid>https://arxiv.org/abs/2403.14186</guid>
      <pubDate>Fri, 22 Mar 2024 02:24:02 GMT</pubDate>
    </item>
    <item>
      <title>通过内容帧运动潜在分解的高效视频扩散模型</title>
      <link>https://arxiv.org/abs/2403.14148</link>
      <description><![CDATA[视频扩散模型最近在生成质量方面取得了长足进步，但仍然受到高内存和计算要求的限制。这是因为当前的视频扩散模型通常尝试直接处理高维视频。为了解决这个问题，我们提出了内容运动潜在扩散模型（CMD），这是一种用于视频生成的预训练图像扩散模型的新颖有效扩展。具体来说，我们提出了一种自动编码器，它将视频简洁地编码为内容帧（如图像）和低维运动潜在表示的组合。前者代表常见内容，后者代表视频中的基本运动。我们通过微调预训练的图像扩散模型来生成内容帧，并通过训练新的轻量级扩散模型来生成运动潜在表示。这里的一个关键创新是设计了一个紧凑的潜在空间，可以直接利用预训练的图像扩散模型，这在以前的潜在视频扩散模型中还没有做到。这会显着提高生成质量并降低计算成本。例如，CMD 可以在 3.1 秒内生成分辨率为 512×1024、长度为 16 的视频，从而比以前的方法快 7.7 倍地采样视频。此外，CMD 在 WebVid-10M 上的 FVD 得分为 212.7，比之前的最佳得分 292.4 提高了 27.3%。]]></description>
      <guid>https://arxiv.org/abs/2403.14148</guid>
      <pubDate>Fri, 22 Mar 2024 02:18:10 GMT</pubDate>
    </item>
    </channel>
</rss>