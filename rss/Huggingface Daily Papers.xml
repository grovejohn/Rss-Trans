<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>拥抱日报</title>
    <link>https://huggingface.co/papers</link>
    <description>Huggingface Daily Papers - RSSHub 用爱制作的(https://github.com/DIYgod/RSSHub)</description>
    <lastBuildDate>Tue, 02 Apr 2024 19:10:11 GMT</lastBuildDate>
    <item>
      <title>WavLLM：迈向稳健且自适应的语音大型语言模型</title>
      <link>https://arxiv.org/abs/2404.00656</link>
      <description><![CDATA[大语言模型 (LLM) 的最新进展彻底改变了自然语言处理领域，逐渐将其范围扩大到多模态感知和生成。然而，将听力能力有效地整合到法学硕士中提出了重大挑战，特别是在跨不同背景进行概括和执行复杂的听觉任务方面。在这项工作中，我们介绍了 WavLLM，一种具有双编码器的鲁棒自适应语音大语言模型，以及通过两阶段课程学习方法优化的提示感知 LoRA 权重适配器。利用双编码器，我们解耦不同类型的语音信息，利用 Whisper 编码器处理语音的语义内容，并利用 WavLM 编码器捕获说话者身份的独特特征。在课程学习框架内，WavLLM 首先通过优化混合基本单一任务来构建其基础能力，然后针对更复杂的任务（例如基本任务的组合）进行高级多任务训练。为了增强对不同任务和指令的灵活性和依从性，在第二个高级多任务训练阶段引入了提示感知 LoRA 重量适配器。我们在通用语音基准（包括 ASR、ST、SV、ER 等任务）上验证了所提出的模型，并将其应用于专门的数据集，例如用于 SQA 的高考英语听力理解集和语音思想链（CoT）评估集。实验表明，所提出的模型在相同模型大小的一系列语音任务中实现了最先进的性能，在使用 CoT 方法执行复杂任务时表现出强大的泛化能力。此外，我们的模型无需专门训练即可成功完成高考任务。代码、模型、音频和高考评估集可在 aka.ms/wavllm 访问。]]></description>
      <guid>https://arxiv.org/abs/2404.00656</guid>
      <pubDate>Tue, 02 Apr 2024 15:46:15 GMT</pubDate>
    </item>
    <item>
      <title>布局感知语言模型的噪声感知训练</title>
      <link>https://arxiv.org/abs/2404.00488</link>
      <description><![CDATA[视觉丰富的文档 (VRD) 利用视觉特征和语言线索来传播信息。训练从文档中识别命名实体的自定义提取器需要大量以文本和视觉方式注释的目标文档类型实例。这是企业场景中一个昂贵的瓶颈，我们希望以可扩展的方式为数千种不同的文档类型训练自定义提取器。在目标文档类型的未标记实例上预训练提取器模型，然后对人工标记实例进行微调步骤在这些场景中不起作用，因为它超出了为提取器分配的最大允许训练时间。我们通过在本文中提出噪声感知训练方法或 NAT 来解决这种情况。 NAT 不是获取昂贵的人工标记文档，而是利用弱标记文档以可扩展的方式训练提取器。为了避免由于噪声、弱标记样本而导致模型质量下降，NAT 会估计每个训练样本的置信度，并将其作为训练期间的不确定性度量。我们使用 NAT 训练多个最先进的提取器模型。对许多公开可用和内部数据集的实验表明，经过 NAT 训练的模型不仅性能稳健——在宏观 F1 分数方面比迁移学习基线高出 6%，而且标签效率更高——它可以将获得可比性能所需的人力减少多达 73%。]]></description>
      <guid>https://arxiv.org/abs/2404.00488</guid>
      <pubDate>Tue, 02 Apr 2024 15:42:28 GMT</pubDate>
    </item>
    <item>
      <title>Aurora-M：第一个根据美国行政命令红队的开源多语言语言模型</title>
      <link>https://arxiv.org/abs/2404.00399</link>
      <description><![CDATA[预训练的语言模型支撑着多种人工智能应用程序，但其训练的高计算成本限制了可访问性。 BLOOM 和 StarCoder 等举措旨在使预训练模型的访问民主化，以促进协作社区开发。然而，此类现有模型面临着挑战：多语言能力有限、持续预训练会导致灾难性遗忘，而从头开始预训练的计算成本高昂，以及遵守人工智能安全和开发法律。本文介绍了 Aurora-M，这是一个 15B 参数多语言开源模型，经过英语、芬兰语、印地语、日语、越南语和代码训练。通过 StarCoderPlus 持续对 4350 亿个额外令牌进行预训练，Aurora-M 的训练令牌总数超过了 2 万亿个令牌。它是第一个根据人工审查的安全指令进行微调的开源多语言模型，因此其开发不仅符合传统的红队考虑因素，而且还符合拜登-哈里斯安全行政命令中阐述的具体问题，安全、值得信赖的人工智能开发和使用。 Aurora-M 在各种任务和语言中经过了严格的评估，展示了针对灾难性遗忘的稳健性，并且在多语言环境中（尤其是在安全评估方面）优于替代方案。为了促进负责任的开源 LLM 开发，Aurora-M 及其变体在 https://huggingface.co/collections/aurora-m/aurora-m-models-65fdfdff62471e09812f5407 上发布。]]></description>
      <guid>https://arxiv.org/abs/2404.00399</guid>
      <pubDate>Tue, 02 Apr 2024 15:38:02 GMT</pubDate>
    </item>
    <item>
      <title>测量扩散模型中的风格相似度</title>
      <link>https://arxiv.org/abs/2404.01292</link>
      <description><![CDATA[生成模型现在被图形设计师和艺术家广泛使用。先前的研究表明，这些模型会在生成过程中记住并经常复制训练数据中的内容。因此，随着图像数量的增加，每次在生成的图像用于专业目的之前，执行数据库搜索以确定图像的属性是否可归因于特定的训练数据变得非常重要。用于此目的的现有工具侧重于检索相似语义内容的图像。与此同时，许多艺术家关心文本到图像模型中的风格复制。我们提出了一个用于理解和从图像中提取风格描述符的框架。我们的框架包含一个新的数据集，该数据集使用以下见解来策划：风格是图像的主观属性，它捕获复杂但有意义的因素之间的相互作用，包括但不限于颜色、纹理、形状等。我们还提出了一种提取风格描述符的方法，可用于将生成图像的样式归因于文本到图像模型的训练数据集中使用的图像。我们展示了各种风格检索任务中令人鼓舞的结果。我们还定量和定性分析稳定扩散模型中的风格归因和匹配。代码和工件可在 https://github.com/learn2phoenix/CSD 获取。]]></description>
      <guid>https://arxiv.org/abs/2404.01292</guid>
      <pubDate>Tue, 02 Apr 2024 15:16:00 GMT</pubDate>
    </item>
    <item>
      <title>ST-LLM：大型语言模型是有效的时间学习者</title>
      <link>https://arxiv.org/abs/2404.00308</link>
      <description><![CDATA[大型语言模型 (LLM) 在文本理解和生成方面展示了令人印象深刻的能力，推动了视频 LLM 的研究工作，以促进视频级别的人机交互。然而，如何在基于视频的对话系统中有效地编码和理解视频仍有待解决。在本文中，我们研究了一个简单但尚未探索的问题：我们能否将所有时空标记输入到法学硕士中，从而将视频序列建模的任务委托给法学硕士？令人惊讶的是，这种简单的方法显着提高了视频理解能力。基于此，我们提出了 ST-LLM，一种有效的视频-LLM 基线，在 LLM 内进行时空序列建模。此外，为了解决法学硕士内未压缩视频令牌带来的开销和稳定性问题，我们开发了一种具有定制培训目标的动态屏蔽策略。对于特别长的视频，我们还设计了全局-本地输入模块来平衡效率和效果。因此，我们利用法学硕士进行熟练的时空建模，同时保持效率和稳定性。大量的实验结果证明了我们方法的有效性。通过更简洁的模型和训练流程，ST-LLM 在 VideoChatGPT-Bench 和 MVBench 上建立了新的最先进结果。代码已在 https://github.com/TencentARC/ST-LLM 提供。]]></description>
      <guid>https://arxiv.org/abs/2404.00308</guid>
      <pubDate>Tue, 02 Apr 2024 15:10:47 GMT</pubDate>
    </item>
    <item>
      <title>流式传输密集视频字幕</title>
      <link>https://arxiv.org/abs/2404.01297</link>
      <description><![CDATA[密集视频字幕的理想模型（预测视频中临时定位的字幕）应该能够处理长输入视频，预测丰富、详细的文本描述，并能够在处理整个视频之前生成输出。然而，当前最先进的模型处理固定数量的下采样帧，并在观看整个视频后进行单个完整预测。我们提出了一种流密集视频字幕模型，该模型由两个新颖的组件组成：首先，我们提出了一种基于聚类传入令牌的新内存模块，由于内存大小固定，因此可以处理任意长的视频。其次，我们开发了一种流式解码算法，使我们的模型能够在整个视频处理之前进行预测。我们的模型实现了这种流媒体能力，并显着提高了三个密集视频字幕基准测试的最新水平：ActivityNet、YouCook2 和 ViTT。我们的代码发布于 https://github.com/google-research/scenic。]]></description>
      <guid>https://arxiv.org/abs/2404.01297</guid>
      <pubDate>Tue, 02 Apr 2024 14:59:46 GMT</pubDate>
    </item>
    <item>
      <title>CosmicMan：人类的文本到图像基础模型</title>
      <link>https://arxiv.org/abs/2404.01294</link>
      <description><![CDATA[我们提出了 CosmicMan，一种专门用于生成高保真人类图像的文本到图像基础模型。与当前通用基础模型陷入质量低劣和人类文本图像错位的困境不同，CosmicMan 能够生成外观细致、结构合理、文本图像对齐精确、描述详细密集的逼真人类图像。 CosmicMan 成功的核心是对数据和模型的新反思和视角：（1）我们发现数据质量和可扩展的数据生产流程对于训练模型的最终结果至关重要。因此，我们提出了一种新的数据生产范式，Annotate Anybody，它作为永久的数据飞轮，随着时间的推移产生具有准确且具有成本效益的注释的高质量数据。在此基础上，我们构建了一个大规模数据集 CosmicMan-HQ 1.0，其中包含 600 万张平均分辨率为 1488x1255 的高质量真实人类图像，并附有来自 1.15 亿个不同粒度属性的精确文本注释。 （2）我们认为专门针对人类的文本到图像基础模型必须是实用的——易于集成到下游任务中，同时有效地生成高质量的人类图像。因此，我们建议以分解的方式对密集文本描述和图像像素之间的关系进行建模，并提出分解注意力重新聚焦（Daring）训练框架。它无缝分解现有文本到图像扩散模型中的交叉注意力特征，并在不添加额外模块的情况下强制注意力重新聚焦。通过 Daring，我们表明，将连续文本空间显式离散为与人体结构对齐的几个基本组是轻松解决错位问题的关键。]]></description>
      <guid>https://arxiv.org/abs/2404.01294</guid>
      <pubDate>Tue, 02 Apr 2024 14:55:59 GMT</pubDate>
    </item>
    <item>
      <title>正确做法：提高文本到图像模型的空间一致性</title>
      <link>https://arxiv.org/abs/2404.01197</link>
      <description><![CDATA[当前文本到图像（T2I）模型的主要缺点之一是它们无法一致地生成忠实遵循文本提示中指定的空间关系的图像。在本文中，我们对这一限制进行了全面的研究，同时还开发了实现最先进性能的数据集和方法。首先，我们发现当前的视觉语言数据集不能很好地表示空间关系；为了缓解这一瓶颈，我们通过重新描述来自 4 个广泛使用的视觉数据集的 600 万张图像，创建了 SPRIGHT，这是第一个以空间为中心的大型数据集。通过三重评估和分析流程，我们发现 SPRIGHT 在捕获空间关系方面大大改进了现有数据集。为了证明其功效，我们仅利用约 0.25% 的 SPRIGHT，在生成空间精确图像方面实现了 22% 的改进，同时还提高了 FID 和 CMMD 分数。其次，我们发现对包含大量对象的图像进行训练可以显着提高空间一致性。值得注意的是，通过对 &lt;500 张图像进行微调，我们在 T2I-CompBench 上达到了最先进的水平，空间得分为 0.2133。最后，通过一组受控实验和消融，我们记录了多项发现，我们相信这些发现将增强对影响文本到图像模型空间一致性的因素的理解。我们公开发布我们的数据集和模型，以促进该领域的进一步研究。]]></description>
      <guid>https://arxiv.org/abs/2404.01197</guid>
      <pubDate>Tue, 02 Apr 2024 05:41:30 GMT</pubDate>
    </item>
    <item>
      <title>FlexiDreamer：使用 FlexiCube 生成单一图像到 3D</title>
      <link>https://arxiv.org/abs/2404.00987</link>
      <description><![CDATA[最近，通过文本提示或单个图像生成 3D 内容在质量和速度方面取得了显着进步。其主要范例之一涉及生成一致的多视图图像，然后进行稀疏视图重建。然而，由于直接变形网格表示以接近目标拓扑的挑战，大多数方法在稀疏视图重建期间学习隐式表示（例如 NeRF），并通过后处理提取来获取目标网格。尽管隐式表示可以有效地对丰富的 3D 信息进行建模，但其训练通常需要较长的收敛时间。此外，从隐式场中进行的后提取操作也会导致不良的视觉伪影。在本文中，我们提出了 FlexiDreamer，一种新颖的单图像到 3D 生成框架，它以端到端的方式重建目标网格。通过利用灵活的基于梯度的提取（称为 FlexiCubes），我们的方法避免了后处理带来的缺陷，并有助于直接获取目标网格。此外，我们采用了多分辨率哈希网格编码方案，该方案逐步将编码级别激活到 FlexiCube 中的隐式字段中，以帮助捕获每步优化的几何细节。值得注意的是，FlexiDreamer 在单个 NVIDIA A100 GPU 上大约 1 分钟内从单视图图像中恢复密集的 3D 结构，大大优于以前的方法。]]></description>
      <guid>https://arxiv.org/abs/2404.00987</guid>
      <pubDate>Tue, 02 Apr 2024 05:22:13 GMT</pubDate>
    </item>
    <item>
      <title>来自语言模型奖励的视频大型多模态模型的直接偏好优化</title>
      <link>https://arxiv.org/abs/2404.01258</link>
      <description><![CDATA[偏好建模技术，例如直接偏好优化（DPO），在增强大语言模型（LLM）的泛化能力方面已被证明是有效的。然而，在涉及遵循视频指令的任务中，提供信息反馈，尤其是检测生成的响应中的幻觉，仍然是一个重大挑战。先前的研究已经探索使用大型多模态模型（LMM）作为奖励模型来指导偏好建模，但它们准确评估生成的响应与相应视频相比的真实性的能力尚未最终确定。本文介绍了一种新颖的框架，该框架利用详细的视频字幕作为视频内容的代理，使语言模型能够将此信息合并为视频问答（QA）预测评分的支持证据。我们的方法展示了与 OpenAI GPT-4V 模型的奖励机制的稳健一致性，该机制直接将视频帧作为输入。此外，我们还表明，通过 DPO 应用这种定制奖励可以显着提高视频 LMM 在视频 QA 任务上的性能。]]></description>
      <guid>https://arxiv.org/abs/2404.01258</guid>
      <pubDate>Tue, 02 Apr 2024 05:17:22 GMT</pubDate>
    </item>
    </channel>
</rss>