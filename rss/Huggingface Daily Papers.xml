<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>拥抱日报</title>
    <link>https://huggingface.co/papers</link>
    <description>Huggingface Daily Papers - RSSHub 用爱制作的(https://github.com/DIYgod/RSSHub)</description>
    <lastBuildDate>Tue, 30 Apr 2024 05:35:45 GMT</lastBuildDate>
    <item>
      <title>用陪审团取代法官：用不同模型小组评估法学硕士一代</title>
      <link>https://arxiv.org/abs/2404.18796</link>
      <description><![CDATA[随着大型语言模型 (LLM) 变得更加先进，它们已经超出了我们准确评估其质量的能力。不仅找到数据来充分探测特定模型属性很困难，而且单独评估模型自由形式生成的正确性也是一个挑战。为了解决这个问题，许多评估现在依靠法学硕士本身作为评委来对其他法学硕士的输出质量进行评分。评估最常使用单个大型模型，例如 GPT4。虽然这种方法越来越受欢迎，但成本高昂，并且已被证明会引入模型内偏差，并且在这项工作中，我们发现非常大的模型通常是不必要的。我们建议使用法学硕士评估小组（PoLL）来评估模型。在三个不同的法官设置和跨越六个不同数据集的情况下，我们发现使用由大量较小模型组成的 PoLL 优于单个大型法官，并且由于其由不相交的模型系列组成而表现出较少的模型内偏差，并且在价格便宜七倍多。]]></description>
      <guid>https://arxiv.org/abs/2404.18796</guid>
      <pubDate>Tue, 30 Apr 2024 03:04:02 GMT</pubDate>
    </item>
    <item>
      <title>Ag2Manip：通过与代理无关的视觉和动作表示来学习新颖的操作技能</title>
      <link>https://arxiv.org/abs/2404.17521</link>
      <description><![CDATA[能够学习新颖的操作任务的自主机器人系统有望将行业从制造业转变为服务自动化。然而，现代方法（例如 VIP 和 R3M）仍然面临重大障碍，特别是机器人实施例之间的域差距以及特定动作空间内成功任务执行的稀疏性，导致任务表示不一致和模糊。我们引入了 Ag2Manip（与代理无关的操纵表示），该框架旨在通过两项关键创新来克服这些挑战：一种源自人类操纵视频的新颖的与代理无关的视觉表示，模糊了实施例的具体细节以增强普遍性；与代理无关的动作表示将机器人的运动学抽象为通用代理代理，强调末端执行器和物体之间的关键交互。 Ag2Manip 对 FrankaKitchen、ManiSkill 和 PartManip 等模拟基准的实证验证显示，性能提高了 325%，无需特定领域的演示即可实现。消融研究强调了视觉和动作表现对这一成功的重要贡献。将我们的评估扩展到现实世界，Ag2Manip 显着地将模仿学习的成功率从 50% 提高到 77.5%，证明了其在模拟和物理环境中的有效性和普遍性。]]></description>
      <guid>https://arxiv.org/abs/2404.17521</guid>
      <pubDate>Tue, 30 Apr 2024 02:47:06 GMT</pubDate>
    </item>
    <item>
      <title>双子座模型在医学中的能力</title>
      <link>https://arxiv.org/abs/2404.18416</link>
      <description><![CDATA[各种医疗应用的卓越表现给人工智能带来了巨大的挑战，需要先进的推理、获取最新的医学知识以及对复杂的多模态数据的理解。 Gemini 模型在多模式和长上下文推理方面具有强大的通用能力，为医学领域提供了令人兴奋的可能性。基于 Gemini 的这些核心优势，我们推出了 Med-Gemini，这是一个功能强大的多模式模型系列，专门用于医学，能够无缝使用网络搜索，并且可以使用自定义编码器有效地针对新颖的模式进行定制。我们在 14 个医疗基准上评估 Med-Gemini，在其中 10 个基准上建立了新的最先进 (SoTA) 性能，并在每个可以进行直接比较的基准上超越了 GPT-4 模型系列，通常是广泛的利润。在流行的 MedQA (USMLE) 基准上，我们性能最佳的 Med-Gemini 模型使用新颖的不确定性引导搜索策略，实现了 91.1% 准确度的 SoTA 性能。在包括 NEJM Image Challenges 和 MMMU（健康与医学）在内的 7 个多模态基准测试中，Med-Gemini 比 GPT-4V 提高了 44.5% 的平均相对优势。我们通过从长期去识别化的健康记录和医疗视频问答中进行大海捞针检索任务的 SoTA 性能，证明了 Med-Gemini 的长上下文能力的有效性，超越了之前仅使用上下文学习的定制方法。最后，Med-Gemini 的表现表明了其在现实世界中的实用性，在医学文本摘要等任务上超越了人类专家，同时展示了多模式医学对话、医学研究和教育的巨大潜力。总而言之，我们的结果为 Med-Gemini 的潜力提供了令人信服的证据，尽管在这个安全关键领域的实际部署之前，进一步严格的评估至关重要。]]></description>
      <guid>https://arxiv.org/abs/2404.18416</guid>
      <pubDate>Tue, 30 Apr 2024 02:38:38 GMT</pubDate>
    </item>
    <item>
      <title>LEGENT：实体代理的开放平台</title>
      <link>https://arxiv.org/abs/2404.18243</link>
      <description><![CDATA[尽管大型语言模型（LLM）和大型多模态模型（LMM）取得了进步，但它们与基于语言的类人实体的集成仍然不完整，阻碍了物理环境中复杂的现实任务的执行。现有的集成通常具有有限的开源功能，这对这一领域的集体进步构成了挑战。我们推出 LEGENT，这是一个开放、可扩展的平台，用于使用 LLM 和 LMM 开发实体代理。 LEGENT 提供了双重方法：丰富的交互式 3D 环境，具有可通信和可操作的代理，搭配用户友好的界面，以及利用先进算法大规模利用模拟世界监督的复杂数据生成管道。在我们的实验中，在 LEGENT 生成的数据上训练的胚胎视觉-语言-动作模型在具体任务中超越了 GPT-4V，展示了有前途的泛化能力。]]></description>
      <guid>https://arxiv.org/abs/2404.18243</guid>
      <pubDate>Tue, 30 Apr 2024 02:35:41 GMT</pubDate>
    </item>
    </channel>
</rss>