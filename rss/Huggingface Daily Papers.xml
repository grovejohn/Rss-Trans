<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>拥抱日报</title>
    <link>https://huggingface.co/papers</link>
    <description>Huggingface Daily Papers - 由 RSSHub 用爱制作（https://github.com/DIYgod/RSSHub）</description>
    <lastBuildDate>Tue, 26 Mar 2024 17:36:05 GMT</lastBuildDate>
    <item>
      <title>RakutenAI-7B：扩展日语的大型语言模型</title>
      <link>https://arxiv.org/abs/2403.15484</link>
      <description><![CDATA[我们推出 RakutenAI-7B，这是一套面向日语的大语言模型，在开放的 7B 模型中，它在日语 LM Harness 基准测试中实现了最佳性能。除了基础模型之外，我们还根据 Apache 2.0 许可证分别发布了指令和聊天调整模型 RakutenAI-7B-instruct 和 RakutenAI-7B-chat。]]></description>
      <guid>https://arxiv.org/abs/2403.15484</guid>
      <pubDate>Tue, 26 Mar 2024 04:23:17 GMT</pubDate>
    </item>
    <item>
      <title>解码压缩信任：审查压缩下高效法学硕士的可信度</title>
      <link>https://arxiv.org/abs/2403.15447</link>
      <description><![CDATA[压缩高性能大型语言模型 (LLM) 已成为资源高效推理的首选策略。虽然最先进的 (SoTA) 压缩方法在保持良性任务性能方面取得了令人印象深刻的进步，但压缩在安全性和可信度方面的潜在风险在很大程度上被忽视了。本研究使用五 (5) 项 SoTA 压缩技术，涵盖八 (8) 个可信度维度，对三 (3) 名领先的法学硕士进行了首次全面评估。我们的实验强调了压缩和可信度之间复杂的相互作用，揭示了一些有趣的模式。我们发现，在同时实现效率和可信度方面，量化目前是比剪枝更有效的方法。例如，4 位量化模型保留了其原始对应模型的可信度，但模型剪枝会显着降低可信度，即使稀疏度为 50% 也是如此。此外，在适度的比特范围内采用量化可以出乎意料地提高某些可信度维度，例如道德和公平性。相反，极端量化到非常低的位级别（3 位）往往会显着降低可信度。这种增加的风险不能仅通过观察良性表现来发现，进而要求在实践中进行全面的可信度评估。这些发现最终为法学硕士同时实现高实用性、效率和可信度提出了实用建议。模型和代码可在 https://decoding-comp-trust.github.io/ 获取。]]></description>
      <guid>https://arxiv.org/abs/2403.15447</guid>
      <pubDate>Tue, 26 Mar 2024 04:17:36 GMT</pubDate>
    </item>
    <item>
      <title>VP3D：释放 2D 视觉提示以生成文本到 3D</title>
      <link>https://arxiv.org/abs/2403.17001</link>
      <description><![CDATA[文本到 3D 生成的最新创新以分数蒸馏采样 (SDS) 为特色，它通过直接从 2D 扩散模型中提取先验知识来实现​​隐式 3D 模型 (NeRF) 的零样本学习。然而，当前基于 SDS 的模型仍然难以应对复杂的文本提示，并且通常会导致 3D 模型扭曲、纹理不真实或跨视图不一致问题。在这项工作中，我们引入了一种新颖的视觉提示引导的文本到 3D 扩散模型 (VP3D)，该模型明确释放 2D 视觉提示中的视觉外观知识，以促进文本到 3D 的生成。 VP3D 不是仅仅通过文本提示来监督 SDS，而是首先利用 2D 扩散模型从输入文本生成高质量图像，然后作为视觉提示，通过明确的视觉外观来加强 SDS 优化。同时，我们将 SDS 优化与额外的可微奖励函数结合起来，鼓励 3D 模型的渲染图像在视觉上更好地与 2D 视觉提示对齐，并在语义上与文本提示匹配。通过大量实验，我们表明 VP3D 中的 2D 视觉提示显着简化了 3D 模型视觉外观的学习，从而通过更详细的纹理带来更高的视觉保真度。同样吸引人的是，当用给定的参考图像替换自生成的视觉提示时，VP3D 能够触发样式化文本到 3D 生成的新任务。我们的项目页面位于 https://vp3d-cvpr24.github.io。]]></description>
      <guid>https://arxiv.org/abs/2403.17001</guid>
      <pubDate>Tue, 26 Mar 2024 04:06:02 GMT</pubDate>
    </item>
    <item>
      <title>做你自己：多主题文本到图像生成的有限注意力</title>
      <link>https://arxiv.org/abs/2403.16990</link>
      <description><![CDATA[文本到图像的扩散模型具有前所未有的生成多样化和高质量图像的能力。然而，他们常常难以忠实地捕捉包含多个主题的复杂输入提示的预期语义。最近，引入了许多布局到图像的扩展来改进用户控制，旨在本地化由特定标记表示的主题。然而，这些方法通常会产生语义不准确的图像，特别是在处理多个语义或视觉相似的主题时。在这项工作中，我们研究并分析了这些限制的原因。我们的探索表明，主要问题源于去噪过程中主体之间无意的语义泄漏。这种泄漏归因于扩散模型的注意力层，它倾向于融合不同主体的视觉特征。为了解决这些问题，我们引入了 Bounded Attention，这是一种无需训练的方法，用于限制采样过程中的信息流。有限注意力可以防止受试者之间的有害泄漏，并能够指导一代人促进每个受试者的个性，即使在复杂的多受试者调节下也是如此。通过广泛的实验，我们证明我们的方法能够生成多个主题，更好地符合给定的提示和布局。]]></description>
      <guid>https://arxiv.org/abs/2403.16990</guid>
      <pubDate>Tue, 26 Mar 2024 03:59:48 GMT</pubDate>
    </item>
    <item>
      <title>FlashFace：具有高保真身份保护的人类图像个性化</title>
      <link>https://arxiv.org/abs/2403.17008</link>
      <description><![CDATA[这项工作提出了 FlashFace，这是一种实用工具，用户可以通过提供一个或几个参考面部图像和文本提示，轻松地动态个性化自己的照片。我们的方法与现有的人类照片定制方法的区别在于更高保真度的身份保存和更好的指令遵循，这得益于两种微妙的设计。首先，我们将面部身份编码为一系列特征图，而不是像现有技术中的一个图像标记，从而允许模型保留参考面部的更多细节（例如，疤痕、纹身和面部形状）。其次，我们引入了一种解开的集成策略，以在文本到图像的生成过程中平衡文本和图像引导，减轻参考面孔和文本提示之间的冲突（例如，将成人个性化为“孩子”或“长老”）。大量的实验结果证明了我们的方法在各种应用中的有效性，包括人体图像个性化、语言提示下的面部交换、将虚拟角色变成真人等。项目页面：https://jshilong.github.io/flashface-page。]]></description>
      <guid>https://arxiv.org/abs/2403.17008</guid>
      <pubDate>Tue, 26 Mar 2024 03:53:11 GMT</pubDate>
    </item>
    <item>
      <title>LLM代理操作系统</title>
      <link>https://arxiv.org/abs/2403.16971</link>
      <description><![CDATA[基于大语言模型 (LLM) 的智能代理的集成和部署充满了挑战，影响了其效率和功效。这些问题包括代理通过 LLM 请求的次优调度和资源分配、代理与 LLM 之间交互期间维护上下文的困难，以及集成具有不同功能和专业的异构代理所固有的复杂性。代理数量和复杂性的快速增加进一步加剧了这些问题，通常会导致资源瓶颈和次优利用。受这些挑战的启发，本文提出了 AIOS，一种 LLM 代理操作系统，它将大型语言模型嵌入到操作系统 (OS) 中。具体来说，AIOS旨在优化资源分配，促进跨代理的上下文切换，实现代理的并发执行，为代理提供工具服务，并维护代理的访问控制。我们提出了这样一个操作系统的架构，概述了它旨在解决的核心挑战，并提供了 AIOS 的基本设计和实现。我们对多个代理并发执行的实验证明了我们的 AIOS 模块的可靠性和效率。通过这一点，我们的目标不仅是提高LLM代理的性能和效率，而且是未来更好地开发和部署AIOS生态系统的先锋。该项目是开源的，位于 https://github.com/agiresearch/AIOS。]]></description>
      <guid>https://arxiv.org/abs/2403.16971</guid>
      <pubDate>Tue, 26 Mar 2024 03:49:01 GMT</pubDate>
    </item>
    <item>
      <title>SDXS：具有图像条件的实时一步潜扩散模型</title>
      <link>https://arxiv.org/abs/2403.16627</link>
      <description><![CDATA[扩散模型的最新进展使其处于图像生成的最前沿。尽管扩散模型性能优越，但也并非没有缺点。它们的特点是复杂的架构和大量的计算需求，由于迭代采样过程而导致显着的延迟。为了缓解这些限制，我们引入了一种涉及模型小型化和减少采样步骤的双重方法，旨在显着减少模型延迟。我们的方法利用知识蒸馏来简化 U-Net 和图像解码器架构，并引入了一种利用特征匹配和分数蒸馏的创新一步 DM 训练技术。我们推出了两种模型：SDXS-512 和 SDXS-1024，在单个 GPU 上分别实现了约 100 FPS（比 SD v1.5 快 30 倍）和 30 FP（比 SDXL 快 60 倍）的推理速度。此外，我们的训练方法在图像条件控制中提供了有前景的应用，促进高效的图像到图像的转换。]]></description>
      <guid>https://arxiv.org/abs/2403.16627</guid>
      <pubDate>Tue, 26 Mar 2024 03:47:21 GMT</pubDate>
    </item>
    <item>
      <title>TRIP：图像到视频扩散模型的图像噪声先验的时间残差学习</title>
      <link>https://arxiv.org/abs/2403.17005</link>
      <description><![CDATA[文本到视频生成的最新进展证明了强大的扩散模型的实用性。然而，当塑造扩散模型以动画静态图像（即图像到视频生成）时，问题并不是微不足道的。困难源于以下方面：后续动画帧的扩散过程不仅要保持与给定图像的忠实对齐，还要追求相邻帧之间的时间连贯性。为了缓解这个问题，我们提出了 TRIP，一种图像到视频扩散范例的新配方，它以静态图像先验的图像噪声为基础，共同触发帧间关系推理，并通过时间残差学习简化连贯时间建模。从技术上讲，图像噪声先验是首先通过基于静态图像和噪声视频潜码的一步后向扩散过程获得的。接下来，TRIP执行类似残差的双路径方案进行噪声预测：1）直接将图像噪声先验作为每帧的参考噪声来放大第一帧和后续帧之间的对齐的捷径路径； 2）残差路径，在噪声视频和静态图像潜在代码上采用3D-UNet来实现帧间关系推理，从而简化对每帧残差噪声的学习。此外，每帧的参考噪声和残留噪声都通过注意机制动态合并，以生成最终的视频。在 WebVid-10M、DTDB 和 MSR-VTT 数据集上进行的大量实验证明了我们的 TRIP 在图像到视频生成方面的有效性。请参阅我们的项目页面：https://trip-i2v.github.io/TRIP/。]]></description>
      <guid>https://arxiv.org/abs/2403.17005</guid>
      <pubDate>Tue, 26 Mar 2024 03:41:57 GMT</pubDate>
    </item>
    </channel>
</rss>