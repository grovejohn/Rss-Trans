<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>拥抱日报</title>
    <link>https://huggingface.co/papers</link>
    <description>Huggingface 日报 - 由 RSSHub(https://github.com/DIYgod/RSSHub) 精心制作</description>
    <lastBuildDate>Fri, 26 Apr 2024 04:14:40 GMT</lastBuildDate>
    <item>
      <title>MoDE：通过聚类获得 CLIP 数据专家</title>
      <link>https://arxiv.org/abs/2404.16030</link>
      <description><![CDATA[对比语言图像预训练（CLIP）的成功依赖于图像和字幕之间配对的监督，这在网络爬虫数据中往往是有噪声的。我们提出混合数据专家（MoDE）并通过聚类学习 CLIP 数据专家系统。每个数据专家都在一个数据集群上进行训练，对其他集群中的假阴性噪声不太敏感。在推理时，我们通过应用通过任务元数据和集群条件之间的相关性确定的权重来集成它们的输出。为了精确地估计相关性，一个簇中的样本应该在语义上相似，但数据专家的数量仍然应该合理，以便于训练和推理。因此，我们考虑人类语言的本体，并建议使用细粒度的聚类中心来表示粗粒度级别的每个数据专家。实验研究表明，ViT-B/16 上的四名 CLIP 数据专家在零样本图像分类方面优于 OpenAI CLIP 和 OpenCLIP 的 ViT-L/14，但训练成本更低（&lt;35%）。同时，MoDE可以异步训练所有数据专家，并可以灵活地包含新的数据专家。该代码可在 https://github.com/facebookresearch/MetaCLIP/tree/main/mode 获取。]]></description>
      <guid>https://arxiv.org/abs/2404.16030</guid>
      <pubDate>Thu, 25 Apr 2024 18:10:52 GMT</pubDate>
    </item>
    <item>
      <title>CatLIP：CLIP 级视觉识别精度，网络规模图像文本数据预训练速度提高 2.7 倍</title>
      <link>https://arxiv.org/abs/2404.15653</link>
      <description><![CDATA[对比学习已成为一种通过图像和文本嵌入的对齐来学习有效视觉表示的变革方法。然而，图像和文本对之间的对比损失中的成对相似性计算提出了计算挑战。本文提出了一种基于网络规模图像文本数据的视觉模型的新型弱监督预训练。该方法将图像文本数据的预训练重新定义为分类任务。因此，它消除了对比损失中成对相似性计算的需要，与网络规模数据的对比学习相比，训练速度显着提高了 2.7 倍。通过跨越不同视觉任务（包括检测和分割）的广泛实验，我们证明所提出的方法保持了较高的表示质量。我们的源代码以及预训练的模型权重和训练配方可在 https://github.com/apple/corenet 上获取。]]></description>
      <guid>https://arxiv.org/abs/2404.15653</guid>
      <pubDate>Thu, 25 Apr 2024 18:00:59 GMT</pubDate>
    </item>
    <item>
      <title>MaGGIe：蒙面引导渐进人体实例抠图</title>
      <link>https://arxiv.org/abs/2404.16035</link>
      <description><![CDATA[人体抠图是图像和视频处理中的一项基础任务，其中从输入中提取人体前景像素。先前的工作要么通过额外的指导来提高准确性，要么提高跨帧的单个实例的时间一致性。我们提出了一个新的框架 MaGGIe，Masked Guided Gradual Human Instance Matting，它逐步预测每个人类实例的 alpha mattes，同时保持计算成本、精度和一致性。我们的方法利用现代架构，包括变压器注意力和稀疏卷积，同时输出所有实例遮罩，而不会增加内存和延迟。尽管在多实例场景中保持恒定的推理成本，但我们的框架在我们提出的综合基准上实现了稳健且通用的性能。随着更高质量的图像和视频抠图基准，引入了来自公开来源的新颖的多实例合成方法，以提高模型在现实场景中的泛化能力。]]></description>
      <guid>https://arxiv.org/abs/2404.16035</guid>
      <pubDate>Thu, 25 Apr 2024 17:56:33 GMT</pubDate>
    </item>
    <item>
      <title>XC-Cache：交叉参与缓存上下文以实现高效的 LLM 推理</title>
      <link>https://arxiv.org/abs/2404.15420</link>
      <description><![CDATA[上下文学习 (ICL) 方法通常利用提示来根据参考信息生成仅解码器的语言模型。由于自注意力操作的二次成本，上下文的即时处理效率低下，因此需要缓存。然而，缓存变压器状态很容易需要几乎与模型参数一样多的空间。当事先不知道正确的上下文时，缓存 ICL 可能会很困难。这项工作通过引入模型来解决这些限制，这些模型受编码器-解码器架构的启发，使用交叉注意力来在没有提示的情况下对参考文本进行条件生成。更准确地说，我们利用预训练的仅解码器模型，并且仅训练少量的添加层。我们使用问答（QA）作为测试平台来评估我们的模型执行条件生成的能力，并观察到它们的性能优于 ICL，与微调提示的 LLM 相当，并且相对于标准 KV 缓存大幅减少了空间占用：两个数量级。]]></description>
      <guid>https://arxiv.org/abs/2404.15420</guid>
      <pubDate>Thu, 25 Apr 2024 17:48:48 GMT</pubDate>
    </item>
    <item>
      <title>用于可控合成的可编辑图像元素</title>
      <link>https://arxiv.org/abs/2404.16029</link>
      <description><![CDATA[扩散模型在文本引导的合成任务中取得了重大进展。然而，编辑用户提供的图像仍然具有挑战性，因为扩散模型的高维噪声输入空间并不自然适合图像反演或空间编辑。在这项工作中，我们提出了一种图像表示，可以使用扩散模型促进输入图像的空间编辑。具体来说，我们学习将输入编码为可以忠实地重建输入图像的“图像元素”。这些元素可以由用户直观地编辑，并通过扩散模型解码为逼真的图像。我们展示了我们的表示在各种图像编辑任务上的有效性，例如对象调整大小、重新排列、拖动、去遮挡、移除、变化和图像合成。项目页面：https://jitengmu.github.io/Editable_Image_Elements/]]></description>
      <guid>https://arxiv.org/abs/2404.16029</guid>
      <pubDate>Thu, 25 Apr 2024 17:44:33 GMT</pubDate>
    </item>
    <item>
      <title>BASS：批量注意力优化推测采样</title>
      <link>https://arxiv.org/abs/2404.15778</link>
      <description><![CDATA[推测性解码已成为改善托管大型语言模型的延迟和吞吐量的强大方法。然而，大多数现有的实现都集中于生成单个序列。现实世界的生成式人工智能应用程序通常需要多个响应，如何在批量设置中执行推测性解码，同时保留其延迟优势带来了不小的挑战。本文描述了一种批量推测解码系统，该系统在多序列生成延迟方面设定了新的技术水平，并展示了卓越的 GPU 利用率以及时间预算内的生成质量。例如，对于单个 A100 GPU 上的 7.8B 大小的模型，批量大小为 8，每个序列的生成速度平均为每个令牌 5.8 毫秒，总体吞吐量为每秒 1.1K 个令牌。这些结果代表了最先进的延迟，并且比优化的常规解码速度提高了 2.15 倍。在常规解码无法完成的时间预算内，我们的系统能够生成 HumanEval Pass@First 为 43% 和 Pass@All 为 61% 的序列，远远超过单序列推测解码的可行性。我们在解码过程中的峰值 GPU 利用率高达 15.8%，是常规解码最高利用率的 3 倍以上，是单序列推测解码最高利用率的 10 倍左右。]]></description>
      <guid>https://arxiv.org/abs/2404.15778</guid>
      <pubDate>Thu, 25 Apr 2024 17:30:25 GMT</pubDate>
    </item>
    <item>
      <title>MotionMaster：用于视频生成的免训练相机运动传输</title>
      <link>https://arxiv.org/abs/2404.15789</link>
      <description><![CDATA[扩散模型的出现极大地推动了图像和视频生成的进步。最近，在可控视频生成方面做出了一些努力，包括文本到视频生成和视频运动控制，其中相机运动控制是一个重要的课题。然而，现有的相机运动控制方法依赖于训练时间相机模块，并且由于视频生成模型中的大量参数而需要大量的计算资源。此外，现有方法在训练期间预先定义相机运动类型，这限制了它们在相机控制方面的灵活性。因此，为了降低训练成本并实现灵活的摄像机控制，我们提出了 COMD，一种新颖的免训练视频运动传输模型，它将源视频中的摄像机运动和物体运动分开，并将提取的摄像机运动传输到新视频。我们首先提出了一种一次性摄像机运动解缠方法，从单个源视频中提取摄像机运动，该方法将运动对象与背景分离，并通过求解泊松方程，根据背景中的运动来估计运动对象区域中的摄像机运动。方程。此外，我们提出了一种少镜头相机运动解缠结方法，从具有相似相机运动的多个视频中提取共同的相机运动，该方法采用基于窗口的聚类技术来提取多个视频的时间注意力图中的共同特征。最后，我们提出了一种运动组合方法，将不同类型的相机运动组合在一起，使我们的模型更加可控和灵活的相机控制。大量的实验表明，我们的免训练方法可以有效地解耦相机与物体的运动，并将解耦的相机运动应用于广泛的可控视频生成任务，实现灵活多样的相机运动控制。]]></description>
      <guid>https://arxiv.org/abs/2404.15789</guid>
      <pubDate>Thu, 25 Apr 2024 15:28:40 GMT</pubDate>
    </item>
    <item>
      <title>PuLID：通过对比对齐进行 Pure 和 Lightning ID 定制</title>
      <link>https://arxiv.org/abs/2404.16022</link>
      <description><![CDATA[我们提出了 Pure and Lightning ID 定制 (PuLID)，这是一种用于文本到图像生成的新型无调整 ID 定制方法。通过将 Lightning T2I 分支与标准扩散分支相结合，PuLID 引入了对比对齐损失和准确 ID 损失，最大限度地减少了对原始模型的破坏并确保了高 ID 保真度。实验表明，PuLID 在 ID 保真度和可编辑性方面都取得了优异的表现。PuLID 的另一个吸引人的特性是，插入 ID 之前和之后的图像元素（例如背景、灯光、构图和样式）尽可能保持一致。代码和模型将在 https://github.com/ToTheBeginning/PuLID 上提供]]></description>
      <guid>https://arxiv.org/abs/2404.16022</guid>
      <pubDate>Thu, 25 Apr 2024 15:08:29 GMT</pubDate>
    </item>
    <item>
      <title>ID-Aligner：通过奖励反馈学习增强保留身份的文本到图像的生成</title>
      <link>https://arxiv.org/abs/2404.15449</link>
      <description><![CDATA[扩散模型的快速发展引发了多样化的应用。身份保留文本到图像生成（ID-T2I）因其广泛的应用场景（例如人工智能肖像和广告）而受到广泛关注。虽然现有的 ID-T2I 方法已经展示了令人印象深刻的结果，但仍然存在一些关键挑战：（1）很难准确地保持参考肖像的身份特征，（2）生成的图像缺乏审美吸引力，特别是在执行身份保留时，以及（3） ）存在无法同时兼容基于LoRA和基于Adapter的方法的限制。为了解决这些问题，我们提出了 ID-Aligner，这是一个用于增强 ID-T2I 性能的通用反馈学习框架。为了解决身份特征丢失的问题，我们引入了身份一致性奖励微调，以利用人脸检测和识别模型的反馈来改善生成的身份保留。此外，我们提出身份美学奖励微调，利用人类注释的偏好数据的奖励和自动构建的角色结构生成反馈来提供美学调整信号。得益于其通用反馈微调框架，我们的方法可以轻松应用于 LoRA 和 Adapter 模型，从而实现一致的性能增益。 SD1.5 和 SDXL 扩散模型的大量实验验证了我们方法的有效性。项目页面：\url{https://idaligner.github.io/}]]></description>
      <guid>https://arxiv.org/abs/2404.15449</guid>
      <pubDate>Thu, 25 Apr 2024 15:02:19 GMT</pubDate>
    </item>
    </channel>
</rss>