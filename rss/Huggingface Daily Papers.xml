<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>拥抱日报</title>
    <link>https://huggingface.co/papers</link>
    <description>Huggingface Daily Papers - 由 RSSHub 用爱制作（https://github.com/DIYgod/RSSHub）</description>
    <lastBuildDate>Tue, 16 Apr 2024 04:35:58 GMT</lastBuildDate>
    <item>
      <title>驯服神经辐射场修复的潜在扩散模型</title>
      <link>https://arxiv.org/abs/2404.09995</link>
      <description><![CDATA[神经辐射场 (NeRF) 是多视图图像 3D 重建的一种表示。尽管最近的一些工作显示在使用扩散先验编辑重建的 NeRF 方面取得了初步成功，但他们仍然在努力在完全未覆盖的区域中合成合理的几何结构。一个主要原因是扩散模型合成内容的高度多样性，这阻碍了辐射场收敛到清晰且确定性的几何形状。此外，由于自动编码错误，在实际数据上应用潜在扩散模型通常会产生与图像条件不相干的纹理变化。使用像素距离损失进一步加剧了这两个问题。为了解决这些问题，我们建议通过按场景定制来调节扩散模型的随机性，并通过蒙面对抗训练来减轻纹理变化。在分析过程中，我们还发现常用的像素和感知损失在 NeRF 修复任务中是有害的。通过严格的实验，我们的框架在各种现实场景中产生了最先进的 NeRF 修复结果。项目页面：https://hubert0527.github.io/MALD-NeRF]]></description>
      <guid>https://arxiv.org/abs/2404.09995</guid>
      <pubDate>Tue, 16 Apr 2024 03:11:46 GMT</pubDate>
    </item>
    <item>
      <title>HQ-Edit：用于基于指令的图像编辑的高质量数据集</title>
      <link>https://arxiv.org/abs/2404.09990</link>
      <description><![CDATA[本研究引入了 HQ-Edit，这是一个基于指令的高质量图像编辑数据集，包含约 200,000 次编辑。与之前依赖属性指导或人工反馈来构建数据集的方法不同，我们利用先进的基础模型（即 GPT-4V 和 DALL-E 3）设计了一个可扩展的数据收集管道。为了确保其高质量，首先在线收集不同的示例，然后进行扩展，然后用于创建高质量的双联画，其中包含带有详细文本提示的输入和输出图像，然后通过后处理确保精确对齐。此外，我们提出了两个评估指标：对齐和连贯性，以使用 GPT-4V 定量评估图像编辑对的质量。 HQ-Edits 高分辨率图像，细节丰富，并配有全面的编辑提示，大大增强了现有图像编辑模型的功能。例如，经过 HQ-Edit 微调的 InstructPix2Pix 可以获得最先进的图像编辑性能，甚至超越那些使用人工注释数据微调的模型。项目页面为https://thefllood.github.io/HQEdit_web。]]></description>
      <guid>https://arxiv.org/abs/2404.09990</guid>
      <pubDate>Tue, 16 Apr 2024 03:04:31 GMT</pubDate>
    </item>
    <item>
      <title>了解您的参考模型以实现真正的良好对准</title>
      <link>https://arxiv.org/abs/2404.09656</link>
      <description><![CDATA[对齐问题的复杂性源于现有方法不稳定。研究人员不断发明各种技巧来解决这个缺点。例如，在语言模型对齐的基本人类反馈强化学习 (RLHF) 技术中，除了奖励最大化之外，可训练策略和 SFT 策略之间的 Kullback-Leibler 差异也被最小化。此添加可防止模型过度拟合奖励模型 (RM) 并生成 RM 域外的文本。直接偏好优化（DPO）方法重新表述了RLHF的优化任务，消除了奖励模型，同时默认保持策略接近SFT策略的要求。在我们的论文中，我们认为 DPO 方法中的这种隐含限制会导致次优结果。我们提出了一种称为信任区域 DPO (TR-DPO) 的新方法，它在训练期间更新参考策略。通过如此简单的更新，我们在 Anthropic HH 和 TLDR 数据集上证明了 TR-DPO 相对于 DPO 的有效性。我们通过 GPT-4 自动评估进行测量，结果表明 TR-DPO 的性能比 DPO 高出 19%。我们提出的新对齐方法使我们能够同时提高多个参数的模型质量，例如连贯性、正确性、细节水平、有用性和无害性。]]></description>
      <guid>https://arxiv.org/abs/2404.09656</guid>
      <pubDate>Tue, 16 Apr 2024 03:00:09 GMT</pubDate>
    </item>
    <item>
      <title>Ctrl-Adapter：一个高效且多功能的框架，用于使各种控制适应任何扩散模型</title>
      <link>https://arxiv.org/abs/2404.09967</link>
      <description><![CDATA[ControlNet 广泛用于在不同条件下的图像生成中添加空间控制，例如深度图、精明边缘和人体姿势。然而，利用预训练图像 ControlNet 进行受控视频生成时存在一些挑战。首先，由于特征空间的不匹配，预训练的ControlNet无法直接插入新的骨干模型，并且为新的骨干网络训练ControlNet的成本是一个很大的负担。其次，不同帧的 ControlNet 特征可能无法有效处理时间一致性。为了应对这些挑战，我们引入了 Ctrl-Adapter，这是一种高效且多功能的框架，通过调整预训练的 ControlNet（并改进视频的​​时间对齐），为任何图像/视频扩散模型添加多种控制。 Ctrl-Adapter 提供多种功能，包括图像控制、视频控制、稀疏帧视频控制、多条件控制、与不同骨干网的兼容性、适应不可见的控制条件以及视频编辑。在 Ctrl-Adapter 中，我们训练适配器层，将预训练的 ControlNet 特征融合到不同的图像/视频扩散模型，同时保持 ControlNet 和扩散模型的参数冻结。 Ctrl-Adapter由时间和空间模块组成，可以有效处理视频的时间一致性。我们还提出了潜在跳跃和反时间步采样，以实现鲁棒自适应和稀疏控制。此外，Ctrl-Adapter 只需取 ControlNet 输出的（加权）平均值即可实现多种条件下的控制。凭借不同的图像/视频扩散主干（SDXL、Hotshot-XL、I2VGen-XL 和 SVD），Ctrl-Adapter 在图像控制方面与 ControlNet 相匹配，并优于视频控制的所有基线（在 DAVIS 2017 数据集上实现了 SOTA 精度）更低的计算成本（少于 10 个 GPU 小时）。]]></description>
      <guid>https://arxiv.org/abs/2404.09967</guid>
      <pubDate>Tue, 16 Apr 2024 02:55:25 GMT</pubDate>
    </item>
    <item>
      <title>Tango 2：通过直接偏好优化调整基于扩散的文本到音频生成</title>
      <link>https://arxiv.org/abs/2404.09956</link>
      <description><![CDATA[生成式多模式内容在许多内容创作领域越来越普遍，因为它有潜力允许艺术家和媒体人员通过快速将他们的想法变为现实来创建预制作模型。从文本提示生成音频是音乐和电影行业此类过程的一个重要方面。最近许多基于扩散的文本到音频模型都专注于在大量提示音频对数据集上训练日益复杂的扩散模型。这些模型没有明确关注概念或事件的存在及其在输出音频中相对于输入提示的时间顺序。我们的假设重点是音频生成的这些方面如何在数据有限的情况下提高音频生成性能。因此，在这项工作中，我们使用现有的文本到音频模型 Tango，综合创建一个偏好数据集，其中每个提示都有一个获胜者音频输出和一些失败者音频输出，供扩散模型学习。理论上，失败者的输出有一些来自提示的概念缺失或顺序不正确。我们在偏好数据集上使用扩散-DPO（直接偏好优化）损失对公开可用的 Tango 文本到音频模型进行了微调，并表明它在自动和手动方面都比 Tango 和 AudioLDM2 改进了音频输出- 评估指标。]]></description>
      <guid>https://arxiv.org/abs/2404.09956</guid>
      <pubDate>Tue, 16 Apr 2024 02:41:27 GMT</pubDate>
    </item>
    </channel>
</rss>