<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>拥抱日报</title>
    <link>https://huggingface.co/papers</link>
    <description>Huggingface Daily Papers - RSSHub 用爱制作的(https://github.com/DIYgod/RSSHub)</description>
    <lastBuildDate>Tue, 19 Mar 2024 05:13:02 GMT</lastBuildDate>
    <item>
      <title>SV3D：使用潜在视频扩散从单个图像进行新颖的多视图合成和 3D 生成</title>
      <link>https://arxiv.org/abs/2403.12008</link>
      <description><![CDATA[我们提出了稳定视频 3D (SV3D)——一种潜在视频扩散模型，用于围绕 3D 对象生成高分辨率、图像到多视图的轨道视频。最近关于 3D 生成的工作提出了调整 2D 生成模型以实现新视图合成 (NVS) 和 3D 优化的技术。然而，由于视图有限或 NVS 不一致，这些方法存在一些缺点，从而影响 3D 对象生成的性能。在这项工作中，我们提出了 SV3D，它采用图像到视频扩散模型来实现新颖的多视图合成和 3D 生成，从而利用视频模型的泛化和多视图一致性，同时进一步为 NVS 添加显式摄像机控制。我们还提出了改进的 3D 优化技术，以使用 SV3D 及其 NVS 输出进行图像到 3D 的生成。对具有 2D 和 3D 指标的多个数据集的广泛实验结果以及用户研究证明，与之前的作品相比，SV3D 在 NVS 以及 3D 重建方面具有最先进的性能。]]></description>
      <guid>https://arxiv.org/abs/2403.12008</guid>
      <pubDate>Tue, 19 Mar 2024 03:46:47 GMT</pubDate>
    </item>
    <item>
      <title>Infinite-ID：通过 ID 语义解耦范式实现身份保留的个性化</title>
      <link>https://arxiv.org/abs/2403.11781</link>
      <description><![CDATA[利用文本到图像生成扩散模型的最新进展，保留身份的个性化在仅使用单个参考图像准确捕获特定身份方面取得了重大进展。然而，现有的方法主要将参考图像集成到文本嵌入空间中，导致图像和文本信息的复杂纠缠，这对保持身份保真度和语义一致性提出了挑战。为了应对这一挑战，我们提出了 Infinite-ID，这是一种用于身份保留个性化的 ID 语义解耦范例。具体来说，我们引入了身份增强训练，结合了一个额外的图像交叉注意模块来捕获足够的ID信息，同时停用扩散模型的原始文本交叉注意模块。这确保图像流忠实地表示参考图像提供的身份，同时减轻文本输入的干扰。此外，我们引入了一种特征交互机制，它将混合注意力模块与 AdaIN-mean 操作相结合，以无缝合并两个流。这种机制不仅增强了身份的保真度和语义的一致性，而且还可以方便地控制生成图像的风格。原始照片生成和风格图像生成的大量实验结果证明了我们提出的方法的优越性能。]]></description>
      <guid>https://arxiv.org/abs/2403.11781</guid>
      <pubDate>Tue, 19 Mar 2024 03:35:04 GMT</pubDate>
    </item>
    <item>
      <title>LightIt：漫射模型的照明建模和控制</title>
      <link>https://arxiv.org/abs/2403.10615</link>
      <description><![CDATA[我们介绍 LightIt，一种用于图像生成的显式照明控制的方法。最近的生成方法缺乏照明控制，这对于图像生成的许多艺术方面（例如设置整体情绪或电影外观）至关重要。为了克服这些限制，我们建议以着色和法线贴图为生成条件。我们使用单次反射着色对光照进行建模，其中包括投射阴影。我们首先训练一个阴影估计模块来生成真实世界图像和阴影对的数据集。然后，我们使用估计的阴影和法线作为输入来训练控制网络。我们的方法在许多场景中展示了高质量的图像生成和照明控制。此外，我们使用生成的数据集来训练以图像和目标着色为条件的身份保留重新照明模型。我们的方法是第一个能够使用可控、一致的照明生成图像的方法，并且其性能与专门的重新照明最先进的方法相当。]]></description>
      <guid>https://arxiv.org/abs/2403.10615</guid>
      <pubDate>Tue, 19 Mar 2024 03:21:57 GMT</pubDate>
    </item>
    <item>
      <title>具有潜在对抗扩散蒸馏的快速高分辨率图像合成</title>
      <link>https://arxiv.org/abs/2403.12015</link>
      <description><![CDATA[扩散模型是图像和视频合成进步的主要驱动力，但推理速度慢。蒸馏方法，例如最近引入的对抗扩散蒸馏（ADD），旨在将模型从多次推理转变为单步推理，尽管由于其依赖于固定的预训练 DINOv2 判别器，因此代价昂贵且难以优化。我们引入潜在对抗扩散蒸馏（LADD），这是一种克服 ADD 局限性的新颖蒸馏方法。与基于像素的 ADD 相比，LADD 利用预训练的潜在扩散模型的生成特征。这种方法简化了训练并提高了性能，从而实现了高分辨率多宽高比图像合成。我们将 LADD 应用于稳定扩散 3 (8B) 以获得 SD3-Turbo，这是一种快速模型，仅使用四个无引导采样步骤即可与最先进的文本到图像生成器的性能相匹配。此外，我们系统地研究了它的缩放行为，并证明了 LADD 在图像编辑和修复等各种应用中的有效性。]]></description>
      <guid>https://arxiv.org/abs/2403.12015</guid>
      <pubDate>Tue, 19 Mar 2024 03:16:09 GMT</pubDate>
    </item>
    </channel>
</rss>