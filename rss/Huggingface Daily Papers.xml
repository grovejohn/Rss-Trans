<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>拥抱日报</title>
    <link>https://huggingface.co/papers</link>
    <description>Huggingface Daily Papers - RSSHub 用爱制作的(https://github.com/DIYgod/RSSHub)</description>
    <lastBuildDate>Thu, 28 Mar 2024 04:35:26 GMT</lastBuildDate>
    <item>
      <title>ViTAR：任何分辨率的视觉转换器</title>
      <link>https://arxiv.org/abs/2403.18361</link>
      <description><![CDATA[他的论文解决了视觉变换器 (ViTs) 面临的重大挑战：它们在不同图像分辨率下的可扩展性受到限制。通常，当处理的分辨率与训练期间看到的分辨率不同时，ViT 会出现性能下降。我们的工作引入了两项关键创新来解决这个问题。首先，我们提出了一种用于动态分辨率调整的新颖模块，该模块采用单个 Transformer 块设计，专门用于实现高效的增量代币集成。其次，我们在 Vision Transformer 中引入模糊位置编码，以在多个分辨率下提供一致的位置感知，从而防止对任何单一训练分辨率的过度拟合。我们的最终模型 ViTAR（任意分辨率的视觉变换器）展示了令人印象深刻的适应性，在 1120x1120 分辨率下实现 83.3% 的 top-1 精度，在 4032x4032 分辨率下实现 80.4% 的精度，同时降低了计算成本。 ViTAR 在实例和语义分割等下游任务中也表现出了强大的性能，并且可以轻松地与 Masked AutoEncoder 等自监督学习技术相结合。我们的工作为增强 ViT 的分辨率可扩展性提供了一种经济高效的解决方案，为更通用、更高效的高分辨率图像处理铺平了道路。]]></description>
      <guid>https://arxiv.org/abs/2403.18361</guid>
      <pubDate>Thu, 28 Mar 2024 02:25:00 GMT</pubDate>
    </item>
    <item>
      <title>BioMedLM：基于生物医学文本训练的 2.7B 参数语言模型</title>
      <link>https://arxiv.org/abs/2403.18421</link>
      <description><![CDATA[GPT-4 和 Med-PaLM 2 等模型在各种生物医学 NLP 任务中表现出了令人印象深刻的性能。然而，这些模型具有数千亿个参数，运行计算成本昂贵，需要用户通过互联网发送输入数据，并且需要在未知数据源上进行训练。更小、更有针对性的模型可以竞争吗？为了解决这个问题，我们构建并发布了 BioMedLM，这是一个 27 亿参数的 GPT 式自回归模型，专门在 PubMed 摘要和完整文章上进行训练。经过微调后，BioMedLM 可以产生强大的多项选择生物医学问答结果，可与更大的模型相媲美，例如在 MedMCQA (dev) 上获得 57.3% 的分数，在 MMLU 医学遗传学考试中获得 69.0% 的分数。 BioMedLM 还可以进行微调，为患者提出的医学主题问题提供有用的答案。这表明较小的模型有可能成为特定 NLP 应用（例如生物医学）的透明、隐私保护、经济和环保的基础。该模型可在 Hugging Face Hub 上找到：https://huggingface.co/stanford-crfm/BioMedLM。]]></description>
      <guid>https://arxiv.org/abs/2403.18421</guid>
      <pubDate>Thu, 28 Mar 2024 02:10:49 GMT</pubDate>
    </item>
    <item>
      <title>Mini-Gemini：挖掘多模态视觉语言模型的潜力</title>
      <link>https://arxiv.org/abs/2403.18814</link>
      <description><![CDATA[在这项工作中，我们介绍了 Mini-Gemini，这是一个增强多模态视觉语言模型（VLM）的简单而有效的框架。尽管 VLM 取得了进步，促进了基本的视觉对话和推理，但与 GPT-4 和 Gemini 等高级模型相比，性能差距仍然存在。我们试图从高分辨率视觉标记、高质量数据和 VLM 引导生成三个方面挖掘 VLM 的潜力，以实现更好的性能和任意工作流程，从而缩小差距。为了增强视觉标记，我们建议利用额外的视觉编码器进行高分辨率细化，而不增加视觉标记数量。我们进一步构建了一个高质量的数据集，促进精确的图像理解和基于推理的生成，扩大了当前 VLM 的操作范围。总的来说，Mini-Gemini 进一步挖掘了 VLM 的潜力，并同时为现有框架提供了图像理解、推理和生成的能力。 Mini-Gemini 支持一系列从 2B 到 34B 的密集和 MoE 大型语言模型 (LLM)。它在多个零样本基准测试中表现出领先的性能，甚至超越了已开发的私有模型。代码和模型可在 https://github.com/dvlab-research/MiniGemini 获取。]]></description>
      <guid>https://arxiv.org/abs/2403.18814</guid>
      <pubDate>Thu, 28 Mar 2024 02:06:11 GMT</pubDate>
    </item>
    <item>
      <title>大型语言模型中的长篇事实性</title>
      <link>https://arxiv.org/abs/2403.18802</link>
      <description><![CDATA[大型语言模型 (LLM) 在响应有关开放式主题的事实寻求提示时，通常会生成包含事实错误的内容。为了在开放领域对模型的长格式事实性进行基准测试，我们首先使用 GPT-4 生成 LongFact，这是一个包含 38 个主题的数千个问题的提示集。然后，我们建议 LLM 代理可以通过我们称为搜索增强事实评估器（SAFE）的方法用作长形式事实性的自动评估器。 SAFE 利用 LLM 将长格式响应分解为一组单独的事实，并使用多步骤推理过程评估每个事实的准确性，包括将搜索查询发送到 Google 搜索并确定搜索是否支持事实结果。此外，我们建议将 F1 分数扩展为长篇事实性的聚合指标。为此，我们平衡响应中支持的事实的百分比（精度）与相对于表示用户首选响应长度（召回）的超参数提供的事实的百分比。根据经验，我们证明 LLM 代理可以实现超人的评级性能 - 在一组约 16k 个单独事实上，SAFE 在 72% 的时间内与众包人类注释者一致，并且在 100 个分歧案例的随机子集上，SAFE 赢得了 76% 的结果时间。同时，SAFE 的成本比人类注释者便宜 20 倍以上。我们还在 LongFact 上对四个模型系列（Gemini、GPT、Claude 和 PaLM-2）的 13 种语言模型进行了基准测试，发现较大的语言模型通常可以实现更好的长格式事实性。 LongFact、SAFE 和所有实验代码均可在 https://github.com/google-deepmind/long-form-factuality 上获取。]]></description>
      <guid>https://arxiv.org/abs/2403.18802</guid>
      <pubDate>Thu, 28 Mar 2024 02:01:17 GMT</pubDate>
    </item>
    <item>
      <title>Gamba：将高斯泼溅法与 Mamba 结合起来进行单视图 3D 重建</title>
      <link>https://arxiv.org/abs/2403.18795</link>
      <description><![CDATA[随着对自动化 3D 内容创建管道的需求不断增长，我们应对从单个图像高效重建 3D 资产的挑战。以前的方法主要依赖于分数蒸馏采样（SDS）和神经辐射场（NeRF）。尽管取得了巨大的成功，但由于冗长的优化和大量的内存使用，这些方法遇到了实际限制。在本报告中，我们介绍了 Gamba，一种来自单视图图像的端到端摊销 3D 重建模型，强调了两个主要见解：（1）3D 表示：利用大量 3D 高斯进行高效的 3D 高斯泼溅过程； （2）主干设计：引入基于Mamba的顺序网络，促进上下文相关推理和序列（令牌）长度的线性可扩展性，容纳大量高斯。 Gamba 在数据预处理、正则化设计和训练方法方面取得了重大进展。我们使用现实世界扫描的 OmniObject3D 数据集根据现有的基于优化和前馈 3D 生成方法评估了 Gamba。在这里，Gamba 在质量和数量上都展示了具有竞争力的生成能力，同时实现了惊人的速度，在单个 NVIDIA A100 GPU 上约为 0.6 秒。]]></description>
      <guid>https://arxiv.org/abs/2403.18795</guid>
      <pubDate>Thu, 28 Mar 2024 01:45:01 GMT</pubDate>
    </item>
    </channel>
</rss>