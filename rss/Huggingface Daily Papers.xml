<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>拥抱日报</title>
    <link>https://huggingface.co/papers</link>
    <description>Huggingface Daily Papers - 由 RSSHub 用爱制作（https://github.com/DIYgod/RSSHub）</description>
    <lastBuildDate>Mon, 08 Apr 2024 04:37:22 GMT</lastBuildDate>
    <item>
      <title>搜索流（SoS）：学习用语言搜索</title>
      <link>https://arxiv.org/abs/2404.03683</link>
      <description><![CDATA[语言模型在训练时很少出现富有成果的错误。然后，他们努力去超越下一个标记，遭受滚雪球般的错误的困扰，并努力预测他们的行为的后果。在本文中，我们展示了如何通过将语言中的搜索过程表示为扁平化字符串——搜索流（SoS）来训练语言模型进行搜索。我们提出了一种统一的搜索语言，它捕获了一系列不同的符号搜索策略。我们使用简单但困难的倒计时游戏演示了我们的方法，其目标是将输入数字与算术运算相结合以达到目标数字。我们在启发式求解器生成的搜索流数据集上从头开始预训练基于变压器的语言模型。我们发现，与仅预测最佳搜索轨迹的模型相比，SoS 预训练的搜索精度提高了 25%。我们通过两种策略改进方法进一步微调该模型：优势诱导策略调整（APA）和自学推理器（STaR）。经过微调的 SoS 模型解决了 36% 以前未解决的问题，包括任何启发式求解器都无法解决的问题。我们的结果表明，语言模型可以学习通过搜索解决问题，自我改进以灵活使用不同的搜索策略，并有可能发现新的策略。]]></description>
      <guid>https://arxiv.org/abs/2404.03683</guid>
      <pubDate>Mon, 08 Apr 2024 02:38:04 GMT</pubDate>
    </item>
    <item>
      <title>直接纳什优化：教授语言模型根据一般偏好进行自我改进</title>
      <link>https://arxiv.org/abs/2404.03715</link>
      <description><![CDATA[本文研究了训练后大型语言模型 (LLM)，使用来自强大预言机的偏好反馈来帮助模型迭代地改进自身。培训后法学硕士的典型方法涉及人类反馈强化学习（RLHF），传统上它将奖励学习和后续策略优化分开。然而，这种奖励最大化方法受到“逐点”奖励（例如 Bradley-Terry 模型）性质的限制，无法表达复杂的不及物或循环偏好关系。虽然 RLHF 的进展表明奖励学习和策略优化可以合并为一个单一的对比目标以实现稳定性，但它们仍然受到奖励最大化框架的束缚。最近，新一波研究回避了奖励最大化假设，转而直接优化“成对”或一般偏好。在本文中，我们介绍了直接纳什优化（DNO），这是一种可证明且可扩展的算法，它将对比学习的简单性和稳定性与优化一般偏好的理论通用性结合起来。由于 DNO 是一种使用基于回归的目标的批处理策略算法，因此其实现简单且高效。此外，DNO 在迭代中享有单调改进，这有助于它甚至超越强大的教师（例如 GPT-4）。在我们的实验中，由 DNO 对齐的 7B 参数 Orca-2.5 模型在 AlpacaEval 2.0 上实现了针对 GPT-4-Turbo 的最先进的 33% 胜率（即使在控制响应长度之后），绝对的与初始化模型相比，增益提高了 26%（7% 至 33%）。它的性能优于具有更多参数的模型，包括 Mistral Large、Self-Rewarding LM（70B 参数）和旧版本的 GPT-4。]]></description>
      <guid>https://arxiv.org/abs/2404.03715</guid>
      <pubDate>Mon, 08 Apr 2024 02:03:29 GMT</pubDate>
    </item>
    <item>
      <title>一致性模型的强化学习：更快的奖励引导文本到图像生成</title>
      <link>https://arxiv.org/abs/2404.03673</link>
      <description><![CDATA[强化学习 (RL) 通过直接优化捕捉图像质量、美观和指令遵循能力的奖励，改进了扩散模型的引导图像生成。然而，由此产生的生成策略继承了导致生成缓慢的扩散模型的相同迭代采样过程。为了克服这一限制，一致性模型提出学习一类新的生成模型，将噪声直接映射到数据，从而产生一个可以在短短一次采样迭代中生成图像的模型。在这项工作中，为了优化特定任务奖励的文本到图像生成模型并实现快速训练和推理，我们提出了一个通过 RL 微调一致性模型的框架。我们的框架称为一致性模型强化学习 (RLCM)，将一致性模型的迭代推理过程构建为 RL 过程。 RLCM 在文本到图像生成功能上改进了 RL 微调扩散模型，并在推理时间内以计算量换取样本质量。通过实验，我们表明 RLCM 可以使文本到图像的一致性模型适应难以通过提示表达的目标，例如图像可压缩性，以及源自人类反馈的目标，例如美学质量。与 RL 微调扩散模型相比，RLCM 训练速度明显更快，提高了奖励目标下测量的生成质量，并通过仅需两个推理步骤生成高质量图像来加快推理过程。我们的代码可在 https://rlcm.owenoertell.com 获取]]></description>
      <guid>https://arxiv.org/abs/2404.03673</guid>
      <pubDate>Mon, 08 Apr 2024 01:55:33 GMT</pubDate>
    </item>
    </channel>
</rss>