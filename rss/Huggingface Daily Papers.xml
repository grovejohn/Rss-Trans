<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>拥抱日报</title>
    <link>https://huggingface.co/papers</link>
    <description>Huggingface Daily Papers - 由 RSSHub 用爱制作（https://github.com/DIYgod/RSSHub）</description>
    <lastBuildDate>Mon, 18 Mar 2024 04:14:19 GMT</lastBuildDate>
    <item>
      <title>StreamMultiDiffusion：具有基于区域的语义控制的实时交互生成</title>
      <link>https://arxiv.org/abs/2403.09055</link>
      <description><![CDATA[扩散模型在文本到图像合成方面取得的巨大成功，使其成为下一代图像生成和编辑最终用户应用程序的有希望的候选者。以前的工作重点是通过减少推理时间来提高扩散模型的可用性，或者通过允许新的细粒度控制（例如基于区域的文本提示）来增加用户交互性。然而，我们凭经验发现，整合两个工作分支并非易事，限制了扩散模型的潜力。为了解决这种不兼容性，我们提出了 StreamMultiDiffusion，这是第一个基于区域的实时文本到图像生成框架。通过稳定快速推理技术并将模型重组为新提出的多提示流批处理架构，我们的全景图生成速度比现有解决方案快 10 倍，并且在基于区域的文本到图像合成中的生成速度为 1.57 FPS单个 RTX 2080 Ti GPU。我们的解决方案开辟了一种名为语义调色板的交互式图像生成的新范例，其中从给定的多个手绘区域实时生成高质量图像，并编码规定的语义含义（例如，鹰、女孩）。我们的代码和演示应用程序可从 https://github.com/ironjr/StreamMultiDiffusion 获取。]]></description>
      <guid>https://arxiv.org/abs/2403.09055</guid>
      <pubDate>Fri, 15 Mar 2024 04:13:49 GMT</pubDate>
    </item>
    <item>
      <title>3D-VLA：3D 视觉-语言-动作生成世界模型</title>
      <link>https://arxiv.org/abs/2403.09631</link>
      <description><![CDATA[最近的视觉-语言-动作 (VLA) 模型依赖于 2D 输入，缺乏与更广泛的 3D 物理世界领域的集成。此外，他们通过学习从感知到行动的直接映射来进行行动预测，忽略了世界的巨大动态以及行动与动态之间的关系。相比之下，人类被赋予了世界模型，可以描绘对未来场景的想象力，从而计划相应的行动。为此，我们通过引入一系列新的具体化基础模型来提出 3D-VLA，这些模型通过生成世界模型无缝链接 3D 感知、推理和行动。具体来说，3D-VLA 构建在基于 3D 的大语言模型 (LLM) 之上，并引入了一组交互令牌来与具体环境进行交互。此外，为了将生成能力注入模型中，我们训练了一系列具体扩散模型并将它们对齐到 LLM 中以预测目标图像和点云。为了训练我们的 3D-VLA，我们通过从现有机器人数据集中提取大量 3D 相关信息来构建大规模 3D 体现指令数据集。我们对保留数据集的实验表明，3D-VLA 显着提高了具体环境中的推理、多模态生成和规划能力，展示了其在现实世界应用中的潜力。]]></description>
      <guid>https://arxiv.org/abs/2403.09631</guid>
      <pubDate>Fri, 15 Mar 2024 04:09:06 GMT</pubDate>
    </item>
    <item>
      <title>Quiet-STAR：语言模型可以在说话之前自学思考</title>
      <link>https://arxiv.org/abs/2403.09629</link>
      <description><![CDATA[在写作和说话时，人们有时会停下来思考。尽管以推理为中心的作品通常将推理视为回答问题或完成代理任务的方法，但推理几乎隐含在所有书面文本中。例如，这适用于证明的字里行间未说明的步骤或对话背后的心理理论。在自学推理机（STaR，Zelikman 等人，2022）中，有用的思维是通过从问答中的少数例子中推断基本原理并从那些导致正确答案的例子中学习来学习的。这是一个高度受限的环境——理想情况下，语言模型可以学习推断任意文本中未阐明的基本原理。我们提出了 Quiet-STaR，这是 STaR 的推广，其中 LM 学习为每个标记生成基本原理来解释未来的文本，从而改进他们的预测。我们解决了关键挑战，包括 1) 生成延续的计算成本，2) LM 最初不知道如何生成或使用内部思想，3) 需要预测超出单个下一个标记的需求。为了解决这些问题，我们提出了一种标记并行采样算法，使用可学习的标记来指示思想的开始和结束，以及扩展的教师强制技术。令人鼓舞的是，生成的理由很大程度上有助于对难以预测的标记进行建模，并提高 LM 直接回答难题的能力。特别是，在使用 Quiet-STaR 对互联网文本语料库继续对 LM 进行预训练后，我们发现 GSM8K (5.9%rightarrow10.9%) 和 CommonsenseQA (36.3%rightarrow47.2%) 上有零样本改进，并观察到了困惑改进自然文本中的困难标记。至关重要的是，这些改进不需要对这些任务进行微调。 Quiet-STARaR 标志着 LM 迈出了一步，可以以更通用和可扩展的方式学习推理。]]></description>
      <guid>https://arxiv.org/abs/2403.09629</guid>
      <pubDate>Fri, 15 Mar 2024 03:51:23 GMT</pubDate>
    </item>
    <item>
      <title>GiT：通过通用语言接口迈向通才视觉转换器</title>
      <link>https://arxiv.org/abs/2403.09394</link>
      <description><![CDATA[本文提出了一个简单而有效的框架，称为 GiT，仅使用普通 ViT 即可同时适用于各种视觉任务。受大型语言模型（LLM）中广泛使用的多层 Transformer 架构（例如 GPT）的通用性的推动，我们寻求扩大其范围，以充当强大的视觉基础模型（VFM）。然而，与语言建模不同，视觉任务通常需要特定的模块，例如用于检测的边界框头和用于分割的像素解码器，这极大地阻碍了强大的多层变压器在视觉领域的应用。为了解决这个问题，我们设计了一个通用语言界面，它能够成功地进行自回归解码，从而巧妙地统一各种视觉任务，从图像级理解（例如字幕）、稀疏感知（例如检测）到密集预测（例如，分割）。基于上述设计，整个模型仅由ViT组成，没有任何特定的添加，架构上得到了显着的简化。 GiT 是一个多任务视觉模型，在五个代表性基准上联合训练，无需针对特定任务进行微调。有趣的是，我们的 GiT 在通才表现方面建立了新的基准，并促进跨任务的相互增强，与孤立的训练相比，取得了显着的进步。这反映了法学硕士中观察到的类似影响。 GiT 通过 27 个数据集进一步丰富训练，在各种任务中实现了出色的零样本结果。由于其简单的设计，这种范例有望缩小视觉和语言之间的架构差距。代码和模型将在 https://github.com/Haiyang-W/GiT 上提供。]]></description>
      <guid>https://arxiv.org/abs/2403.09394</guid>
      <pubDate>Fri, 15 Mar 2024 03:45:58 GMT</pubDate>
    </item>
    <item>
      <title>BurstAttention：一种针对极长序列的高效分布式注意力框架</title>
      <link>https://arxiv.org/abs/2403.09347</link>
      <description><![CDATA[有效的注意力模块在基于 Transformer 的大型语言模型 (LLM) 的成功中发挥了至关重要的作用，但这些注意力模块的二次时间和内存复杂性在处理长序列时也构成了挑战。长序列问题的一种潜在解决方案是利用分布式集群跨多个设备（例如 GPU）并行计算注意力模块。然而，采用分布式方法不可避免地会引入额外的内存开销来存储本地注意力结果，并且会产生额外的通信成本以将本地结果聚合到全局结果中。在本文中，我们提出了一种名为“BurstAttention”的分布式注意力框架，以优化全局集群和本地设备级别的内存访问和通信操作。在我们的实验中，我们将 BurstAttention 与其他用于长序列处理的竞争分布式注意力解决方案进行比较。不同长度设置下的实验结果表明，与这些竞争基线相比，BurstAttention 在处理长序列方面具有显着优势，减少了 40% 的通信开销，并在 8 X A100 上训练 32K 序列长度期间实现了 2 倍的加速。]]></description>
      <guid>https://arxiv.org/abs/2403.09347</guid>
      <pubDate>Fri, 15 Mar 2024 03:40:18 GMT</pubDate>
    </item>
    <item>
      <title>Griffon v2：通过高分辨率缩放和视觉语言共同参考推进多模态感知</title>
      <link>https://arxiv.org/abs/2403.09333</link>
      <description><![CDATA[大视觉语言模型已经实现了细粒度的物体感知，但图像分辨率的限制仍然是在复杂和密集的场景中超越特定任务专家的性能的重大障碍。这种限制进一步限制了模型在 GUI 代理、计数等领域实现细致入微的视觉和语言引用的潜力。为了解决这个问题，我们引入了一个统一的高分辨率通才模型 Griffon v2，通过视觉和文本提示实现灵活的对象引用。为了有效地提高图像分辨率，我们设计了一个简单且轻量级的下采样投影仪来克服大型语言模型中的输入标记约束。这种设计本质上保留了完整的上下文和精细的细节，并显着提高了多模态感知能力，特别是对于小物体。在此基础上，我们通过即插即用的视觉分词器进一步为模型配备了视觉语言共同指代功能。它可以与灵活的目标图像、自由格式的文本甚至坐标进行用户友好的交互。实验表明，Griffon v2 可以通过视觉和文本引用来定位任何感兴趣的对象，在 REC、短语基础和 REG 任务上实现最先进的性能，并在对象检测和对象计数方面优于专家模型。数据、代码和模型将在 https://github.com/jefferyZhan/Griffon 发布。]]></description>
      <guid>https://arxiv.org/abs/2403.09333</guid>
      <pubDate>Fri, 15 Mar 2024 03:38:22 GMT</pubDate>
    </item>
    <item>
      <title>Veagle：多模态表示学习的进展</title>
      <link>https://arxiv.org/abs/2403.08773</link>
      <description><![CDATA[最近，人工智能研究人员对语言和视觉如何结合在一起非常感兴趣，从而促进了旨在无缝集成文本和视觉信息的多模态模型的发展。多模态模型是大型语言模型 (LLM) 的扩展，在解决从图像字幕和视觉问答 (VQA) 到视觉基础等各种任务方面表现出了卓越的能力。虽然这些模型展示了显着的进步，但准确解释图像和回答问题仍然存在挑战，这在现实场景中很常见。本文介绍了一种增强现有模型多模态能力的新方法。为了应对当前视觉语言模型 (VLM) 和多模态大语言模型 (MLLM) 中观察到的局限性，我们提出的模型 Veagle 结合了受先前工作的成功和见解启发的独特机制。 Veagle 利用动态机制将编码的视觉信息直接投射到语言模型中。这种动态方法可以更细致地理解视觉环境中存在的复杂细节。为了验证 Veagle 的有效性，我们对基准数据集进行了全面的实验，重点关注视觉问答和图像理解等任务。我们的结果表明，Veagle 的性能提高了 5-6%，明显优于现有模型。结果强调了该模型超越传统基准的多功能性和适用性。]]></description>
      <guid>https://arxiv.org/abs/2403.08773</guid>
      <pubDate>Fri, 15 Mar 2024 03:31:02 GMT</pubDate>
    </item>
    <item>
      <title>Video Mamba Suite：状态空间模型作为视频理解的多功能替代方案</title>
      <link>https://arxiv.org/abs/2403.09626</link>
      <description><![CDATA[理解视频是计算机视觉研究的基本方向之一，人们致力于探索 RNN、3D CNN 和 Transformers 等各种架构。新提出的状态空间模型架构，例如 Mamba，显示出将其在长序列建模中的成功扩展到视频建模的良好特性。为了评估 Mamba 是否可以成为视频理解领域中 Transformers 的可行替代品，在这项工作中，我们进行了一系列全面的研究，探讨 Mamba 在视频建模中可以扮演的不同角色，同时调查 Mamba 可以表现出优势的各种任务。我们将 Mamba 分为四个用于视频建模的角色，派生出由 14 个模型/模块组成的 Video Mamba Suite，并在 12 个视频理解任务上对其进行评估。我们广泛的实验揭示了 Mamba 在纯视频和视频语言任务上的强大潜力，同时显示出有希望的效率与性能权衡。我们希望这项工作能够为未来的视频理解研究提供有价值的数据点和见解。代码是公开的：https://github.com/OpenGVLab/video-mamba-suite。]]></description>
      <guid>https://arxiv.org/abs/2403.09626</guid>
      <pubDate>Fri, 15 Mar 2024 03:26:53 GMT</pubDate>
    </item>
    <item>
      <title>LocalMamba：具有窗口选择性扫描的视觉状态空间模型</title>
      <link>https://arxiv.org/abs/2403.09338</link>
      <description><![CDATA[状态空间模型（尤其是 Mamba）的最新进展在为语言理解等任务建模长序列方面取得了重大进展。然而，它们在视觉任务中的应用并没有明显超越传统卷积神经网络（CNN）和视觉变压器（ViT）的性能。本文认为增强 Vision Mamba (ViM) 的关键在于优化序列建模的扫描方向。传统的 ViM 方法扁平化空间标记，忽略了局部 2D 依赖性的保留，从而拉长了相邻标记之间的距离。我们引入了一种新颖的局部扫描策略，将图像划分为不同的窗口，有效捕获局部依赖性，同时保持全局视角。此外，考虑到不同网络层对扫描模式的不同偏好，我们提出了一种动态方法来独立搜索每一层的最佳扫描选择，从而显着提高性能。跨普通模型和分层模型的广泛实验强调了我们的方法在有效捕获图像表示方面的优越性。例如，在相同 1.5G FLOP 的情况下，我们的模型在 ImageNet 上的性能明显优于 Vim-Ti 3.1%。代码位于：https://github.com/hunto/LocalMamba。]]></description>
      <guid>https://arxiv.org/abs/2403.09338</guid>
      <pubDate>Fri, 15 Mar 2024 03:11:06 GMT</pubDate>
    </item>
    <item>
      <title>使用 WebSight 数据集解锁 Web 屏幕截图到 HTML 代码的转换</title>
      <link>https://arxiv.org/abs/2403.09029</link>
      <description><![CDATA[在 Web 开发中使用视觉语言模型 (VLM) 提出了一种很有前途的策略，可以提高效率并解锁无代码解决方案：通过提供 UI 的屏幕截图或草图，VLM 可以生成代码来重现它，例如在像 HTML 这样的语言。尽管 VLM 在各种任务方面取得了进步，但将屏幕截图转换为相应 HTML 的具体挑战却很少被探讨。我们认为这主要是由于缺乏合适的高质量数据集。这项工作介绍了 WebSight，这是一个由 200 万对 HTML 代码及其相应屏幕截图组成的合成数据集。我们在数据集上微调了基础 VLM，并展示了将网页屏幕截图转换为功能性 HTML 代码的熟练程度。为了加速这一领域的研究，我们开源了 WebSight。]]></description>
      <guid>https://arxiv.org/abs/2403.09029</guid>
      <pubDate>Fri, 15 Mar 2024 02:51:31 GMT</pubDate>
    </item>
    </channel>
</rss>