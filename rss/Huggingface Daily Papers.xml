<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>拥抱日报</title>
    <link>https://huggingface.co/papers</link>
    <description>Huggingface Daily Papers - RSSHub 用爱制作的(https://github.com/DIYgod/RSSHub)</description>
    <lastBuildDate>Wed, 24 Apr 2024 09:13:25 GMT</lastBuildDate>
    <item>
      <title>Transformer 可以表示 $n$-gram 语言模型</title>
      <link>https://arxiv.org/abs/2404.14994</link>
      <description><![CDATA[大量现有工作通过用形式计算模型描述变压器架构的表示能力来分析变压器架构的能力。然而，到目前为止，重点是从语言接受方面分析架构。我们认为，这在语言模型（LM）研究中是一个不合适的问题，语言模型在定义上是字符串上的概率分布。在本文中，我们重点关注 Transformer LM 和 n-gram LM 之间的关系，n-gram LM 是一类简单且与历史相关的语言模型。我们证明，使用硬注意力或稀疏注意力机制的 Transformer LM 可以精确地表示任何 n 元 LM，从而为我们提供了其概率表示能力的具体下限。这为理解 Transformer LM 可用来表示字符串概率分布的机制迈出了第一步。]]></description>
      <guid>https://arxiv.org/abs/2404.14994</guid>
      <pubDate>Wed, 24 Apr 2024 03:50:26 GMT</pubDate>
    </item>
    <item>
      <title>FlashSpeech：高效的零样本语音合成</title>
      <link>https://arxiv.org/abs/2404.14700</link>
      <description><![CDATA[语言模型和扩散模型极大地推进了大规模零样本语音合成的最新进展。然而，这两种方法的生成过程都很慢并且计算量大。使用较低的计算预算实现高效的语音合成以达到与以前的工作相当的质量仍然是一个重大挑战。在本文中，我们提出了 FlashSpeech，这是一种大规模零样本语音合成系统，与之前的工作相比，推理时间缩短了约 5%。 FlashSpeech 建立在潜在一致性模型的基础上，并应用了一种新颖的对抗性一致性训练方法，可以从头开始训练，而不需要预先训练的扩散模型作为教师。此外，新的韵律生成模块增强了韵律的多样性，使语音的节奏听起来更加自然。 FlashSpeech 的生成过程可以通过一两个采样步骤高效地实现，同时保持高音频质量和与音频提示的高相似性，以实现零样本语音生成。我们的实验结果证明了 FlashSpeech 的优越性能。值得注意的是，FlashSpeech 的速度比其他零样本语音合成系统快约 20 倍，同时在语音质量和相似性方面保持可比的性能。此外，FlashSpeech 通过高效执行语音转换、语音编辑和多样化语音采样等任务，展示了其多功能性。音频样本可以在 https://flashspeech.github.io/ 中找到。]]></description>
      <guid>https://arxiv.org/abs/2404.14700</guid>
      <pubDate>Wed, 24 Apr 2024 02:05:01 GMT</pubDate>
    </item>
    <item>
      <title>Pegasus-v1 技术报告</title>
      <link>https://arxiv.org/abs/2404.14687</link>
      <description><![CDATA[本技术报告介绍了 Pegasus-1，一种专门用于通过自然语言理解视频内容和交互的多模态语言模型。 Pegasus-1 旨在解决视频数据带来的独特挑战，例如解释时空信息，以提供各种长度的细致入微的视频内容理解。这份技术报告概述了 Pegasus-1 的架构、训练策略及其在视频对话、零样本视频问答和视频摘要基准测试中的表现。我们还探讨了 Pegasus-1 的定性特征，展示其功能和局限性，以便为读者提供对其当前状态和未来方向的平衡看法。]]></description>
      <guid>https://arxiv.org/abs/2404.14687</guid>
      <pubDate>Wed, 24 Apr 2024 01:59:40 GMT</pubDate>
    </item>
    <item>
      <title>多头专家混合</title>
      <link>https://arxiv.org/abs/2404.15045</link>
      <description><![CDATA[稀疏专家混合 (SMoE) 可以在不显着增加训练和推理成本的情况下扩展模型容量，但存在以下两个问题：(1) 专家激活率低，仅激活一小部分专家进行优化。 (2)缺乏对单个token内多个语义概念的细粒度分析能力。我们提出多头专家混合（MH-MoE），它采用多头机制将每个令牌拆分为多个子令牌。然后，这些子代币被分配给一组不同的专家并行处理，并无缝地重新集成到原始代币形式中。多头机制使模型能够共同关注来自不同专家内的各种表示空间的信息，同时显着增强专家激活，从而加深上下文理解并缓解过度拟合。此外，我们的 MH-MoE 易于实现，并且与其他 SMoE 优化方法解耦，从而可以轻松与其他 SMoE 模型集成以增强性能。跨三个任务的广泛实验结果：以英语为中心的语言建模、多语言语言建模和 Masked 多模态建模任务，证明了 MH-MoE 的有效性。]]></description>
      <guid>https://arxiv.org/abs/2404.15045</guid>
      <pubDate>Wed, 24 Apr 2024 01:56:00 GMT</pubDate>
    </item>
    <item>
      <title>OpenELM：具有开源训练和推理框架的高效语言模型系列</title>
      <link>https://arxiv.org/abs/2404.14619</link>
      <description><![CDATA[大型语言模型的可重复性和透明度对于推进开放研究、确保结果的可信性以及对数据和模型偏差以及潜在风险的调查至关重要。为此，我们发布了 OpenELM，一种最先进的开放语言模型。 OpenELM 使用分层缩放策略来有效地分配变压器模型每一层内的参数，从而提高准确性。例如，在参数预算约为 10 亿个参数的情况下，OpenELM 与 OLMo 相比，精度提高了 2.36%，同时需要的预训练令牌减少了 2 倍。与之前仅提供模型权重和推理代码以及在私有数据集上进行预训练的做法不同，我们的版本包括在公开数据集上训练和评估语言模型的完整框架，包括训练日志、多个检查点和预训练。训练配置。我们还发布了将模型转换为 MLX 库的代码，以便在 Apple 设备上进行推理和微调。这一全面的发布旨在增强和加强开放研究社区的力量，为未来的开放研究努力铺平道路。我们的源代码以及预训练的模型权重和训练配方可在 https://github.com/apple/corenet 上获取。此外，\model 模型可以在 HuggingFace 上找到：https://huggingface.co/apple/OpenELM。]]></description>
      <guid>https://arxiv.org/abs/2404.14619</guid>
      <pubDate>Wed, 24 Apr 2024 01:44:11 GMT</pubDate>
    </item>
    <item>
      <title>调整您的步骤：优化扩散模型中的采样计划</title>
      <link>https://arxiv.org/abs/2404.14507</link>
      <description><![CDATA[扩散模型 (DM) 已成为视觉领域及其他领域最先进的生成建模方法。 DM 的一个关键缺点是采样速度慢，依赖于通过大型神经网络进行的许多顺序函数评估。从 DM 采样可以被视为通过一组离散化的噪声水平（称为采样计划）求解微分方程。虽然过去的工作主要集中在推导有效的求解器，但很少关注寻找最佳采样计划，并且整个文献都依赖于手工设计的启发式方法。在这项工作中，我们首次提出了一种通用且有原则的方法来优化 DM 的采样计划以获得高质量的输出，称为“对齐您的步骤”。我们利用随机微积分的方法，找到针对不同求解器、经过训练的 DM 和数据集的最佳调度。我们使用各种不同的采样器在多个图像、视频以及 2D 玩具数据合成基准上评估我们的新颖方法，并观察到我们的优化计划在几乎所有实验中都优于以前手工制作的计划。我们的方法展示了采样计划优化的未开发潜力，特别是在少步合成机制中。]]></description>
      <guid>https://arxiv.org/abs/2404.14507</guid>
      <pubDate>Wed, 24 Apr 2024 01:40:49 GMT</pubDate>
    </item>
    <item>
      <title>SnapKV：法学硕士在一代之前就知道你在寻找什么</title>
      <link>https://arxiv.org/abs/2404.14469</link>
      <description><![CDATA[大型语言模型 (LLM) 在处理广泛的上下文方面取得了显着的进步，其中键值 (KV) 缓存在提高其性能方面发挥着至关重要的作用。然而，随着输入长度的增加，KV 缓存的增长对内存和时间效率提出了挑战。为了解决这个问题，本文引入了 SnapKV，这是一种创新且无需微调的方法，可以有效地最小化 KV 缓存大小，同时仍然在实际应用程序中提供可比的性能。我们发现模型中的每个注意力头在生成过程中始终关注特定的即时注意力特征。同时，这种稳健的模式可以从位于提示末尾的“观察”窗口获得。利用这一见解，SnapKV 通过为每个注意力头选择聚集的重要 KV 位置来自动压缩 KV 缓存。我们的方法显着减少了处理长输入序列时不断增长的计算开销和内存占用。具体来说，在处理 16K 令牌的输入时，与基线相比，SnapKV 实现了一致的解码速度，生成速度提高了 3.6 倍，内存效率提高了 8.2 倍。同时，它在 16 个长序列数据集上保持了与基线模型相当的性能。此外，SnapKV 可以使用 HuggingFace 实现在单个 A100-80GB GPU 上处理多达 380K 上下文令牌，只需进行微小的更改，在大海捞针测试中仅表现出可以忽略不计的准确性下降。进一步的综合研究表明 SnapKV 具有实际应用的潜力。]]></description>
      <guid>https://arxiv.org/abs/2404.14469</guid>
      <pubDate>Wed, 24 Apr 2024 01:35:40 GMT</pubDate>
    </item>
    </channel>
</rss>