<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>拥抱日报</title>
    <link>https://huggingface.co/papers</link>
    <description>Huggingface Daily Papers - 由 RSSHub 用爱制作（https://github.com/DIYgod/RSSHub）</description>
    <lastBuildDate>Wed, 01 May 2024 04:15:10 GMT</lastBuildDate>
    <item>
      <title>一夜之间将 Llama-3 的上下文扩展十倍</title>
      <link>https://arxiv.org/abs/2404.19553</link>
      <description><![CDATA[我们通过 QLoRA 微调将 Llama-3-8B-Instruct 的上下文长度从 8K 扩展到 80K。整个训练周期非常高效，在一台 8xA800 (80G) GPU 机器上需要 8 个小时。所得到的模型在广泛的评估任务中表现出卓越的性能，例如 NIHS、主题检索和长上下文语言理解；同时，它也很好地保留了短上下文中的原始能力。戏剧性的上下文扩展主要归因于 GPT-4 生成的仅 3.5K 合成训练样本，这表明法学硕士具有扩展其原始上下文长度的固有（但在很大程度上被低估）潜力。事实上，使用更多的计算资源，上下文长度可以远远超出 80K。因此，团队将公开发布全部资源（包括数据、模型、数据生成管道、训练代码），以方便社区未来的研究：https://github.com/FlagOpen/FlagEmbedding。]]></description>
      <guid>https://arxiv.org/abs/2404.19553</guid>
      <pubDate>Wed, 01 May 2024 03:25:29 GMT</pubDate>
    </item>
    <item>
      <title>Octopus v4：语言模型图</title>
      <link>https://arxiv.org/abs/2404.19296</link>
      <description><![CDATA[语言模型在广泛的应用中都很有效，但最复杂的模型通常是专有的。例如，OpenAI 的 GPT-4 和 Anthropic 的各种模型价格昂贵且消耗大量能源。相比之下，开源社区已经产生了具有竞争力的模型，例如 Llama3。此外，针对特定领域的小型语言模型（例如为法律、医疗或金融任务量身定制的模型）的表现优于其专有模型。本文介绍了一种新方法，该方法使用功能标记来集成多个开源模型，每个模型都针对特定任务进行了优化。我们新开发的 Octopus v4 模型利用功能标记将用户查询智能地引导到最合适的垂直模型并重新格式化查询以实现最佳性能。Octopus v4 是 Octopus v1、v2 和 v3 模型的演变，在选择和参数理解以及重新格式化方面表现出色。此外，我们探索了将图用作一种多功能数据结构，通过利用 Octopus 模型和功能标记的功能有效地协调多个开源模型。使用我们的开源 GitHub (https://www.nexa4ai.com/) 尝试 Octopus v4 模型 (https://huggingface.co/NexaAIDev/Octopus-v4)，并向更大的语言模型图致敬。通过激活少于 10B 参数的模型，我们在同级别模型中取得了 74.8 的 SOTA MMLU 分数。]]></description>
      <guid>https://arxiv.org/abs/2404.19296</guid>
      <pubDate>Wed, 01 May 2024 03:20:41 GMT</pubDate>
    </item>
    <item>
      <title>迭代推理偏好优化</title>
      <link>https://arxiv.org/abs/2404.19733</link>
      <description><![CDATA[迭代偏好优化方法最近被证明对于一般指令调整任务表现良好，但通常对推理任务几乎没有改进（Yuan 等人，2024；Chen 等人，2024）。在这项工作中，我们开发了一种迭代方法，通过优化导致正确答案的获胜与失败推理步骤来优化竞争生成的思想链 (CoT) 候选者之间的偏好。我们使用修改后的 DPO 损失（Rafailov 等人，2023）和附加的负对数似然项进行训练，我们发现这至关重要。我们展示了该方案的重复迭代中推理能力的提高。虽然仅依赖训练集中的示例，但我们的方法使 Llama-2-70B-Chat 在 GSM8K 上的准确率从 55.6% 提高到 81.6%（32 个样本中的多数投票为 88.7%），从 12.5% 提高到 20.8 MATH 上的得分从 77.8% 上升到 86.7%，ARC-Challenge 的得分优于其他基于 Llama-2 且不依赖额外来源数据集的模型。]]></description>
      <guid>https://arxiv.org/abs/2404.19733</guid>
      <pubDate>Wed, 01 May 2024 02:15:48 GMT</pubDate>
    </item>
    <item>
      <title>SAGS：结构感知 3D 高斯泼溅</title>
      <link>https://arxiv.org/abs/2404.19149</link>
      <description><![CDATA[随着 NeRF 的出现，3D 高斯分布 (3D-GS) 为实时神经渲染铺平了道路，克服了体积方法的计算负担。继 3D-GS 的开创性工作之后，多种方法尝试实现可压缩和高保真性能替代方案。然而，通过采用与几何无关的优化方案，这些方法忽略了场景固有的 3D 结构，从而限制了表现力和表示质量，导致出现各种浮点和伪影。在这项工作中，我们提出了一种结构感知高斯泼溅方法（SAGS），该方法隐式编码场景的几何形状，这反映了最先进的渲染性能并减少了基准新颖视图合成数据集的存储要求。 SAGS 基于局部全局图形表示，有助于复杂场景的学习并强制执行有意义的点位移，以保留场景的几何形状。此外，我们引入了 SAGS 的轻量级版本，使用简单而有效的中点插值方案，该方案展示了场景的紧凑表示，尺寸减少了 24 倍，而无需依赖任何压缩策略。跨多个基准数据集的大量实验证明了 SAGS 在渲染质量和模型大小方面均优于最先进的 3D-GS 方法。此外，我们证明了我们的结构感知方法可以有效减轻先前方法的浮动伪影和不规则失真，同时获得精确的深度图。项目页面 https://eververas.github.io/SAGS/。]]></description>
      <guid>https://arxiv.org/abs/2404.19149</guid>
      <pubDate>Wed, 01 May 2024 02:12:03 GMT</pubDate>
    </item>
    <item>
      <title>通过多标记预测更好更快的大型语言模型</title>
      <link>https://arxiv.org/abs/2404.19737</link>
      <description><![CDATA[GPT 和 Llama 等大型语言模型是通过下一个令牌预测损失进行训练的。在这项工作中，我们建议训练语言模型来同时预测多个未来标记，从而提高样本效率。更具体地说，在训练语料库中的每个位置，我们要求模型使用在共享模型主干上运行的 n 个独立输出头来预测以下 n 个标记。将多标记预测视为辅助训练任务，我们测量了改进的下游能力，而代码和自然语言模型的训练时间没有开销。该方法对于较大的模型尺寸越来越有用，并且在训练多个时期时保持其吸引力。在编码等生成基准上，收益尤其明显，我们的模型始终比强大的基准高出几个百分点。与同类 next-token 模型相比，我们的 13B 参数模型在 HumanEval 上解决的问题多解决了 12%，在 MBPP 上解决的问题多解决了 17%。小算法任务的实验表明，多token预测有利于归纳头和算法推理能力的发展。另一个好处是，即使批量大小较大，使用 4 令牌预测训练的模型的推理速度也可提高 3 倍。]]></description>
      <guid>https://arxiv.org/abs/2404.19737</guid>
      <pubDate>Wed, 01 May 2024 02:08:27 GMT</pubDate>
    </item>
    <item>
      <title>GS-LRM：3D 高斯泼溅的大型重建模型</title>
      <link>https://arxiv.org/abs/2404.19702</link>
      <description><![CDATA[我们提出了 GS-LRM，这是一种可扩展的大型重建模型，可以在单个 A100 GPU 上在 0.23 秒内从 2-4 个姿势稀疏图像中预测高质量 3D 高斯基元。我们的模型具有非常简单的基于变压器的架构；我们修补输入的姿势图像，通过一系列转换器块传递连接的多视图图像标记，并直接从这些标记中解码最终的每像素高斯参数以进行可微分渲染。与之前只能重建对象的 LRM 相比，GS-LRM 通过预测每像素高斯分布，可以自然地处理规模和复杂性变化较大的场景。我们通过分别在 Objaverse 和 RealEstate10K 上进行训练来证明我们的模型可以处理对象和场景捕获。在这两种情况下，模型的性能都远远优于最先进的基线。我们还演示了我们的模型在下游 3D 生成任务中的应用。我们的项目网页位于：https://sai-bi.github.io/project/gs-lrm/。]]></description>
      <guid>https://arxiv.org/abs/2404.19702</guid>
      <pubDate>Wed, 01 May 2024 01:52:20 GMT</pubDate>
    </item>
    <item>
      <title>DOCCI：连接和对比图像的描述</title>
      <link>https://arxiv.org/abs/2404.19753</link>
      <description><![CDATA[视觉语言数据集对于文本到图像（T2I）和图像到文本（I2T）研究都至关重要。然而，当前的数据集缺乏细粒度的描述，而这些细节允许模型学习更丰富的关联。为了填补这一空白，我们引入了连接和对比图像描述 (DOCCI)，这是一个数据集，其中包含 15k 幅图像的长篇人工注释英文描述，这些图像由一位研究人员拍摄、策划和捐赠，旨在捕捉空间关系等关键挑战、计数、文本渲染、世界知识等等。我们指导人类注释者为每张图像创建全面的描述；这些平均长度为 136 个单词，旨在将每张图像与相关或相似的图像清楚地区分开来。每个描述都具有高度的综合性，并且通常包含多个挑战。通过定量和定性分析，我们证明 DOCCI 可作为图像到文本生成的有效训练资源——在 DOCCI 上微调的 PaLI 5B 模型与 LLaVA-1.5 7B 等高性能大型模型相比，显示出相同或更好的结果并指导BLIP 7B。此外，我们表明 DOCCI 是文本到图像生成的有用测试平台，突出了当前文本到图像模型在捕获长描述和精细细节方面的局限性。]]></description>
      <guid>https://arxiv.org/abs/2404.19753</guid>
      <pubDate>Wed, 01 May 2024 01:46:44 GMT</pubDate>
    </item>
    <item>
      <title>InstantFamily：用于零样本多 ID 图像生成的掩蔽注意力</title>
      <link>https://arxiv.org/abs/2404.19427</link>
      <description><![CDATA[在个性化图像生成领域，创建保留概念的图像的能力显着提高。创建一个将多个概念自然地整合到一个有凝聚力且具有视觉吸引力的构图中的图像确实具有挑战性。本文介绍了“InstantFamily”，一种采用新颖的屏蔽交叉注意机制和多模态嵌入堆栈来实现零样本多 ID 图像生成的方法。我们的方法有效地保留了 ID，因为它利用了与文本条件集成的预先训练的人脸识别模型中的全局和局部特征。此外，我们的屏蔽交叉注意机制可以精确控制生成图像中的多 ID 和合成。我们通过实验展示了 InstantFamily 的有效性，展示了它在生成多 ID 图像方面的优势，同时解决了众所周知的多 ID 生成问题。此外，我们的模型在单 ID 和多 ID 保存方面都实现了最先进的性能。此外，我们的模型表现出显着的可扩展性，比最初训练时保存的 ID 数量更多。]]></description>
      <guid>https://arxiv.org/abs/2404.19427</guid>
      <pubDate>Wed, 01 May 2024 01:33:45 GMT</pubDate>
    </item>
    </channel>
</rss>