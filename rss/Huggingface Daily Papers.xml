<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>拥抱日报</title>
    <link>https://huggingface.co/papers</link>
    <description>Huggingface Daily Papers - 由 RSSHub 用爱制作（https://github.com/DIYgod/RSSHub）</description>
    <lastBuildDate>Thu, 25 Apr 2024 16:37:22 GMT</lastBuildDate>
    <item>
      <title>MotionMaster：用于视频生成的免训练相机运动传输</title>
      <link>https://arxiv.org/abs/2404.15789</link>
      <description><![CDATA[扩散模型的出现极大地推动了图像和视频生成的进步。最近，在可控视频生成方面做出了一些努力，包括文本到视频生成和视频运动控制，其中相机运动控制是一个重要的课题。然而，现有的相机运动控制方法依赖于训练时间相机模块，并且由于视频生成模型中的大量参数而需要大量的计算资源。此外，现有方法在训练期间预先定义相机运动类型，这限制了它们在相机控制方面的灵活性。因此，为了降低训练成本并实现灵活的摄像机控制，我们提出了 COMD，一种新颖的免训练视频运动传输模型，它将源视频中的摄像机运动和物体运动分开，并将提取的摄像机运动传输到新视频。我们首先提出了一种一次性摄像机运动解缠方法，从单个源视频中提取摄像机运动，该方法将运动对象与背景分离，并通过求解泊松方程，根据背景中的运动来估计运动对象区域中的摄像机运动。方程。此外，我们提出了一种少镜头相机运动解缠结方法，从具有相似相机运动的多个视频中提取共同的相机运动，该方法采用基于窗口的聚类技术来提取多个视频的时间注意力图中的共同特征。最后，我们提出了一种运动组合方法，将不同类型的相机运动组合在一起，使我们的模型更加可控和灵活的相机控制。大量的实验表明，我们的免训练方法可以有效地解耦相机-物体运动，并将解耦的相机运动应用于广泛的可控视频生成任务，实现灵活多样的相机运动控制。]]></description>
      <guid>https://arxiv.org/abs/2404.15789</guid>
      <pubDate>Thu, 25 Apr 2024 15:28:40 GMT</pubDate>
    </item>
    <item>
      <title>PuLID：通过对比对齐实现 Pure 和 Lightning ID 定制</title>
      <link>https://arxiv.org/abs/2404.16022</link>
      <description><![CDATA[我们提出了 Pure 和 Lightning ID 定制（PuLID），这是一种用于文本到图像生成的新颖的免调整 ID 定制方法。通过将 Lightning T2I 分支与标准扩散分支相结合，PuLID 引入了对比对准损失和精确 ID 损失，最大限度地减少对原始模型的破坏并确保高 ID 保真度。实验表明，PuLID 在 ID 保真度和可编辑性方面均取得了优异的性能。 PuLID 的另一个吸引人的特性是 ID 插入前后的图像元素（例如背景、灯光、构图和风格）尽可能保持一致。代码和模型将在 https://github.com/ToTheBeginning/PuLID 上提供]]></description>
      <guid>https://arxiv.org/abs/2404.16022</guid>
      <pubDate>Thu, 25 Apr 2024 15:08:29 GMT</pubDate>
    </item>
    <item>
      <title>ID-Aligner：通过奖励反馈学习增强保留身份的文本到图像的生成</title>
      <link>https://arxiv.org/abs/2404.15449</link>
      <description><![CDATA[扩散模型的快速发展引发了多样化的应用。身份保留文本到图像生成（ID-T2I）因其广泛的应用场景（例如人工智能肖像和广告）而受到广泛关注。虽然现有的 ID-T2I 方法已经展示了令人印象深刻的结果，但仍然存在一些关键挑战：（1）很难准确地保持参考肖像的身份特征，（2）生成的图像缺乏审美吸引力，特别是在执行身份保留时，以及（3） ）存在无法同时兼容基于LoRA和基于Adapter的方法的限制。为了解决这些问题，我们提出了 ID-Aligner，这是一个用于增强 ID-T2I 性能的通用反馈学习框架。为了解决身份特征丢失的问题，我们引入了身份一致性奖励微调，以利用人脸检测和识别模型的反馈来改善生成的身份保留。此外，我们提出身份美学奖励微调，利用人类注释的偏好数据的奖励和自动构建的角色结构生成反馈来提供美学调整信号。得益于其通用反馈微调框架，我们的方法可以轻松应用于 LoRA 和 Adapter 模型，从而实现一致的性能增益。 SD1.5 和 SDXL 扩散模型的大量实验验证了我们方法的有效性。项目页面：\url{https://idaligner.github.io/}]]></description>
      <guid>https://arxiv.org/abs/2404.15449</guid>
      <pubDate>Thu, 25 Apr 2024 15:02:19 GMT</pubDate>
    </item>
    </channel>
</rss>