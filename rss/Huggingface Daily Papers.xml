<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>拥抱日报</title>
    <link>https://huggingface.co/papers</link>
    <description>Huggingface Daily Papers - 由 RSSHub 用爱制作（https://github.com/DIYgod/RSSHub）</description>
    <lastBuildDate>Thu, 04 Apr 2024 04:36:49 GMT</lastBuildDate>
    <item>
      <title>ChatGLM-Math：通过自我批评管道提高大型语言模型中的数学问题解决能力</title>
      <link>https://arxiv.org/abs/2404.02893</link>
      <description><![CDATA[大型语言模型（LLM）已经表现出对人类语言的出色掌握，但在需要解决数学问题的现实应用中仍然举步维艰。虽然开发了许多增强法学硕士数学的策略和数据集，但在部署的法学硕士系统中同时维护和提高语言和数学能力仍然是一个挑战。在这项工作中，我们定制了自我批评管道，它解决了法学硕士数学中的挑战。 LLM对齐的反馈学习阶段。我们首先从法学硕士本身训练一个通用的数学批判模型来提供反馈信号。然后，我们依次对法学硕士自己的一代进行拒绝微调和直接偏好优化来收集数据。基于 ChatGLM3-32B，我们对学术数据集和新创建的挑战性数据集 MathUserEval 进行了一系列实验。结果表明，我们的管道显着增强了法学硕士解决数学问题的能力，同时仍然提高了其语言能力，其表现优于可能大两倍的法学硕士。相关技术已部署到在线服务法学硕士ChatGLM\url{https://chatglm.cn}。相关评估数据集和脚本发布于https://github.com/THUDM/ChatGLM-Math。]]></description>
      <guid>https://arxiv.org/abs/2404.02893</guid>
      <pubDate>Thu, 04 Apr 2024 03:16:22 GMT</pubDate>
    </item>
    <item>
      <title>交叉注意力使文本到图像扩散模型的推理变得麻烦</title>
      <link>https://arxiv.org/abs/2404.02747</link>
      <description><![CDATA[本研究探讨了文本条件扩散模型推理过程中交叉注意力的作用。我们发现，经过几个推理步骤后，交叉注意力输出会收敛到一个固定点。因此，收敛的时间点自然地将整个推理过程分为两个阶段：初始语义规划阶段，模型依靠交叉注意力来规划面向文本的视觉语义，以及随后的保真度提高阶段，在此期间，模型尝试根据先前计划的语义生成图像。令人惊讶的是，在保真度提高阶段忽略文本条件不仅降低了计算复杂度，而且还保持了模型性能。这产生了一种称为 TGATE 的简单且无需训练的方法，用于高效生成，一旦交叉注意力输出收敛，该方法就会对其进行缓存，并在剩余的推理步骤中保持固定。我们对 MS-COCO 验证集的实证研究证实了其有效性。 TGATE的源代码可以在https://github.com/HaozheLiu-ST/T-GATE获取。]]></description>
      <guid>https://arxiv.org/abs/2404.02747</guid>
      <pubDate>Thu, 04 Apr 2024 03:12:11 GMT</pubDate>
    </item>
    <item>
      <title>视觉自回归建模：通过下一代预测生成可扩展的图像</title>
      <link>https://arxiv.org/abs/2404.02905</link>
      <description><![CDATA[我们提出了视觉自回归建模（VAR），这是一种新一代范式，它将图像的自回归学习重新定义为从粗到细的“下一个尺度预测”或“下一个分辨率预测”，与标准光栅扫描“下一个分辨率”不同。代币预测”。这种简单、直观的方法使自回归 (AR) 转换器能够快速学习视觉分布并很好地概括：VAR 首次使 AR 模型在图像生成方面超越了扩散转换器。在 ImageNet 256x256 基准上，VAR 通过将 Frechet 起始距离 (FID) 从 18.65 提高到 1.80、起始分数 (IS) 从 80.4 提高到 356.4，显着改善了 AR 基线，推理速度提高了约 20 倍。实证还验证了 VAR 在图像质量、推理速度、数据效率和可扩展性等多个维度上均优于 Diffusion Transformer (DiT)。扩大 VAR 模型表现出与法学硕士中观察到的清晰的幂律缩放定律，线性相关系数接近 -0.998，这是确凿的证据。 VAR 进一步展示了下游任务中的零样本泛化能力，包括图像内画、外画和编辑。这些结果表明 VAR 最初模拟了法学硕士的两个重要属性：缩放定律和零样本任务泛化。我们已经发布了所有模型和代码，以推动AR/VAR模型在视觉生成和统一学习方面的探索。]]></description>
      <guid>https://arxiv.org/abs/2404.02905</guid>
      <pubDate>Thu, 04 Apr 2024 03:07:13 GMT</pubDate>
    </item>
    <item>
      <title>基于扩散的文本到图像生成的可扩展性</title>
      <link>https://arxiv.org/abs/2404.02883</link>
      <description><![CDATA[对于法学硕士的发展来说，扩大模型和数据规模非常成功。然而，基于扩散的文本到图像（T2I）模型的缩放法则尚未得到充分探索。目前还不清楚如何有效地扩展模型以降低成本以获得更好的性能。不同的训练设置和昂贵的训练成本使得公平的模型比较变得极其困难。在这项工作中，我们通过对去噪骨干网和训练集的缩放进行广泛而严格的消融，实证研究基于扩散的 T2I 模型的缩放特性，包括在高达 6 亿图像的数据集上训练从 0.4B 到 4B 参数的缩放 UNet 和 Transformer 变体。对于模型缩放，我们发现交叉注意力的位置和数量区分了现有 UNet 设计的性能。与增加通道数相比，增加变换器块对于改善文本图像对齐来说参数效率更高。然后，我们确定了一种高效的 UNet 变体，它比 SDXL 的 UNet 小 45%，快 28%。在数据扩展方面，我们表明训练集的质量和多样性比简单的数据集大小更重要。增加字幕密度和多样性可以提高文本图像对齐性能和学习效率。最后，我们提供缩放函数来预测文本图像对齐性能，作为模型大小、计算和数据集大小的缩放函数。]]></description>
      <guid>https://arxiv.org/abs/2404.02883</guid>
      <pubDate>Thu, 04 Apr 2024 03:00:07 GMT</pubDate>
    </item>
    <item>
      <title>InstantStyle：文本到图像生成中风格保留的免费午餐</title>
      <link>https://arxiv.org/abs/2404.02733</link>
      <description><![CDATA[基于免调整扩散的模型在图像个性化和定制领域表现出了巨大的潜力。然而，尽管取得了显着的进步，当前的模型在生成风格一致的图像方面仍然面临着一些复杂的挑战。首先，风格的概念本质上是不确定的，它包含色彩、材质、氛围、设计、结构等多种元素。其次，基于反演的方法很容易出现风格退化，通常会导致细粒度细节的丢失。最后，基于适配器的方法经常需要对每个参考图像进行细致的权重调整，以实现风格强度和文本可控性之间的平衡。在本文中，我们首先研究几个令人信服但经常被忽视的观察结果。然后，我们继续介绍 InstantStyle，这是一个旨在通过实施两个关键策略来解决这些问题的框架：1）一种简单的机制，可以将样式和内容与特征空间内的参考图像解耦，基于相同空间内的特征的假设可以相互相加或相减。 2）将参考图像特征专门注入到特定风格的块中，从而防止风格泄漏并避免繁琐的权重调整，这通常是参数较多的设计的特征。我们的工作展示了卓越的视觉风格化结果，在两者之间取得了最佳平衡风格的强度和文本元素的可控性。我们的代码将在 https://github.com/InstantStyle/InstantStyle 上提供。]]></description>
      <guid>https://arxiv.org/abs/2404.02733</guid>
      <pubDate>Thu, 04 Apr 2024 02:51:01 GMT</pubDate>
    </item>
    </channel>
</rss>