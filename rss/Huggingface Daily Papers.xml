<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>拥抱日报</title>
    <link>https://huggingface.co/papers</link>
    <description>Huggingface Daily Papers - 由 RSSHub 用爱制作（https://github.com/DIYgod/RSSHub）</description>
    <lastBuildDate>Mon, 25 Mar 2024 05:13:23 GMT</lastBuildDate>
    <item>
      <title>InternVideo2：扩展视频基础模型以实现多模态视频理解</title>
      <link>https://arxiv.org/abs/2403.15377</link>
      <description><![CDATA[我们推出了 InternVideo2，这是一种新的视频基础模型 (ViFM)，它在动作识别、视频文本任务和以视频为中心的对话方面实现了最先进的性能。我们的方法采用渐进式训练范式，统一了掩码视频令牌重建、跨模式对比学习和下一个令牌预测的不同自监督或弱监督学习框架。不同的训练阶段将指导我们的模型通过不同的借口任务捕获不同级别的结构和语义信息。在数据层面，我们通过对视频进行语义分割并生成视频音频语音字幕来优先考虑时空一致性。这改善了视频和文本之间的对齐。我们缩放了 InternVideo2 的数据和模型大小。通过大量实验，我们验证了我们的设计，并在 60 多个视频和音频任务中展示了最先进的性能。值得注意的是，我们的模型在各种与视频相关的字幕、对话和长视频理解基准方面优于其他模型，突显了其推理和理解长时间上下文的能力。代码和模型可在 https://github.com/OpenGVLab/InternVideo2/ 获取。]]></description>
      <guid>https://arxiv.org/abs/2403.15377</guid>
      <pubDate>Mon, 25 Mar 2024 01:39:49 GMT</pubDate>
    </item>
    <item>
      <title>SiMBA：基于 Mamba 的简化视觉和多元时间序列架构</title>
      <link>https://arxiv.org/abs/2403.15360</link>
      <description><![CDATA[Transformer 广泛采用注意力网络进行序列混合，采用 MLP 进行通道混合，在实现跨领域突破方面发挥着关键作用。然而，最近的文献强调了注意力网络的问题，包括低归纳偏差和有关输入序列长度的二次复杂性。 S4 等状态空间模型 (SSM)（Hippo、Global Convolutions、liquid S4、LRU、Mega 和 Mamba）的出现是为了解决上述问题，以帮助处理更长的序列长度。 Mamba 虽然是最先进的 SSM，但在扩展到计算机视觉数据集的大型网络时存在稳定性问题。我们提出了 SiMBA，这是一种新架构，它引入了爱因斯坦 FFT (EinFFT)，通过特定的特征值计算进行通道建模，并使用 Mamba 模块进行序列建模。跨图像和时间序列基准的广泛性能研究表明，SiMBA 的性能优于现有 SSM，缩小了与最先进 Transformer 的性能差距。值得注意的是，SiMBA 将自己确立为 ImageNet 上最先进的新 SSM 和斯坦福汽车和 Flower 等迁移学习基准以及任务学习基准以及七个时间序列基准数据集。该项目页面可在该网站〜https://github.com/badripatro/Simba 上找到。]]></description>
      <guid>https://arxiv.org/abs/2403.15360</guid>
      <pubDate>Mon, 25 Mar 2024 01:37:18 GMT</pubDate>
    </item>
    <item>
      <title>大型语言模型可以在上下文中进行探索吗？</title>
      <link>https://arxiv.org/abs/2403.15371</link>
      <description><![CDATA[我们研究了当代大型语言模型 (LLM) 能够在多大程度上进行探索，这是强化学习和决策的核心能力。我们专注于现有法学硕士的本地表现，无需培训干预。我们在简单的多臂老虎机环境中将 LLM 部署为代理，完全在上下文中（即在 LLM 提示符内）指定环境描述和交互历史记录。我们使用各种提示设计对 GPT-3.5、GPT-4 和 Llama2 进行实验，发现这些模型在没有大量干预的情况下不能稳健地进行探索：i) 在我们所有的实验中，只有一种配置取得了令人满意的结果探索行为：具有思想链推理和外部总结的交互历史的 GPT-4，以充分的统计数据形式呈现； ii）所有其他配置都不会导致稳健的探索行为，包括那些具有思想链推理但未总结历史的配置。尽管这些发现可以被积极地解释，但它们表明外部总结（在更复杂的环境中可能是不可能的）对于从法学硕士代理人那里获得理想的行为非常重要。我们的结论是，可能需要进行一些重要的算法干预，例如微调或数据集管理，以在复杂的环境中增强基于 LLM 的决策代理的能力。]]></description>
      <guid>https://arxiv.org/abs/2403.15371</guid>
      <pubDate>Mon, 25 Mar 2024 01:33:46 GMT</pubDate>
    </item>
    <item>
      <title>StreamingT2V：从文本生成一致、动态且可扩展的长视频</title>
      <link>https://arxiv.org/abs/2403.14773</link>
      <description><![CDATA[文本到视频的扩散模型可以生成遵循文本指令的高质量视频，从而可以轻松创建多样化和个性化的内容。然而，现有的方法主要关注高质量的短视频生成（通常是 16 或 24 帧），当天真地扩展到长视频合成的情况时，最终会导致硬剪切。为了克服这些限制，我们引入了 StreamingT2V，这是一种自回归方法，用于生成具有平滑过渡的 80、240、600、1200 或更多帧的长视频。关键组件是：（i）称为条件注意模块（CAM）的短期记忆块，它通过注意机制根据从前一个块中提取的特征来调节当前一代，从而导致一致的块转换，（ii）称为外观保留模块的长期记忆块，它从第一个视频块中提取高级场景和对象特征，以防止模型忘记初始场景，以及（iii）一种随机混合方法，可以自回归地应用视频增强器对于无限长的视频，块之间没有不一致。实验表明StreamingT2V产生高运动量。相比之下，当以自回归方式简单应用时，所有竞争的图像到视频方法都容易出现视频停滞。因此，我们通过 StreamingT2V 提出了一种高质量的无缝文本到长视频生成器，其在一致性和运动方面优于竞争对手。我们的代码将在以下位置提供：https://github.com/Picsart-AI-Research/StreamingT2V]]></description>
      <guid>https://arxiv.org/abs/2403.14773</guid>
      <pubDate>Mon, 25 Mar 2024 01:25:12 GMT</pubDate>
    </item>
    </channel>
</rss>