<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>拥抱日报</title>
    <link>https://huggingface.co/papers</link>
    <description>Huggingface Daily Papers - 由 RSSHub 用爱制作（https://github.com/DIYgod/RSSHub）</description>
    <lastBuildDate>Wed, 10 Apr 2024 23:13:59 GMT</lastBuildDate>
    <item>
      <title>修改高斯泼溅中的致密化</title>
      <link>https://arxiv.org/abs/2404.06109</link>
      <description><![CDATA[在本文中，我们解决了 3D 高斯分布 (3DGS) 中自适应密度控制 (ADC) 的局限性，3DGS 是一种场景表示方法，可为新颖的视图合成实现高质量、逼真的结果。 ADC 已被引入用于自动 3D 点基元管理、控制致密化和修剪，但在致密化逻辑方面存在一定的限制。我们的主要贡献是为 3DGS 中的密度控制提供了一种更有原则性的像素误差驱动公式，利用辅助的每像素误差函数作为致密化的标准。我们进一步引入了一种机制来控制每个场景生成的图元总数，并纠正克隆操作期间 ADC 当前不透明度处理策略中的偏差。我们的方法可以在各种基准场景中实现一致的质量改进，而不会牺牲方法的效率。]]></description>
      <guid>https://arxiv.org/abs/2404.06109</guid>
      <pubDate>Wed, 10 Apr 2024 04:04:50 GMT</pubDate>
    </item>
    <item>
      <title>Hash3D：免训练加速 3D 生成</title>
      <link>https://arxiv.org/abs/2404.06091</link>
      <description><![CDATA[2D 扩散模型的采用显着推动了 3D 生成建模的发展。尽管取得了这些进展，但繁琐的优化过程本身却是效率的关键障碍。在本文中，我们介绍了 Hash3D，这是一种无需模型训练的 3D 生成通用加速器。 Hash3D 的核心是认识到特征图冗余在从相机位置和邻近的扩散时间步渲染的图像中普遍存在。通过跨相邻时间步长和摄像机角度有效地散列和重用这些特征图，Hash3D 基本上防止了冗余计算，从而加速了扩散模型在 3D 生成任务中的推理。我们通过基于自适应网格的哈希来实现这一点。令人惊讶的是，这种特征共享机制不仅加快了生成速度，还增强了合成 3D 对象的平滑度和视图一致性。我们的实验涵盖了 5 个文本转 3D 模型和 3 个图像转 3D 模型，展示了 Hash3D 加速优化的多功能性，将效率提高了 1.3 至 4 倍。此外，Hash3D 与 3D 高斯喷射的集成大大加快了 3D 模型的创建速度，将文本到 3D 的处理时间减少到大约 10 分钟，将图像到 3D 的转换时间减少到大约 30 秒。项目页面位于 https://adamdad.github.io/hash3D/。]]></description>
      <guid>https://arxiv.org/abs/2404.06091</guid>
      <pubDate>Wed, 10 Apr 2024 03:57:27 GMT</pubDate>
    </item>
    <item>
      <title>以 3D 方式重建手持物体</title>
      <link>https://arxiv.org/abs/2404.06507</link>
      <description><![CDATA[从野外 RGB 图像或视频中重建用手操纵的物体（即操纵器）尤其具有挑战性。手不仅遮挡了物体的大部分，而且物体通常仅在少量图像像素中可见。同时，在此设置中出现了两个强大的锚点：（1）估计的 3D 手有助于消除对象的位置和比例的歧义，以及（2）相对于所有可能的对象，操纵器集较小。考虑到这些见解，我们提出了一种可扩展的手持式对象重建范例，该范例基于大型语言/视觉模型和 3D 对象数据集的最新突破。我们的模型 MCC-Hand-Object (MCC-HO) 在给定单个 RGB 图像和推断的 3D 手作为输入的情况下联合重建手和物体的几何形状。随后，我们使用 GPT-4(V) 检索与图像中的对象相匹配的 3D 对象模型，并将该模型与网络推断的几何图形严格对齐；我们将这种对齐方式称为检索增强重建（RAR）。实验表明，MCC-HO 在实验室和互联网数据集上实现了最先进的性能，并且我们展示了如何使用 RAR 自动获取手部物体交互的野外图像的 3D 标签。]]></description>
      <guid>https://arxiv.org/abs/2404.06507</guid>
      <pubDate>Wed, 10 Apr 2024 02:32:56 GMT</pubDate>
    </item>
    <item>
      <title>Eagle 和 Finch：具有矩阵值状态和动态递归的 RWKV</title>
      <link>https://arxiv.org/abs/2404.05892</link>
      <description><![CDATA[我们提出了 Eagle (RWKV-5) 和 Finch (RWKV-6)，这是在 RWKV (RWKV-4) 架构上改进的序列模型。我们的架构设计进步包括多头矩阵值状态和动态递归机制，可提高表达能力，同时保持 RNN 的推理效率特征。我们引入了一个包含 1.12 万亿个标记的新多语言语料库和一个基于贪婪匹配的快速标记器，以增强多语言能力。我们训练了四个 Eagle 模型，参数范围为 0.46 到 75 亿个参数，以及两个 Finch 模型，参数范围为 1.6 到 31 亿个参数，发现它们在各种基准测试中都实现了具有竞争力的性能。我们在 Apache 2.0 许可证下在 HuggingFace 上发布了所有模型。模型位于：https://huggingface.co/RWKV 训练代码位于：https://github.com/RWKV/RWKV-LM 推理代码位于：https://github.com/RWKV/ChatRWKV 时间并行训练代码位于：https://github.com/RWKV/RWKV-infctx-trainer]]></description>
      <guid>https://arxiv.org/abs/2404.05892</guid>
      <pubDate>Wed, 10 Apr 2024 02:29:47 GMT</pubDate>
    </item>
    <item>
      <title>MiniCPM：通过可扩展的训练策略揭示小语言模型的潜力</title>
      <link>https://arxiv.org/abs/2404.06395</link>
      <description><![CDATA[人们对开发具有多达万亿个参数的大型语言模型 (LLM) 的兴趣日益浓厚，但也引起了对资源效率和实际费用的担忧，特别是考虑到实验成本巨大。这种情况强调了探索小语言模型 (SLM) 作为资源高效替代方案的潜力的重要性。在这种情况下，我们引入了 MiniCPM，特别是 1.2B 和 2.4B 非嵌入参数变体，它们不仅在各自的类别中表现出色，而且还展示了与 7B-13B LLM 相当的功能。在专注于 SLM 的同时，我们的方法在模型和数据维度上都表现出了可扩展性，适合未来的 LLM 研究。关于模型缩放，我们采用广泛的模型风洞实验来实现稳定和最佳的缩放。对于数据扩展，我们引入了预热-稳定-衰减（WSD）学习率调度器（LRS），有利于持续训练和领域适应。我们对 WSD LRS 中发生的有趣的训练动态进行了深入分析。借助 WSD LRS，我们现在能够有效地研究数据模型缩放定律，而无需在模型和数据两个轴上进行大量的再训练实验，从中我们得出比 Chinchilla Optimal 更高的计算最佳数据模型比率。此外，我们还推出了MiniCPM系列，包括MiniCPM-DPO、MiniCPM-MoE和MiniCPM-128K，其卓越的性能进一步巩固了MiniCPM在各种SLM应用中的基础。 MiniCPM 模型可在 https://github.com/OpenBMB/MiniCPM 上公开获取。]]></description>
      <guid>https://arxiv.org/abs/2404.06395</guid>
      <pubDate>Wed, 10 Apr 2024 02:24:17 GMT</pubDate>
    </item>
    <item>
      <title>OmniFusion 技术报告</title>
      <link>https://arxiv.org/abs/2404.06212</link>
      <description><![CDATA[去年，多模式架构引发了基于人工智能的方法和解决方案的革命，扩展了大型语言模型 (LLM) 的功能。我们提出了一个基于预训练的 LLM 和视觉模态适配器的 OmniFusion 模型。我们评估和比较了几种架构设计原则，以实现更好的文本和视觉数据耦合：MLP 和转换器适配器、各种基于 CLIP ViT 的编码器（SigLIP、InternVIT 等）及其融合方法、图像编码方法（整个图像或图块编码） ）和两个 7B 法学硕士（专有的和开源的 Mistral）。 8 个视觉语言基准测试的实验显示，与类似开源 LLaVA 的解决方案相比，最佳 OmniFusion 设置在不同的 VQA 任务方面得分最高：VizWiz、Pope、MM-Vet、ScienceQA、MMBench、TextVQA、VQAv2、MMMU 。我们还提出了各种情况，OmniFusion 在不同领域提供高度详细的答案：家政、观光、文化、医学、手写和扫描方程识别等。基于 Mistral 的 OmniFusion 模型是一个开源解决方案，具有权重、训练等功能和推理脚本可在 https://github.com/AIRI-Institute/OmniFusion 获取。]]></description>
      <guid>https://arxiv.org/abs/2404.06212</guid>
      <pubDate>Wed, 10 Apr 2024 02:20:20 GMT</pubDate>
    </item>
    <item>
      <title>大象永远不会忘记：大型语言模型中表格数据的记忆和学习</title>
      <link>https://arxiv.org/abs/2404.06209</link>
      <description><![CDATA[虽然许多人已经展示了如何将大型语言模型 (LLM) 应用于各种不同的任务，但数据污染和记忆的关键问题常常被忽视。在这项工作中，我们解决了表格数据的问题。具体来说，我们引入了各种不同的技术来评估语言模型在训练期间是否看到了表格数据集。这项调查表明，法学硕士已经逐字记住了许多流行的表格数据集。然后，我们将训练期间看到的 LLM 在数据集上的小样本学习性能与训练后发布的数据集上的性能进行比较。我们发现法学硕士在训练期间看到的数据集上表现更好，这表明记忆会导致过度拟合。与此同时，法学硕士在新颖的数据集上表现出非凡的性能，并且对数据转换具有惊人的鲁棒性。然后我们调查法学硕士的情境统计学习能力。如果不进行微调，我们会发现它们是有限的。这表明，在新颖数据集上的小样本性能很大程度上归功于法学硕士的世界知识。总的来说，我们的结果强调了测试法学硕士在预训练期间是否看到评估数据集的重要性。我们将我们开发的暴露测试作为 tabmemcheck Python 包提供，网址为 https://github.com/interpretml/LLM-Tabular-Memorization-Checker]]></description>
      <guid>https://arxiv.org/abs/2404.06209</guid>
      <pubDate>Wed, 10 Apr 2024 02:13:53 GMT</pubDate>
    </item>
    <item>
      <title>CodecLM：使语言模型与定制的合成数据保持一致</title>
      <link>https://arxiv.org/abs/2404.05875</link>
      <description><![CDATA[指令调优已成为将大型语言模型 (LLM) 与特定任务指令对齐的关键，从而减少下一个令牌预测目标与用户实际目标之间的差异。为了减少人类收集或注释数据的劳动力和时间成本，研究人员开始探索使用法学硕士来生成指令对齐的合成数据。最近的工作重点是生成多样化的指令并应用 LLM 来增加指令复杂性，通常忽略下游用例。目前尚不清楚如何定制高质量数据，以在不同的目标指令分布和法学硕士中获得更好的指令跟踪能力。为此，我们引入了 CodecLM，这是一个通用框架，用于自适应生成高质量合成数据，用于与不同下游指令分布和 LLM 进行 LLM 对齐。借鉴编码-解码原则，我们使用 LLM 作为编解码器来指导数据生成过程。我们首先将种子指令编码为元数据，元数据是即时生成的简洁关键字，用于捕获目标指令分布，然后解码元数据以创建定制指令。我们还在解码过程中引入了自量规和对比过滤，以定制数据高效的样本。根据基准对四个开放域指令进行的广泛实验验证了 CodecLM 相对于当前最先进技术的有效性。]]></description>
      <guid>https://arxiv.org/abs/2404.05875</guid>
      <pubDate>Wed, 10 Apr 2024 02:08:56 GMT</pubDate>
    </item>
    <item>
      <title>SambaLingo：教授大型语言模型新语言</title>
      <link>https://arxiv.org/abs/2404.05829</link>
      <description><![CDATA[尽管法学硕士广泛存在，但其跨不同语言的能力和可用性仍然存在很大差距。解决这些问题的一种方法是采用现有的预训练法学硕士并继续对其进行新语言的培训。虽然之前的工作已经尝试了语言适应，但尚未涵盖有关最佳实践和方法的许多问题。在本文中，我们对法学硕士对新语言的适应进行了全面的调查。我们的研究涵盖了这个过程中的关键组成部分，包括词汇扩展、直接偏好优化以及低资源语言中人类对齐的数据稀缺问题。我们将这些实验扩展到 9 种语言和 2 个参数尺度（7B 和 70B）。我们将我们的模型与 Llama 2、Aya-101、XGLM、BLOOM 和现有语言专家进行比较，优于所有先前发布的基线。此外，所有评估代码和检查点都是公开的，以方便未来的研究。]]></description>
      <guid>https://arxiv.org/abs/2404.05829</guid>
      <pubDate>Wed, 10 Apr 2024 02:03:46 GMT</pubDate>
    </item>
    <item>
      <title>LLM2Vec：大型语言模型是秘密强大的文本编码器</title>
      <link>https://arxiv.org/abs/2404.05961</link>
      <description><![CDATA[大型纯解码器语言模型 (LLM) 是当今大多数 NLP 任务和基准测试中最先进的模型。然而，社区只是慢慢地将这些模型用于文本嵌入任务，这需要丰富的上下文表示。在这项工作中，我们引入了 LLM2Vec，这是一种简单的无监督方法，可以将任何仅解码器的 LLM 转换为强大的文本编码器。 LLM2Vec 包含三个简单步骤：1）启用双向注意力，2）屏蔽下一个标记预测，3）无监督对比学习。我们通过将 LLM2Vec 应用于从 1.3B 到 7B 参数的 3 个流行的 LLM 来证明 LLM2Vec 的有效性，并评估英语单词和序列级任务的转换模型。我们在字级任务上远远优于仅编码器模型，并在大规模文本嵌入基准（MTEB）上达到了新的无监督最先进性能。此外，当将 LLM2Vec 与监督对比学习相结合时，我们在仅使用公开数据进行训练的模型中在 MTEB 上实现了最先进的性能。我们强有力的实证结果和广泛的分析表明，LLM 可以以参数有效的方式有效地转换为通用文本编码器，而不需要昂贵的适应或合成 GPT-4 生成的数据。]]></description>
      <guid>https://arxiv.org/abs/2404.05961</guid>
      <pubDate>Wed, 10 Apr 2024 01:56:06 GMT</pubDate>
    </item>
    </channel>
</rss>