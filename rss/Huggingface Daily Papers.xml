<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>拥抱日报</title>
    <link>https://huggingface.co/papers</link>
    <description>Huggingface Daily Papers - 由 RSSHub 用爱制作（https://github.com/DIYgod/RSSHub）</description>
    <lastBuildDate>Fri, 19 Apr 2024 23:14:30 GMT</lastBuildDate>
    <item>
      <title>MeshLRM：高质量网格的大型重建模型</title>
      <link>https://arxiv.org/abs/2404.12385</link>
      <description><![CDATA[我们提出了 MeshLRM，这是一种基于 LRM 的新颖方法，可以在不到一秒的时间内仅从四个输入图像重建高质量网格。与之前专注于基于 NeRF 重建的大型重建模型 (LRM) 不同，MeshLRM 在 LRM 框架内结合了可微分网格提取和渲染。这允许通过使用网格渲染微调预先训练的 NeRF LRM 来进行端到端网格重建。此外，我们通过简化以前 LRM 中的几个复杂设计来改进 LRM 架构。 MeshLRM的NeRF初始化是用低分辨率和高分辨率图像顺序训练的；这种新的 LRM 训练策略可以显着加快收敛速度​​，从而以更少的计算量获得更好的质量。我们的方法从稀疏视图输入实现了最先进的网格重建，并且还允许许多下游应用，包括文本到 3D 和单图像到 3D 生成。项目页面：https://sarahweiiii.github.io/meshlrm/]]></description>
      <guid>https://arxiv.org/abs/2404.12385</guid>
      <pubDate>Fri, 19 Apr 2024 04:38:17 GMT</pubDate>
    </item>
    <item>
      <title>EdgeFusion：设备上文本到图像生成</title>
      <link>https://arxiv.org/abs/2404.11925</link>
      <description><![CDATA[文本到图像生成的稳定扩散（SD）的密集计算负担为其实际应用带来了重大障碍。为了应对这一挑战，最近的研究重点是减少采样步骤的方法，例如潜在一致性模型（LCM），以及采用架构优化，包括剪枝和知识蒸馏。与现有方法不同，我们独特地从紧凑型 SD 变体 BK-SDM 开始。我们观察到，使用常用的爬行数据集直接将 LCM 应用于 BK-SDM 会产生不令人满意的结果。它引导我们制定两种策略：(1) 利用领先生成模型中的高质量图像文本对；(2) 设计专为 LCM 量身定制的先进蒸馏流程。通过对量化、分析和设备上部署的深入探索，我们只需两步即可快速生成逼真的文本对齐图像，并且在资源有限的边缘设备上延迟低于一秒。]]></description>
      <guid>https://arxiv.org/abs/2404.11925</guid>
      <pubDate>Fri, 19 Apr 2024 04:30:35 GMT</pubDate>
    </item>
    <item>
      <title>动态版式：让文字栩栩如生</title>
      <link>https://arxiv.org/abs/2404.11614</link>
      <description><![CDATA[文本动画作为一种表达媒介，通过将文字与动作相结合来唤起情感、强调意义并构建引人入胜的叙述，将静态交流转化为动态体验。制作语义感知的动画提出了重大挑战，需要图形设计和动画方面的专业知识。我们提出了一种自动文本动画方案，称为“动态版式”，它结合了两个具有挑战性的任务。它使字母变形以传达语义，并根据用户提示为它们注入充满活力的动作。我们的技术利用矢量图形表示和基于端到端优化的框架。该框架采用神经位移场将字母转换为基本形状并应用每帧运动，鼓励与预期文本概念的一致性。采用形状保存技术和感知损失正则化来保持整个动画过程的易读性和结构完整性。我们展示了我们的方法在各种文本到视频模型中的通用性，并强调了我们的端到端方法相对于基线方法的优越性，基线方法可能包含单独的任务。通过定量和定性评估，我们展示了我们的框架在生成连贯的文本动画方面的有效性，这些动画忠实地解释用户提示，同时保持可读性。我们的代码位于：https://animate-your-word.github.io/demo/。]]></description>
      <guid>https://arxiv.org/abs/2404.11614</guid>
      <pubDate>Fri, 19 Apr 2024 04:25:03 GMT</pubDate>
    </item>
    <item>
      <title>重用你的奖励：零样本跨语言对齐的奖励模型转移</title>
      <link>https://arxiv.org/abs/2404.12318</link>
      <description><![CDATA[基于人工注释的偏好数据调整语言模型 (LM) 是获得实用且高性能的基于 LM 的系统的关键一步。然而，多语言人类偏好数据很难大规模获得，这使得将该框架扩展到多种语言具有挑战性。在这项工作中，我们评估了一种零样本跨语言对齐的简单方法，其中奖励模型根据一种源语言的偏好数据进行训练，然后直接应用于其他目标语言。在摘要和开放式对话生成方面，我们表明该方法在综合评估设置（包括人类评估）下始终是成功的：在高达 70% 以上的评估实例中，人类更喜欢跨语言对齐的模型，而不是未对齐的模型。此外，我们发现不同语言奖励模型有时会比同语言奖励模型产生更好的一致性模型。当没有特定于语言的数据甚至监督微调（另一个对齐的组件）时，我们还确定了最佳实践。]]></description>
      <guid>https://arxiv.org/abs/2404.12318</guid>
      <pubDate>Fri, 19 Apr 2024 02:37:26 GMT</pubDate>
    </item>
    <item>
      <title>MoA：个性化图像生成中主题上下文解开的混合注意力</title>
      <link>https://arxiv.org/abs/2404.11565</link>
      <description><![CDATA[我们引入了一种用于文本到图像扩散模型个性化的新架构，创造了混合注意力（MoA）。受到大型语言模型 (LLM) 中使用的专家混合机制的启发，MoA 将生成工作负载分配在两个注意力路径之间：个性化分支和非个性化先验分支。 MoA 旨在通过将注意力层固定在先前分支中来保留原始模型的先验，同时通过学习将主题嵌入到先前分支生成的布局和上下文中的个性化分支最小化地干预生成过程。一种新颖的路由机制管理跨这些分支的每一层中的像素分布，以优化个性化和通用内容创建的混合。经过训练后，MoA 可以促进创建高质量、个性化的图像，该图像具有多个主题，其构图和交互与原始模型生成的图像一样多样化。至关重要的是，MoA 增强了模型预先存在的功能与新增强的个性化干预之间的区别，从而提供了以前无法实现的更加清晰的主题上下文控制。项目页面：https://snap-research.github.io/mixture-of-attention]]></description>
      <guid>https://arxiv.org/abs/2404.11565</guid>
      <pubDate>Fri, 19 Apr 2024 02:30:36 GMT</pubDate>
    </item>
    <item>
      <title>通过想象力、探索和批评实现法学硕士的自我完善</title>
      <link>https://arxiv.org/abs/2404.12253</link>
      <description><![CDATA[尽管大型语言模型 (LLM) 在各种任务上具有令人印象深刻的能力，但它们仍然难以处理涉及复杂推理和规划的场景。最近的工作提出了先进的提示技术以及使用高质量数据进行微调以增强法学硕士推理能力的必要性。然而，这些方法本质上受到数据可用性和质量的限制。有鉴于此，自我纠正和自我学习成为可行的解决方案，采用的策略允许法学硕士改进他们的成果并从自我评估的奖励中学习。然而，法学硕士在自我完善其反应方面的有效性，特别是在复杂的推理和规划任务中，仍然值得怀疑。在本文中，我们引入了用于LLM自我改进的AlphaLLM，它将蒙特卡罗树搜索（MCTS）与LLM集成，建立自我改进循环，从而在无需额外注释的情况下增强LLM的能力。 AlphaLLM 从 AlphaGo 的成功中汲取灵感，解决了将 MCTS 与 LLM 相结合以实现自我提升的独特挑战，包括数据稀缺、语言任务的巨大搜索空间以及语言任务中反馈的主观性。 AlphaLLM 由即时合成组件、专为语言任务量身定制的高效 MCTS 方法以及用于精确反馈的三个批评模型组成。我们在数学推理任务中的实验结果表明，AlphaLLM 在无需额外注释的情况下显着提高了法学硕士的性能，显示了法学硕士自我提升的潜力。]]></description>
      <guid>https://arxiv.org/abs/2404.12253</guid>
      <pubDate>Fri, 19 Apr 2024 01:59:44 GMT</pubDate>
    </item>
    <item>
      <title>推出 MLCommons 的 AI 安全基准 v0.5</title>
      <link>https://arxiv.org/abs/2404.12241</link>
      <description><![CDATA[本文介绍了由 MLCommons AI 安全工作组创建的 AI 安全基准 v0.5。人工智能安全基准旨在评估使用聊天调整语言模型的人工智能系统的安全风险。我们引入了一种原则性方法来指定和构建基准，对于 v0.5，该基准仅涵盖单个用例（成人用英语与通用助理聊天）和一组有限的角色（即典型用户、恶意用户）用户和易受攻击的用户）。我们创建了 13 个危险类别的新分类法，其中 7 个在 v0.5 基准测试中进行了测试。我们计划在 2024 年底之前发布 AI 安全基准 1.0 版本。v1.0 基准将为 AI 系统的安全性提供有意义的见解。然而，v0.5基准不应该用来评估人工智能系统的安全性。我们力求完整记录 v0.5 的限制、缺陷和挑战。此版本的 AI 安全基准 v0.5 包括 (1) 指定和构建基准的原则性方法，其中包括用例、被测系统 (SUT) 类型、语言和上下文、角色、测试和测试项目; (2) 13 个危险类别的分类法及其定义和子类别； (3) 对七个危险类别进行测试，每个危险类别包含一组独特的测试项目，即提示。总共有43,090个测试项，是我们用模板创建的； （4）针对基准的人工智能系统评分系统； (5) 一个名为 ModelBench 的开放平台和可下载工具，可用于在基准上评估人工智能系统的安全性； (6) 评估报告示例，对十多个公开可用的聊天调整语言模型的性能进行基准测试； (7) 基准测试规范。]]></description>
      <guid>https://arxiv.org/abs/2404.12241</guid>
      <pubDate>Fri, 19 Apr 2024 01:54:29 GMT</pubDate>
    </item>
    <item>
      <title>TriForce：使用分层推测解码对长序列生成进行无损加速</title>
      <link>https://arxiv.org/abs/2404.11912</link>
      <description><![CDATA[近年来，随着大型语言模型（LLM）在长内容生成中的广泛部署，对高效长序列推理支持的需求日益增长。然而，存储键值（KV）缓存以避免重新计算，其大小随着序列长度线性增长，已成为关键瓶颈。由于LLM的自回归性质，将为每个生成的令牌加载整个KV缓存，导致计算核心利用率低和延迟高。虽然已经提出了各种 KV 缓存压缩方法来缓解这个问题，但它们的生成质量会下降。我们引入了 TriForce，这是一种分层推测解码系统，可扩展至长序列生成。这种方法通过检索利用原始模型权重和动态稀疏 KV 缓存作为草稿模型，充当层次结构中的中间层，并由较小的模型进一步推测以减少其草稿延迟。 TriForce 不仅为 Llama2-7B-128K 带来令人印象深刻的加速，在 A100 GPU 上实现高达 2.31 倍，而且还展示了处理更长上下文的可扩展性。对于两个 RTX 4090 GPU 上的卸载设置，TriForce 实现了 0.108s/tokenx2014，仅比 A100 上的自回归基线慢一半，在我们优化的卸载系统上达到了 7.78 倍。此外，TriForce 在单个 RTX 4090 GPU 上的性能是 DeepSpeed-Zero-Inference 的 4.86 倍。 TriForce 的坚固性以其在各种温度下始终如一的出色性能而著称。该代码可在 https://github.com/Infini-AI-Lab/TriForce 获取。]]></description>
      <guid>https://arxiv.org/abs/2404.11912</guid>
      <pubDate>Fri, 19 Apr 2024 01:37:40 GMT</pubDate>
    </item>
    <item>
      <title>OpenBezoar：小型、经济高效且开放的模型，基于混合指令数据进行训练</title>
      <link>https://arxiv.org/abs/2404.12195</link>
      <description><![CDATA[针对不同下游任务对预训练的法学硕士进行指令微调已取得了显着的成功，并引起了学者和从业者的兴趣。为了确保这种经过微调的法学硕士符合人类的偏好，诸如 RLHF 和 DPO 等技术应运而生。与此同时，人们对模型的较小参数数量越来越感兴趣。在这项工作中，我们使用 OpenLLaMA 3Bv2 作为基本模型，描述了用于微调 OpenBezoar 系列模型的方法。在本秘籍中：我们首先使用开放且商业上非限制性的 Falcon-40B 模型指令微调变体在基于 LaMini-LM、WizardLM/Evol-Instruct（带有 databricks）的三种方案下生成合成指令微调数据-dolly-15k 作为种子数据集）和 Orca（以 Flan Collection 作为种子数据集），然后使用 GPT-4 作为人类代理来过滤这些代。然后，我们对每个方案依次执行经济有效的基于 QLoRA 的监督微调。在使用 DPO 损失获得最终检查点之前，使用 HH-RLHF 数据集的子集进一步微调生成的检查点，以最小化分布偏移。使用 LM Eval Harness 任务/指标以及使用 Claude 2.1 的“LLM-as-a-judge”框架在 MT-Bench 上完成评估，结果发现最终检查点“OpenBezoar-HH-RLHF-DPO” ”，在 3B 参数尺度上表现出了优于许多模型的性能，甚至超越了 Huggingface Open LLM 排行榜的某一类别中的顶级模型。我们发布了“OpenBezoar-SFT”、“OpenBezoar-HH-RLHF-SFT”、“OpenBezoar-HH-RLHF-DPO”检查点，以及我们在 HuggingFace 上生成的数据集：https://huggingface.co/collections/Sur​​geGlobal/open- bezoar-6620a24923e12127e9e2b9cc 和我们的代码库 https://bitbucket.org/paladinanalytics/workspace/projects/OP。]]></description>
      <guid>https://arxiv.org/abs/2404.12195</guid>
      <pubDate>Fri, 19 Apr 2024 01:22:28 GMT</pubDate>
    </item>
    <item>
      <title>BLINK：多模态大型语言模型可以看到但无法感知</title>
      <link>https://arxiv.org/abs/2404.12390</link>
      <description><![CDATA[我们推出了 Blink，这是多模式语言模型 (LLM) 的新基准，专注于其他评估中未发现的核心视觉感知能力。大多数 Blink 任务都可以由人类“眨眼间”解决（例如，相对深度估计、视觉对应、取证检测和多视图推理）。然而，我们发现这些需要感知的任务给当前的多模式法学硕士带来了重大挑战，因为它们抵制通过自然语言进行调解。 Blink 将 14 个经典计算机视觉任务重新格式化为 3,807 个多项选择题，并配有单个或多个图像和视觉提示。虽然人类的平均准确率达到 95.70%，但 Blink 对于现有的多模态 LLM 来说却具有惊人的挑战性：即使是表现最好的 GPT-4V 和 Gemini 也能达到 51.26% 和 45.72% 的准确率，仅比随机猜测高 13.17% 和 7.63%，这表明在最近的多模式法学硕士中，这种感知能力尚未“出现”。我们的分析还强调，专业的 CV 模型可以更好地解决这些问题，并提出未来改进的潜在途径。我们相信 Blink 将激励社区帮助多模式法学硕士赶上人类水平的视觉感知。]]></description>
      <guid>https://arxiv.org/abs/2404.12390</guid>
      <pubDate>Fri, 19 Apr 2024 01:11:42 GMT</pubDate>
    </item>
    </channel>
</rss>