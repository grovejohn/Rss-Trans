<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>拥抱日报</title>
    <link>https://huggingface.co/papers</link>
    <description>Huggingface Daily Papers - 由 RSSHub 用爱制作（https://github.com/DIYgod/RSSHub）</description>
    <lastBuildDate>Thu, 11 Apr 2024 05:34:59 GMT</lastBuildDate>
    <item>
      <title>DreamScene360：使用全景高斯泼溅生成无约束文本到 3D 场景</title>
      <link>https://arxiv.org/abs/2404.06903</link>
      <description><![CDATA[对虚拟现实应用程序日益增长的需求凸显了制作沉浸式 3D 资产的重要性。我们提出了一个文本到 3D 360^{circ} 场景生成管道，可以在几分钟内为野外环境创建全面的 360^{circ} 场景。我们的方法利用 2D 扩散模型的生成能力并迅速自我完善，以创建高质量且全局一致的全景图像。该图像充当初步的“平面”(2D) 场景表示。随后，它被提升为 3D 高斯，采用喷射技术来实现实时探索。为了产生一致的 3D 几何形状，我们的管道通过将 2D 单目深度对齐到全局优化的点云来构造空间相干结构。该点云用作 3D 高斯质心的初始状态。为了解决单视图输入中固有的隐形问题，我们对合成和输入相机视图施加语义和几何约束作为正则化。这些指导高斯的优化，帮助重建未见过的区域。总之，我们的方法在 360^{circ} 视角内提供了全局一致的 3D 场景，与现有技术相比，提供了增强的沉浸式体验。项目网站：http://dreamscene360.github.io/]]></description>
      <guid>https://arxiv.org/abs/2404.06903</guid>
      <pubDate>Thu, 11 Apr 2024 01:55:25 GMT</pubDate>
    </item>
    <item>
      <title>BRAVE：拓宽视觉语言模型的视觉编码</title>
      <link>https://arxiv.org/abs/2404.07204</link>
      <description><![CDATA[视觉语言模型（VLM）通常由视觉编码器组成，例如CLIP，以及解释编码特征以解决下游任务的语言模型（LM）。尽管取得了显着的进步，但由于视觉编码器的能力有限，VLM 仍然存在一些缺点，例如对某些图像特征的“失明”、幻视等。为了解决这些问题，我们研究拓宽 VLM 的视觉编码能力。我们首先对几种具有不同归纳偏差的视觉编码器进行全面基准测试，以解决 VLM 任务。我们观察到，没有一种编码配置能够在不同的任务中始终如一地实现最佳性能，并且具有不同偏差的编码器可以表现出惊人的相似。受此启发，我们引入了一种名为 BRAVE 的方法，该方法将多个冻结编码器的特征合并为更通用的表示形式，可以直接作为冻结 LM 的输入。 BRAVE 在各种字幕和 VQA 基准上实现了最先进的性能，并显着减少了上述 VLM 问题，同时比现有方法需要更少数量的可训练参数，并具有更压缩的表示。我们的结果凸显了整合不同视觉偏差的潜力，可以对 VLM 进行更广泛和更情境化的视觉理解。]]></description>
      <guid>https://arxiv.org/abs/2404.07204</guid>
      <pubDate>Thu, 11 Apr 2024 01:45:54 GMT</pubDate>
    </item>
    <item>
      <title>使 LLaMA 解码器适应 Vision Transformer</title>
      <link>https://arxiv.org/abs/2404.06773</link>
      <description><![CDATA[这项工作检验了纯解码器 Transformer 是否可以适应计算机视觉领域，例如 LLaMA 最初是为大型语言模型 (LLM) 设计的。我们首先“LLaMAfy”一个标准的ViT，逐步与LLaMA的架构保持一致，发现直接对自注意力应用随意掩模会带来注意力崩溃问题，导致网络训练失败。我们建议使用后序列类标记技术将类标记重新定位在图像标记后面，以克服这一挑战，使因果自注意力能够有效地捕获整个图像的信息。此外，我们开发了一种软掩模策略，在训练开始时逐渐向自注意力引入随意掩模，以促进优化行为。这种定制模型被称为图像 LLaMA (iLLaMA)，类似于架构中的 LLaMA，并支持直接监督学习。它的因果自注意力提高了计算效率，并通过提升注意力图等级来学习复杂的表示。 iLLaMA 的性能可与仅编码器的同类产品相媲美，仅用 570 万个参数即可实现 75.1% 的 ImageNet top-1 准确率。将模型扩展至约 310M 并在 ImageNet-21K 上进行预训练，将准确率进一步提高至 86.0%。大量实验证明了 iLLaMA 的可靠特性：校准、形状纹理偏差、量化兼容性、ADE20K 分割和 CIFAR 迁移学习。我们希望我们的研究能够为法学硕士浪潮中的视觉模型设计带来新的视角。此处提供了预先训练的模型和代码。]]></description>
      <guid>https://arxiv.org/abs/2404.06773</guid>
      <pubDate>Thu, 11 Apr 2024 01:40:41 GMT</pubDate>
    </item>
    <item>
      <title>RULER：您的长上下文语言模型的真实上下文大小是多少？</title>
      <link>https://arxiv.org/abs/2404.06654</link>
      <description><![CDATA[大海捞针（NIAH）测试检查从长干扰文本（“大海捞针”）中检索一条信息（“针”）的能力，已被广泛用于评估长上下文语言模型（LM）。然而，这种简单的基于检索的测试仅表明长上下文理解的表面形式。为了对长上下文 LM 提供更全面的评估，我们创建了一个新的综合基准标尺，该标尺具有灵活的配置，可定制序列长度和任务复杂性。 RULER 对普通 NIAH 测试进行了扩展，涵盖了不同类型和数量的针的变化。此外，RULER 引入了新的任务类别多跳跟踪和聚合来测试从上下文搜索之外的行为。我们在 RULER 中评估了 10 个长上下文 LM，其中包含 13 个代表性任务。尽管在普通 NIAH 测试中实现了近乎完美的准确性，但随着上下文长度的增加，所有模型都表现出大幅性能下降。虽然这些模型都声称上下文大小为 32K 令牌或更大，但只有四种模型（GPT-4、Command-R、Yi-34B 和 Mixtral）可以在 32K 长度下保持令人满意的性能。我们对支持 200K 上下文长度的 Yi-34B 的分析表明，随着输入长度和任务复杂性的增加，还有很大的改进空间。我们开源 RULER 来促进长上下文 LM 的综合评估。]]></description>
      <guid>https://arxiv.org/abs/2404.06654</guid>
      <pubDate>Thu, 11 Apr 2024 01:36:07 GMT</pubDate>
    </item>
    <item>
      <title>RealmDreamer：具有修复和深度扩散的文本驱动 3D 场景生成</title>
      <link>https://arxiv.org/abs/2404.07199</link>
      <description><![CDATA[我们介绍 RealmDreamer，这是一种根据文本描述生成通用前向 3D 场景的技术。我们的技术优化了 3D 高斯泼溅表示以匹配复杂的文本提示。我们通过利用最先进的文本到图像生成器来初始化这些图，将它们的样本提升为 3D，并计算遮挡体积。然后，我们使用图像条件扩散模型将这种跨多个视图的表示优化为 3D 修复任务。为了学习正确的几何结构，我们通过对修复模型中的样本进行调节来合并深度扩散模型，从而提供丰富的几何结构。最后，我们使用图像生成器中的锐化样本对模型进行微调。值得注意的是，我们的技术不需要视频或多视图数据，并且可以合成由多个对象组成的各种不同风格的高质量 3D 场景。其通用性还允许从单个图像进行 3D 合成。]]></description>
      <guid>https://arxiv.org/abs/2404.07199</guid>
      <pubDate>Thu, 11 Apr 2024 01:33:32 GMT</pubDate>
    </item>
    <item>
      <title>不留下任何上下文：具有无限注意力的高效无限上下文转换器</title>
      <link>https://arxiv.org/abs/2404.07143</link>
      <description><![CDATA[这项工作引入了一种有效的方法，可以将基于 Transformer 的大型语言模型 (LLM) 扩展到具有有限内存和计算的无限长输入。我们提出的方法的一个关键组成部分是一种称为“无限注意力”的新注意力技术。 Infini-attention 将压缩记忆融入到普通的注意力机制中，并在单个 Transformer 块中构建了屏蔽局部注意力和长期线性注意力机制。我们通过 1B 和 8B LLM 证明了我们的方法在长上下文语言建模基准、1M 序列长度密钥上下文块检索和 500K 长度书籍摘要任务上的有效性。我们的方法引入了最小的有界内存参数，并支持 LLM 的快速流式推理。]]></description>
      <guid>https://arxiv.org/abs/2404.07143</guid>
      <pubDate>Thu, 11 Apr 2024 01:30:45 GMT</pubDate>
    </item>
    <item>
      <title>城市建筑师：具有布局优先权的可操纵 3D 城市场景生成</title>
      <link>https://arxiv.org/abs/2404.06780</link>
      <description><![CDATA[文本到 3D 生成通过大规模文本到图像扩散模型取得了显着的成功。然而，还没有将这种方法扩展到城市规模的范式。城市场景具有元素众多、排列关系错综复杂、规模庞大的特点，这给模糊文本描述的可解释性和有效模型优化带来了巨大的障碍。在这项工作中，我们通过将组合 3D 布局表示引入文本到 3D 范式来克服限制，作为额外的先验。它由一组具有简单几何结构和明确排列关系的语义基元组成，补充了文本描述并实现了可操纵的生成。在此基础上，我们提出了两项​​修改——（1）我们引入布局引导的变分分数蒸馏来解决模型优化的不足。它利用 3D 布局的几何和语义约束来调节分数蒸馏采样过程。 (2)为了处理城市场景的无界性质，我们用可扩展哈希网格结构来表示3D场景，逐步适应城市场景规模的不断增长。大量实验证实了我们的框架首次将文本到 3D 生成扩展到覆盖超过 1000m 驾驶距离的大规模城市场景的能力。我们还展示了各种场景编辑演示，展示了可引导城市场景生成的强大功能。网站：https://urbanarchitect.github.io。]]></description>
      <guid>https://arxiv.org/abs/2404.06780</guid>
      <pubDate>Thu, 11 Apr 2024 01:27:17 GMT</pubDate>
    </item>
    </channel>
</rss>