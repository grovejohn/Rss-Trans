<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>拥抱日报</title>
    <link>https://huggingface.co/papers</link>
    <description>Huggingface Daily Papers - 由 RSSHub 用爱制作（https://github.com/DIYgod/RSSHub）</description>
    <lastBuildDate>Tue, 30 Apr 2024 18:37:47 GMT</lastBuildDate>
    <item>
      <title>Stylus：扩散模型的自动适配器选择</title>
      <link>https://arxiv.org/abs/2404.18928</link>
      <description><![CDATA[除了使用更多数据或参数扩展基础模型之外，微调适配器还提供了一种以较低成本生成高保真度自定义图像的替代方法。因此，适配器已被开源社区广泛采用，积累了超过 100K 个适配器的数据库 - 其中大多数是高度定制的，描述不足。本文探讨了将提示与一组相关适配器匹配的问题，该问题建立在最近的工作之上，这些工作强调了组合适配器的性能提升。我们介绍了 Stylus，它可以根据提示的关键字有效地选择和自动组合特定于任务的适配器。Stylus 概述了一种三阶段方法，首先总结具有改进的描述和嵌入的适配器，检索相关适配器，然后通过检查它们与提示的匹配程度，根据提示的关键字进一步组装适配器。为了评估 Stylus，我们开发了 StylusDocs，这是一个精选数据集，其中包含 75K 个具有预先计算的适配器嵌入的适配器。在我们对流行的稳定扩散检查点的评估中，Stylus 实现了更高的 CLIP-FID 帕累托效率，并且以人类和多模态模型作为评估者，其受欢迎程度是基础模型的两倍。有关更多信息，请参阅 stylus-diffusion.github.io。]]></description>
      <guid>https://arxiv.org/abs/2404.18928</guid>
      <pubDate>Tue, 30 Apr 2024 05:12:11 GMT</pubDate>
    </item>
    <item>
      <title>DressCode：根据文本指导自动缝纫和生成服装</title>
      <link>https://arxiv.org/abs/2401.16465</link>
      <description><![CDATA[服装在人类外观中的重要作用凸显了服装数字化对于数字人类创造的重要性。 3D 内容创作的最新进展对于数字人类创作至关重要。尽管如此，通过文本指导生成服装仍处于萌芽阶段。我们引入了一个文本驱动的 3D 服装生成框架 DressCode，其旨在使新手设计民主化，并在时装设计、虚拟试穿和数字人类创作方面提供巨大潜力。对于我们的框架，我们首先引入 SewingGPT，这是一种基于 GPT 的架构，它将交叉注意力与文本条件嵌入相结合，以生成带有文本指导的缝纫图案。我们还为高质量、基于图块的 PBR 纹理生成定制了预训练的稳定扩散。通过利用大型语言模型，我们的框架通过自然语言交互生成 CG 友好的服装。我们的方法还有助于图案完成和纹理编辑，通过用户友好的交互简化设计人员的流程。通过全面评估以及与其他最先进方法的比较，我们的方法展示了最佳的质量并与输入提示保持一致。用户研究进一步验证了我们的高质量渲染结果，突出了其在生产环境中的实用性和潜力。]]></description>
      <guid>https://arxiv.org/abs/2401.16465</guid>
      <pubDate>Tue, 30 Apr 2024 04:08:23 GMT</pubDate>
    </item>
    <item>
      <title>袋鼠：通过双提前退出进行无损自推测解码</title>
      <link>https://arxiv.org/abs/2404.18911</link>
      <description><![CDATA[推测解码已证明其在加速大型语言模型的推理同时保持一致的采样分布方面的有效性。然而，训练单独的草稿模型以获得令人满意的令牌接受率的传统方法可能成本高昂。从早期退出中汲取灵感，我们提出了一种新颖的自推测解码框架 Kangaroo，它使用固定的浅层子网络作为自拟模型，其余层作为更大的目标模型。我们在子网络之上训练一个轻量级且高效的适配器模块，以弥合子网络和完整模型表示能力之间的差距。值得注意的是，与大模型相比，自起草模型的推理延迟可能不再可以忽略不计，因此需要采取策略来提高令牌接受率，同时最大限度地减少小模型的起草步骤。为了应对这一挑战，我们引入了一种额外的提前退出机制来生成草稿代币。具体来说，一旦当前令牌的置信水平低于某个阈值，我们就会在起草阶段停止小模型的后续预测。 Spec-Bench 上的大量实验证明了 Kangaroo 的有效性。在单序列验证下，Kangaroo 在 Spec-Bench 上实现了高达 1.68 倍的加速，优于 Medusa-1，附加参数减少了 88.7%（67M 与 591M 相比）。 Kangaroo 的代码可在 https://github.com/Equationliu/Kangaroo 获取。]]></description>
      <guid>https://arxiv.org/abs/2404.18911</guid>
      <pubDate>Tue, 30 Apr 2024 03:58:51 GMT</pubDate>
    </item>
    <item>
      <title>BlenderAlchemy：使用视觉语言模型编辑 3D 图形</title>
      <link>https://arxiv.org/abs/2404.17672</link>
      <description><![CDATA[图形设计对于各种应用都很重要，包括电影制作和游戏设计。为了创建高质量的场景，设计人员通常需要在 Blender 等软件上花费数小时，他们可能需要交错和重复操作，例如连接材质节点数百次。此外，稍微不同的设计目标可能需要完全不同的序列，这使得自动化变得困难。在本文中，我们提出了一个系统，该系统利用视觉语言模型（VLM）（例如 GPT-4V）来智能搜索设计操作空间，以获得可以满足用户意图的答案。具体来说，我们设计了一个基于视觉的编辑生成器和状态评估器，它们一起工作以找到实现目标的正确操作顺序。受视觉想象力在人类设计过程中的作用的启发，我们用图像生成模型中的“想象”参考图像来补充 VLM 的视觉推理能力，为抽象语言描述提供视觉基础。在本文中，我们提供的经验证据表明我们的系统可以为诸如从文本和/或参考图像编辑程序材料以及调整复杂场景中的产品渲染的照明配置等任务生成简单但繁琐的 Blender 编辑序列。]]></description>
      <guid>https://arxiv.org/abs/2404.17672</guid>
      <pubDate>Tue, 30 Apr 2024 03:45:00 GMT</pubDate>
    </item>
    <item>
      <title>用陪审团取代法官：用不同模型小组评估法学硕士一代</title>
      <link>https://arxiv.org/abs/2404.18796</link>
      <description><![CDATA[随着大型语言模型 (LLM) 变得更加先进，它们已经超出了我们准确评估其质量的能力。不仅找到数据来充分探测特定模型属性很困难，而且单独评估模型自由形式生成的正确性也是一个挑战。为了解决这个问题，许多评估现在依靠法学硕士本身作为评委来对其他法学硕士的输出质量进行评分。评估最常使用单个大型模型，例如 GPT4。虽然这种方法越来越受欢迎，但成本高昂，并且已被证明会引入模型内偏差，并且在这项工作中，我们发现非常大的模型通常是不必要的。我们建议使用法学硕士评估小组（PoLL）来评估模型。在三个不同的法官设置和跨越六个不同数据集的情况下，我们发现使用由大量较小模型组成的 PoLL 优于单个大型法官，并且由于其由不相交的模型系列组成而表现出较少的模型内偏差，并且在价格便宜七倍多。]]></description>
      <guid>https://arxiv.org/abs/2404.18796</guid>
      <pubDate>Tue, 30 Apr 2024 03:04:02 GMT</pubDate>
    </item>
    <item>
      <title>Ag2Manip：通过与代理无关的视觉和动作表示来学习新颖的操作技能</title>
      <link>https://arxiv.org/abs/2404.17521</link>
      <description><![CDATA[能够学习新颖的操作任务的自主机器人系统有望将行业从制造业转变为服务自动化。然而，现代方法（例如 VIP 和 R3M）仍然面临重大障碍，特别是机器人实施例之间的域差距以及特定动作空间内成功任务执行的稀疏性，导致任务表示不一致和模糊。我们引入了 Ag2Manip（与代理无关的操纵表示），该框架旨在通过两项关键创新来克服这些挑战：一种源自人类操纵视频的新颖的与代理无关的视觉表示，模糊了实施例的具体细节以增强普遍性；与代理无关的动作表示将机器人的运动学抽象为通用代理代理，强调末端执行器和物体之间的关键交互。 Ag2Manip 对 FrankaKitchen、ManiSkill 和 PartManip 等模拟基准的实证验证显示，性能提高了 325%，无需特定领域的演示即可实现。消融研究强调了视觉和动作表现对这一成功的重要贡献。将我们的评估扩展到现实世界，Ag2Manip 显着地将模仿学习的成功率从 50% 提高到 77.5%，证明了其在模拟和物理环境中的有效性和普遍性。]]></description>
      <guid>https://arxiv.org/abs/2404.17521</guid>
      <pubDate>Tue, 30 Apr 2024 02:47:06 GMT</pubDate>
    </item>
    <item>
      <title>双子座模型在医学中的能力</title>
      <link>https://arxiv.org/abs/2404.18416</link>
      <description><![CDATA[各种医疗应用的卓越表现给人工智能带来了巨大的挑战，需要先进的推理、获取最新的医学知识以及对复杂的多模态数据的理解。 Gemini 模型在多模式和长上下文推理方面具有强大的通用能力，为医学领域提供了令人兴奋的可能性。基于 Gemini 的这些核心优势，我们推出了 Med-Gemini，这是一个功能强大的多模式模型系列，专门用于医学，能够无缝使用网络搜索，并且可以使用自定义编码器有效地针对新颖的模式进行定制。我们在 14 个医疗基准上评估 Med-Gemini，在其中 10 个基准上建立了新的最先进 (SoTA) 性能，并在每个可以进行直接比较的基准上超越了 GPT-4 模型系列，通常是广泛的利润。在流行的 MedQA (USMLE) 基准上，我们性能最佳的 Med-Gemini 模型使用新颖的不确定性引导搜索策略，实现了 91.1% 准确度的 SoTA 性能。在包括 NEJM Image Challenges 和 MMMU（健康与医学）在内的 7 个多模态基准测试中，Med-Gemini 比 GPT-4V 提高了 44.5% 的平均相对优势。我们通过从长期去识别化的健康记录和医疗视频问答中进行大海捞针检索任务的 SoTA 性能，证明了 Med-Gemini 的长上下文能力的有效性，超越了之前仅使用上下文学习的定制方法。最后，Med-Gemini 的表现表明了其在现实世界中的实用性，在医学文本摘要等任务上超越了人类专家，同时展示了多模式医学对话、医学研究和教育的巨大潜力。总而言之，我们的结果为 Med-Gemini 的潜力提供了令人信服的证据，尽管在这个安全关键领域的实际部署之前，进一步严格的评估至关重要。]]></description>
      <guid>https://arxiv.org/abs/2404.18416</guid>
      <pubDate>Tue, 30 Apr 2024 02:38:38 GMT</pubDate>
    </item>
    <item>
      <title>LEGENT：实体代理的开放平台</title>
      <link>https://arxiv.org/abs/2404.18243</link>
      <description><![CDATA[尽管大型语言模型 (LLM) 和大型多模态模型 (LMM) 取得了进展，但它们与基于语言的类人具身代理的集成仍不完整，阻碍了物理环境中复杂的现实任务执行。现有的集成通常具有有限的开源性，这对该领域的集体进步提出了挑战。我们推出了 LEGENT，这是一个开放、可扩展的平台，用于使用 LLM 和 LMM 开发具身代理。LEGENT 提供了一种双重方法：一个丰富的交互式 3D 环境，具有可通信和可操作的代理，搭配用户友好的界面，以及一个复杂的数据生成管道，利用高级算法来大规模利用模拟世界的监督。在我们的实验中，使用 LEGENT 生成的数据训练的胚胎视觉-语言-动作模型在具身任务中超越了 GPT-4V，展示了有希望的泛化能力。]]></description>
      <guid>https://arxiv.org/abs/2404.18243</guid>
      <pubDate>Tue, 30 Apr 2024 02:35:41 GMT</pubDate>
    </item>
    </channel>
</rss>