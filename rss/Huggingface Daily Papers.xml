<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>拥抱日报</title>
    <link>https://huggingface.co/papers</link>
    <description>Huggingface Daily Papers - 由 RSSHub 用爱制作（https://github.com/DIYgod/RSSHub）</description>
    <lastBuildDate>Mon, 25 Mar 2024 12:46:39 GMT</lastBuildDate>
    <item>
      <title>DragAPart：学习铰接对象的零件级运动先验</title>
      <link>https://arxiv.org/abs/2403.15382</link>
      <description><![CDATA[我们引入了 DragAPart，这是一种方法，给定一个图像和一组拖动作为输入，可以生成处于新状态的同一对象的新图像，与拖动的操作兼容。与之前专注于重新定位对象的工作不同，DragAPart 可以预测零件级交互，例如打开和关闭抽屉。我们研究这个问题作为学习通用运动模型的代理，而不限于特定的运动结构或对象类别。为此，我们从预先训练的图像生成器开始，并在我们引入的新合成数据集 Drag-a-Move 上对其进行微调。结合用于拖动和数据集随机化的新编码，新模型可以很好地推广到真实图像和不同类别。与之前的运动控制发生器相比，我们展示了更好的零件级运动理解。]]></description>
      <guid>https://arxiv.org/abs/2403.15382</guid>
      <pubDate>Mon, 25 Mar 2024 05:16:51 GMT</pubDate>
    </item>
    <item>
      <title>Champ：具有 3D 参数化指导的可控且一致的人体图像动画</title>
      <link>https://arxiv.org/abs/2403.14781</link>
      <description><![CDATA[在本研究中，我们引入了一种人类图像动画方法，通过在潜在扩散框架内利用 3D 人体参数模型来增强当前人类生成技术中的形状对齐和运动引导。该方法利用SMPL（Skinned Multi-Person Linear）模型作为3D人体参数模型来建立身体形状和姿势的统一表示。这有助于从源视频中准确捕捉复杂的人体几何形状和运动特征。具体来说，我们结合了从 SMPL 序列获得的渲染深度图像、法线图和语义图，以及基于骨架的运动指导，以丰富具有全面 3D 形状和详细姿势属性的潜在扩散模型的条件。采用集成自注意力机制的多层运动融合模块来融合空间域中的形状和运动潜在表示。通过将 3D 人体参数化模型表示为运动指导，我们可以在参考图像和源视频运动之间进行人体参数化形状对齐。对基准数据集进行的实验评估表明，该方法具有生成高质量人体动画的卓越能力，可以准确捕获姿势和形状变化。此外，我们的方法还在所提出的野生数据集上表现出卓越的泛化能力。项目页面：https://fudan-generative-vision.github.io/champ。]]></description>
      <guid>https://arxiv.org/abs/2403.14781</guid>
      <pubDate>Mon, 25 Mar 2024 05:10:25 GMT</pubDate>
    </item>
    <item>
      <title>AllHands：通过大型语言模型向我询问有关大规模逐字反馈的任何问题</title>
      <link>https://arxiv.org/abs/2403.15157</link>
      <description><![CDATA[逐字反馈构成了软件开发所必需的用户体验、意见和需求的宝贵存储库。从这些数据中有效且高效地提取有价值的见解是一项具有挑战性的任务。本文介绍了 Allhands，这是一种创新的分析框架，旨在利用大型语言模型 (LLM) 通过自然语言界面进行大规模反馈分析。 Allhands 遵循传统的反馈分析工作流程，首先对反馈进行分类和主题建模，将其转换为结构增强的格式，并结合法学硕士来增强准确性、稳健性、泛化性和用户友好性。随后，采用LLM代理以自然语言反馈用户提出的各种问题，将其翻译成Python代码执行，并提供全面的多模态响应，包括文本、代码、表格和图像。我们通过三个不同的反馈数据集评估 Allhands。实验表明，Allhands 在分析的各个阶段（包括分类和主题建模）都取得了卓越的功效，最终为用户提供了“问我任何问题”的体验，并提供全面、正确且人类可读的响应。据我们所知，Allhands 是第一个全面的反馈分析框架，支持通过自然语言界面提取洞察的多样化和定制要求。]]></description>
      <guid>https://arxiv.org/abs/2403.15157</guid>
      <pubDate>Mon, 25 Mar 2024 04:53:01 GMT</pubDate>
    </item>
    <item>
      <title>VidLA：大规模视频语言对齐</title>
      <link>https://arxiv.org/abs/2403.14870</link>
      <description><![CDATA[在本文中，我们提出了 VidLA，一种大规模视频语言对齐的方法。以前的视频语言对齐方法有两个主要限制。首先，它们不能同时捕获短程和长程时间依赖性，并且通常采用复杂的分层深度网络架构，这些架构很难与现有的预训练图像文本基础模型集成。为了有效解决这一限制，我们保持网络架构简单，并使用一组数据令牌，这些令牌以分层方式在不同时间分辨率下运行，从而考虑到视频的时间分层性质。通过采用简单的两塔架构，我们能够使用预训练的图像文本基础模型来初始化视频语言模型，从而提高最终性能。其次，由于缺乏语义对齐的大规模训练数据，现有的视频语言对齐工作举步维艰。为了克服这个问题，我们利用最近的法学硕士来策划迄今为止最大的视频语言数据集，并具有更好的视觉基础。此外，与仅包含短片的现有视频文本数据集不同，我们的数据集富含不同持续时间的视频剪辑，以帮助我们的时间分层数据标记在不同时间尺度上提取更好的表示。总体而言，实证结果表明，我们提出的方法在多个检索基准上超越了最先进的方法，尤其是在较长的视频上，并且在分类基准上表现得具有竞争力。]]></description>
      <guid>https://arxiv.org/abs/2403.14870</guid>
      <pubDate>Mon, 25 Mar 2024 04:48:17 GMT</pubDate>
    </item>
    <item>
      <title>编译器为大型语言模型生成反馈</title>
      <link>https://arxiv.org/abs/2403.14714</link>
      <description><![CDATA[我们引入了一种新颖的编译器优化范例，该范例由大型语言模型提供支持，并带有编译器反馈，以优化 LLVM 程序集的代码大小。该模型采用未优化的 LLVM IR 作为输入，并生成优化的 IR、最佳优化通道以及未优化和优化的 IR 的指令计数。然后，我们使用生成的优化遍来编译输入，并评估预测的指令计数是否正确、生成的 IR 是否可编译以及是否与已编译的代码相对应。我们将此反馈反馈给 LLM，并给它另一次优化代码的机会。与原始模型相比，此方法比 -Oz 额外提高了 0.53%。尽管通过反馈添加更多信息似乎很直观，但简单的采样技术在给定 10 个或更多样本的情况下可以实现更高的性能。]]></description>
      <guid>https://arxiv.org/abs/2403.14714</guid>
      <pubDate>Mon, 25 Mar 2024 04:44:20 GMT</pubDate>
    </item>
    <item>
      <title>FollowIR：评估和教授信息检索模型以遵循说明</title>
      <link>https://arxiv.org/abs/2403.15246</link>
      <description><![CDATA[现代大型语言模型 (LLM) 能够遵循冗长而复杂的指令，从而支持多种用户任务。然而，尽管信息检索（IR）模型使用法学硕士作为其架构的骨干，但几乎所有模型仍然只将查询作为输入，没有指令。对于少数确实接受指令的最新模型，尚不清楚它们如何使用它们。我们介绍了我们的数据集 FollowIR，它包含严格的指令评估基准以及帮助 IR 模型学习更好地遵循现实世界指令的训练集。 FollowIR 建立在 TREC 会议的悠久历史之上：由于 TREC 为人类注释者提供了确定文档相关性的指令（也称为叙述），因此 IR 模型应该能够根据这些详细指令来理解和决定相关性。我们的评估基准从三个经过深度判断的 TREC 集合开始，并更改注释器指令，重新注释相关文档。通过这个过程，我们可以通过新的成对评估框架来衡量 IR 模型遵循指令的情况。我们的结果表明，现有的检索模型无法正确使用指令，将其用于基本关键字并难以理解长格式信息。然而，我们表明 IR 模型有可能学习遵循复杂的指令：我们新的 FollowIR-7B 模型在对训练集进行微调后有了显着的改进（超过 13%）。]]></description>
      <guid>https://arxiv.org/abs/2403.15246</guid>
      <pubDate>Mon, 25 Mar 2024 04:29:36 GMT</pubDate>
    </item>
    <item>
      <title>LLM2LLM：通过新颖的迭代数据增强增强法学硕士</title>
      <link>https://arxiv.org/abs/2403.15042</link>
      <description><![CDATA[预训练的大型语言模型 (LLM) 目前是解决绝大多数自然语言处理任务的最先进技术。虽然许多实际应用程序仍然需要微调才能达到令人满意的性能水平，但其中许多应用程序处于低数据状态，这使得微调具有挑战性。为了解决这个问题，我们提出了LLM2LLM，这是一种有针对性的迭代数据增强策略，它使用教师LLM通过增强可用于对特定任务进行微调的附加数据来增强小型种子数据集。 LLM2LLM (1) 在初始种子数据上微调基线学生 LLM，(2) 评估并提取模型出错的数据点，以及 (3) 使用教师 LLM 基于这些不正确的数据点生成合成数据，然后将其添加回训练数据中。这种方法放大了法学硕士在训练期间错误预测的数据点的信号，并将它们重新整合到数据集中，以专注于法学硕士更具挑战性的示例。我们的结果表明，LLM2LLM 显着增强了 LLM 在低数据情况下的性能，优于传统的微调和其他数据增强基线。 LLM2LLM 减少了对劳动密集型数据管理的依赖，并为更具可扩展性和性能的 LLM 解决方案铺平了道路，使我们能够处理数据受限的领域和任务。与使用 LLaMA2-7B 学生在低数据状态下进行常规微调相比，我们在 GSM8K 数据集上实现了高达 24.2% 的改进，在 CaseHOLD 上实现了 32.6%，在 SNIPS 上实现了 32.0%，在 TREC 上实现了 52.6%，在 SST-2 上实现了 39.8%模型。]]></description>
      <guid>https://arxiv.org/abs/2403.15042</guid>
      <pubDate>Mon, 25 Mar 2024 04:25:47 GMT</pubDate>
    </item>
    <item>
      <title>LATTE3D：大规模摊销文本到增强型 3D 合成</title>
      <link>https://arxiv.org/abs/2403.15385</link>
      <description><![CDATA[最近的文本到 3D 生成方法可产生令人印象深刻的 3D 结果，但需要耗时的优化，每个提示可能需要长达一个小时。 ATT3D 等摊销方法可同时优化多个提示以提高效率，从而实现快速文本到 3D 合成。然而，它们无法捕获高频几何和纹理细节，并且难以扩展到大型提示集，因此它们的泛化能力很差。我们引入了 LATTE3D，解决了这些限制，以在更大的提示集上实现快速、高质量的生成。我们方法的关键是 1) 构建可扩展的架构，2) 在优化期间通过 3D 感知扩散先验、形状正则化和模型初始化利用 3D 数据，以实现对多样化和复杂的训练提示的鲁棒性。 LATTE3D 分摊神经场和纹理表面生成，以在单次前向传递中生成高度详细的纹理网格。 LATTE3D 在 400 毫秒内生成 3D 对象，并且可以通过快速测试时间优化进一步增强。]]></description>
      <guid>https://arxiv.org/abs/2403.15385</guid>
      <pubDate>Mon, 25 Mar 2024 04:24:40 GMT</pubDate>
    </item>
    <item>
      <title>ThemeStation：从少数示例中生成主题感知的 3D 资源</title>
      <link>https://arxiv.org/abs/2403.15383</link>
      <description><![CDATA[现实世界的应用程序通常需要大量共享一致主题的 3D 资源。虽然从文本或图像创建一般 3D 内容已经取得了显着的进步，但按照输入 3D 示例的共享主题合成定制 3D 资产仍然是一个开放且具有挑战性的问题。在这项工作中，我们提出了 ThemeStation，这是一种用于主题感知 3D 到 3D 生成的新颖方法。 ThemeStation 根据给定的几个示例合成定制的 3D 资产，其目标有两个：1) 统一性，用于生成与给定示例主题一致的 3D 资产；2) 多样性，用于生成具有高度变化的 3D 资产。为此，我们设计了一个两阶段框架，首先绘制概念图像，然后是参考参考的 3D 建模阶段。我们提出了一种新颖的双分数蒸馏（DSD）损失来联合利用输入样本和合成概念图像的先验。大量的实验和用户研究证实，ThemeStation 在生成各种主题感知 3D 模型方面超越了之前的作品，质量令人印象深刻。 ThemeStation 还支持各种应用，例如可控 3D 到 3D 生成。]]></description>
      <guid>https://arxiv.org/abs/2403.15383</guid>
      <pubDate>Mon, 25 Mar 2024 04:20:28 GMT</pubDate>
    </item>
    <item>
      <title>InternVideo2：扩展视频基础模型以实现多模态视频理解</title>
      <link>https://arxiv.org/abs/2403.15377</link>
      <description><![CDATA[我们推出了 InternVideo2，这是一种新的视频基础模型 (ViFM)，它在动作识别、视频文本任务和以视频为中心的对话方面实现了最先进的性能。我们的方法采用渐进式训练范式，统一了屏蔽视频令牌重建、跨模式对比学习和下一个令牌预测的不同自监督或弱监督学习框架。不同的训练阶段将指导我们的模型通过不同的借口任务捕获不同级别的结构和语义信息。在数据层面，我们通过对视频进行语义分割并生成视频-音频-语音字幕来优先考虑时空一致性。这改善了视频和文本之间的对齐。我们缩放了 InternVideo2 的数据和模型大小。通过大量实验，我们验证了我们的设计，并在 60 多个视频和音频任务中展示了最先进的性能。值得注意的是，我们的模型在各种与视频相关的字幕、对话和长视频理解基准方面优于其他模型，突显了其推理和理解长时间上下文的能力。代码和模型可在 https://github.com/OpenGVLab/InternVideo2/ 获取。]]></description>
      <guid>https://arxiv.org/abs/2403.15377</guid>
      <pubDate>Mon, 25 Mar 2024 01:39:49 GMT</pubDate>
    </item>
    </channel>
</rss>