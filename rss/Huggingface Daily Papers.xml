<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>拥抱日报</title>
    <link>https://huggingface.co/papers</link>
    <description>Huggingface Daily Papers - RSSHub 用爱制作的(https://github.com/DIYgod/RSSHub)</description>
    <lastBuildDate>Tue, 23 Apr 2024 12:24:14 GMT</lastBuildDate>
    <item>
      <title>学习 H-Infinity 运动控制</title>
      <link>https://arxiv.org/abs/2404.14405</link>
      <description><![CDATA[在险峻环境中稳定运动是四足机器人的一项基本能力，需要能够抵抗各种外部干扰。然而，最近的基于学习的策略仅使用基本域随机化来提高学习策略的鲁棒性，这不能保证机器人具有足够的抗干扰能力。在本文中，我们提出将学习过程建模为参与者与新引入的干扰者之间的对抗性互动，并通过 H_{infty} 约束确保它们的优化。与最大化折扣总体奖励的参与者不同，干扰者负责产生有效的外力，并通过最大化任务奖励与其预言机之间的误差（即每次迭代中的“成本”）来进行优化。为了保持参与者和干扰者之间的联合优化稳定，我们的 H_{infty} 约束规定了成本与外力强度之间的比率界限。通过整个训练阶段的相互互动，参与者可以获得应对日益复杂的物理干扰的能力。我们用 Unitree Aliengo 机器人验证了我们的方法在四足运动任务上的稳健性，还用 Unitree A1 机器人验证了更具挑战性的任务，其中四足机器人预计会像双足机器人一样仅用后腿进行运动。模拟定量结果显示与基线相比有所改进，证明了该方法和每种设计选择的有效性。另一方面，真实机器人实验定性地展示了该策略在干扰各种地形（包括楼梯、高台、斜坡和湿滑地形）上的各种干扰时有多稳健。所有代码、检查点和真实世界部署指南都将公开。]]></description>
      <guid>https://arxiv.org/abs/2404.14405</guid>
      <pubDate>Tue, 23 Apr 2024 03:19:19 GMT</pubDate>
    </item>
    <item>
      <title>场景坐标重建：通过重新定位器的增量学习来摆出图像集合的姿势</title>
      <link>https://arxiv.org/abs/2404.14351</link>
      <description><![CDATA[我们解决从一组描绘场景的图像估计相机参数的任务。流行的基于特征的运动结构 (SfM) 工具通过增量重建来解决此任务：它们重复稀疏 3D 点的三角测量并将更多摄像机视图注册到稀疏点云。我们将增量运动结构重新解释为视觉重定位器的迭代应用和细化，即将新视图注册到重建的当前状态的方法。这个视角使我们能够研究不植根于局部特征匹配的替代视觉重定位器。我们展示了场景坐标回归，一种基于学习的重定位方法，允许我们从未摆出的图像构建隐式的神经场景表示。与其他基于学习的重建方法不同，我们不需要先验姿势，也不需要顺序输入，并且我们可以对数千张图像进行有效优化。我们的方法 ACE0（ACE Zero）估计相机姿态的精度与基于特征的 SfM 相当，正如新颖的视图合成所证明的那样。项目页面：https://nianticlabs.github.io/acezero/]]></description>
      <guid>https://arxiv.org/abs/2404.14351</guid>
      <pubDate>Tue, 23 Apr 2024 03:14:32 GMT</pubDate>
    </item>
    <item>
      <title>指令层次结构：培训法学硕士优先考虑特权指令</title>
      <link>https://arxiv.org/abs/2404.13208</link>
      <description><![CDATA[当今的法学硕士很容易受到提示注入、越狱和其他攻击，这些攻击使攻击者可以用自己的恶意提示覆盖模型的原始指令。在这项工作中，我们认为这些攻击背后的主要漏洞之一是法学硕士通常认为系统提示（例如来自应用程序开发人员的文本）与来自不受信任的用户和第三方的文本具有相同的优先级。为了解决这个问题，我们提出了一种指令层次结构，它明确定义了当不同优先级的指令发生冲突时模型应如何表现。然后，我们提出了一种数据生成方法来演示这种分层指令跟踪行为，该行为教导法学硕士有选择地忽略权限较低的指令。我们将此方法应用于 GPT-3.5，表明它极大地提高了鲁棒性——即使对于训练期间未见的攻击类型也是如此——同时对标准功能的影响最小。]]></description>
      <guid>https://arxiv.org/abs/2404.13208</guid>
      <pubDate>Tue, 23 Apr 2024 03:07:48 GMT</pubDate>
    </item>
    <item>
      <title>低位量化 LLaMA3 模型有多好？实证研究</title>
      <link>https://arxiv.org/abs/2404.14047</link>
      <description><![CDATA[Meta 的 LLaMA 系列已成为最强大的开源大型语言模型 (LLM) 系列之一。值得注意的是，LLaMA3 模型最近已发布，并通过对超过 15T 代币的数据进行超大规模预训练，在各种方面取得了令人印象深刻的性能。鉴于 LLM 低位量化在资源有限的场景中的广泛应用，我们探索了 LLaMA3 在量化为低位宽时的功能。这一探索有可能为 LLaMA3 和其他即将推出的 LLM 的低位量化揭示新的见解和挑战，特别是在解决 LLM 压缩中遇到的性能下降问题方面。具体来说，我们在 1-8 位和不同的数据集上评估了 LLaMA3 现有的 10 种训练后量化和 LoRA 微调方法，以全面揭示 LLaMA3 的低位量化性能。我们的实验结果表明，LLaMA3 在这些场景中仍然会遭受非疏忽的退化，特别是在超低位宽下。这凸显了低位宽下的显着性能差距，需要在未来的开发中弥合。我们预计这项实证研究将在推进未来模型方面发挥重要作用，推动法学硕士以更低的位宽和更高的精度实现实用。我们的项目发布在 https://github.com/Macaronlin/LLaMA3-Quantization 上，量化的 LLaMA3 模型发布在 https://huggingface.co/LLMQ 上。]]></description>
      <guid>https://arxiv.org/abs/2404.14047</guid>
      <pubDate>Tue, 23 Apr 2024 03:05:51 GMT</pubDate>
    </item>
    <item>
      <title>FlowMind：使用法学硕士自动生成工作流程</title>
      <link>https://arxiv.org/abs/2404.13050</link>
      <description><![CDATA[快速发展的机器人流程自动化 (RPA) 领域在自动化重复流程方面取得了重大进展，但在需要用户要求的自发或不可预测任务的场景中，其有效性会降低。本文介绍了一种新颖的方法 FlowMind，利用大型语言模型 (LLM)（例如生成预训练变换器 (GPT)）的功能来解决此限制并创建自动工作流生成系统。在 FlowMind 中，我们提出了一个通用的讲座提示方案，有助于通过可靠的应用程序编程接口 (API) 奠定 LLM 推理的基础。这样，FlowMind 不仅可以缓解法学硕士中常见的幻觉问题，还可以消除法学硕士与专有数据或代码之间的直接交互，从而确保信息的完整性和机密性 - 这是金融服务的基石。 FlowMind 通过呈现自动生成的工作流程的高级描述来进一步简化用户交互，使用户能够有效地检查和提供反馈。我们还引入了 NCEN-QA，这是一个新的金融数据集，用于对 N-CEN 基金报告中的问答任务进行基准测试。我们使用 NCEN-QA 对照 FlowMind 的基线和消融变体来评估 FlowMind 生成的工作流程的性能。我们展示了 FlowMind 的成功、拟议讲座方案中每个组件的重要性以及 FlowMind 中用户交互和反馈的有效性。]]></description>
      <guid>https://arxiv.org/abs/2404.13050</guid>
      <pubDate>Tue, 23 Apr 2024 02:53:01 GMT</pubDate>
    </item>
    <item>
      <title>Hyper-SD：用于高效图像合成的轨迹分段一致性模型</title>
      <link>https://arxiv.org/abs/2404.13686</link>
      <description><![CDATA[最近，出现了一系列扩散感知蒸馏算法，以减轻与扩散模型（DM）的多步骤推理过程相关的计算开销。当前的蒸馏技术通常分为两个不同的方面：i) ODE 轨迹保持； ii) ODE 轨迹重构。然而，这些方法会遭受严重的性能下降或域转移。为了解决这些限制，我们提出了 Hyper-SD，这是一种新颖的框架，它协同地融合了 ODE 轨迹保存和重构的优点，同时在步骤压缩期间保持近乎无损的性能。首先，我们引入轨迹分段一致性蒸馏，在预定义的时间步段内逐步进行一致性蒸馏，这有利于从高阶角度保存原始 ODE 轨迹。其次，我们结合人类反馈学习来提高模型在低步状态下的性能，并减轻蒸馏过程带来的性能损失。第三，我们集成了分数蒸馏，以进一步提高模型的低步生成能力，并首次尝试利用统一的LoRA来支持所有步骤的推理过程。大量实验和用户研究表明，Hyper-SD 对于 SDXL 和 SD1.5 来说都可以通过 1 到 8 个推理步骤实现 SOTA 性能。例如，在 1 步推理中，Hyper-SDXL 在 CLIP 分数上超过 SDXL-Lightning +0.68，在 Aes 分数上超过 +0.51。]]></description>
      <guid>https://arxiv.org/abs/2404.13686</guid>
      <pubDate>Tue, 23 Apr 2024 02:42:36 GMT</pubDate>
    </item>
    <item>
      <title>多模态自动解释代理</title>
      <link>https://arxiv.org/abs/2404.14394</link>
      <description><![CDATA[本文介绍了 MAIA，一种多模式自动解释代理。 MAIA 是一个使用神经模型自动执行神经模型理解任务（例如特征解释和故障模式发现）的系统。它为预先训练的视觉语言模型配备了一组工具，支持对其他模型的子组件进行迭代实验以解释其行为。其中包括人类可解释性研究人员常用的工具：用于合成和编辑输入、计算来自现实世界数据集的最大激活样本以及总结和描述实验结果。 MAIA 提出的可解释性实验组成了这些工具来描述和解释系统行为。我们评估 MAIA 在计算机视觉模型中的应用。我们首先描述 MAIA 在学习的图像表示中描述（神经元级）特征的能力。通过多个经过训练的模型和具有配对真实描述的合成视觉神经元的新颖数据集，MAIA 生成的描述可与人类实验专家生成的描述相媲美。然后，我们证明 MAIA 可以帮助完成两项额外的可解释性任务：降低对虚假特征的敏感性，以及自动识别可能被错误分类的输入。]]></description>
      <guid>https://arxiv.org/abs/2404.14394</guid>
      <pubDate>Tue, 23 Apr 2024 02:36:48 GMT</pubDate>
    </item>
    <item>
      <title>Phi-3 技术报告：手机本地功能强大的语言模型</title>
      <link>https://arxiv.org/abs/2404.14219</link>
      <description><![CDATA[我们引入了 phi-3-mini，这是一个在 3.3 万亿个代币上训练的 38 亿参数语言模型，根据学术基准和内部测试衡量，其整体性能可与 Mixtral 8x7B 和 GPT-3.5 等模型相媲美（例如 phi -3-mini 在 MMLU 上达到了 69%，在 MT-bench 上达到了 8.38），尽管它足够小，可以部署在手机上。创新完全在于我们的训练数据集，这是用于 phi-2 的数据集的放大版本，由经过严格过滤的网络数据和合成数据组成。该模型还进一步调整了稳健性、安全性和聊天格式。我们还提供了一些针对 4.8T 代币训练的 7B 和 14B 模型的初始参数缩放结果，称为 phi-3-small 和 phi-3-medium，两者都比 phi-3-mini 能力更强（例如，分别为 75%） MMLU 上为 78%，MT 基准上为 8.7 和 8.9）。]]></description>
      <guid>https://arxiv.org/abs/2404.14219</guid>
      <pubDate>Tue, 23 Apr 2024 02:31:22 GMT</pubDate>
    </item>
    <item>
      <title>SEED-X：具有统一多粒度理解和生成的多模态模型</title>
      <link>https://arxiv.org/abs/2404.14396</link>
      <description><![CDATA[多模态基础模型的快速发展已经证明了视觉语言理解和生成方面的重大进展，例如我们之前的工作 SEED-LLaMA。然而，其能力与现实世界的适用性之间仍然存在差距，这主要是由于该模型有效响应各种用户指令并与各种视觉数据交互的能力有限。在这项工作中，我们致力于通过集成两个增强功能来弥补这一差距：（1）理解任意大小和比例的图像，以及（2）实现多粒度图像生成。我们提出了一个统一且通用的基础模型，即 SEED-X，它能够为理解和生成任务建模多粒度视觉语义。除了公共基准测试中具有竞争力的结果之外，SEED-X 还展示了其在指令调整后跨各个领域处理实际应用程序的有效性。我们希望我们的工作能够激发未来的研究，探索多功能多模态基础模型在实际应用中可以实现的目标。模型、代码和数据集将在 https://github.com/AILab-CVC/SEED-X 中发布。]]></description>
      <guid>https://arxiv.org/abs/2404.14396</guid>
      <pubDate>Tue, 23 Apr 2024 02:15:24 GMT</pubDate>
    </item>
    <item>
      <title>MultiBooth：从文本生成图像中的所有概念</title>
      <link>https://arxiv.org/abs/2404.14239</link>
      <description><![CDATA[本文介绍了 MultiBooth，这是一种新颖且高效的技术，用于从文本生成图像中进行多概念定制。尽管定制生成方法取得了显着进步，特别是随着扩散模型的成功，但由于概念保真度低和推理成本高，现有方法常常难以应对多概念场景。 MultiBooth 通过将多概念生成过程分为两个阶段来解决这些问题：单概念学习阶段和多概念集成阶段。在单概念学习阶段，我们采用多模态图像编码器和有效的概念编码技术来学习每个概念的简洁且有区别的表示。在多概念整合阶段，我们使用边界框来定义交叉注意力图中每个概念的生成区域。该方法使得能够在其指定区域内创建单独的概念，从而促进多概念图像的形成。这种策略不仅提高了概念保真度，还降低了额外的推理成本。 MultiBooth 在定性和定量评估方面都超越了各种基线，展示了其卓越的性能和计算效率。项目页面：https://multibooth.github.io/]]></description>
      <guid>https://arxiv.org/abs/2404.14239</guid>
      <pubDate>Tue, 23 Apr 2024 01:52:56 GMT</pubDate>
    </item>
    </channel>
</rss>