<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>拥抱日报</title>
    <link>https://huggingface.co/papers</link>
    <description>Huggingface Daily Papers - RSSHub 用爱制作的(https://github.com/DIYgod/RSSHub)</description>
    <lastBuildDate>Thu, 21 Mar 2024 05:12:35 GMT</lastBuildDate>
    <item>
      <title>Magic Fixup：通过观看动态视频简化照片编辑</title>
      <link>https://arxiv.org/abs/2403.13044</link>
      <description><![CDATA[我们提出了一种生成模型，在给定粗略编辑的图像的情况下，合成遵循规定布局的真实感输出。我们的方法从原始图像中转移精细细节并保留其各部分的特性。然而，它使其适应新布局定义的照明和环境。我们的主要见解是，视频是这项任务的强大监督来源：物体和摄像机运动提供了许多关于世界如何随着视点、照明和物理交互而变化的观察。我们构建一个图像数据集，其中每个样本都是以随机选择的时间间隔从同一视频中提取的一对源帧和目标帧。我们使用两个模拟预期测试时用户编辑的运动模型将源帧向目标扭曲。我们从预训练的扩散模型开始，监督我们的模型将扭曲的图像转换为地面实况。我们的模型设计明确地实现了从源帧到生成图像的精细细节传输，同时严格遵循用户指定的布局。我们表明，通过使用简单的分割和粗略的二维操作，我们可以合成忠实于用户输入的逼真编辑，同时解决二阶效果，例如协调编辑对象之间的照明和物理交互。]]></description>
      <guid>https://arxiv.org/abs/2403.13044</guid>
      <pubDate>Thu, 21 Mar 2024 02:56:19 GMT</pubDate>
    </item>
    <item>
      <title>我们什么时候不需要更大的视觉模型？</title>
      <link>https://arxiv.org/abs/2403.13043</link>
      <description><![CDATA[扩大视觉模型的尺寸已经成为获得更强大的视觉表示的事实上的标准。在这项工作中，我们讨论了不需要更大的视觉模型的点。首先，我们展示了尺度缩放 (S^2) 的强大功能，即预训练和冻结的较小视觉模型（例如 ViT-B 或 ViT-L）在多个图像尺度上运行，可以优于较大的模型（例如 ViT-B 或 ViT-L）。 、ViT-H 或 ViT-G），涉及分类、分割、深度估计、多模态 LLM (MLLM) 基准和机器人操作。值得注意的是，S^2 在 V* 基准上对 MLLM 的详细理解方面实现了最先进的性能，超越了 GPT-4V 等模型。我们研究了与模型大小缩放相比 S^2 成为首选缩放方法的条件。虽然较大的模型具有在困难示例上更好泛化的优势，但我们表明，较大视觉模型的特征可以通过多尺度较小模型的特征很好地近似。这表明当前大型预训练模型学习到的大多数（如果不是全部）表示也可以从多尺度较小模型中获得。我们的结果表明，多尺度的较小模型具有与较大模型相当的学习能力，并且使用 S^2 预训练较小模型可以匹配甚至超过较大模型的优势。我们发布了一个 Python 包，可以通过一行代码在任何视觉模型上应用 S^2：https://github.com/bfshi/scaling_on_scales。]]></description>
      <guid>https://arxiv.org/abs/2403.13043</guid>
      <pubDate>Thu, 21 Mar 2024 02:53:18 GMT</pubDate>
    </item>
    <item>
      <title>SceneScript：使用自回归结构化语言模型重建场景</title>
      <link>https://arxiv.org/abs/2403.13064</link>
      <description><![CDATA[我们引入了 SceneScript，这是一种使用自回归、基于标记的方法直接将完整场景模型生成为结构化语言命令序列的方法。我们提出的场景表示的灵感来自于最近在《变形金刚》和《变形金刚》中取得的成功。法学硕士，并不同于通常将场景描述为网格、体素网格、点云或辐射场的更传统方法。我们的方法使用场景语言编码器-解码器架构直接从编码的视觉数据推断结构化语言命令集。为了训练 SceneScript，我们生成并发布了一个名为 Aria Synthetic Environments 的大型合成数据集，其中包含 10 万个高质量室内场景，以及以自我为中心的场景演练的真实感和真实注释渲染。我们的方法在建筑布局估计方面提供了最先进的结果，在 3D 对象检测方面提供了有竞争力的结果。最后，我们探讨了 SceneScript 的一个优势，即能够通过对结构化语言进行简单添加来轻松适应新命令，我们将针对粗略 3D 对象部分重建等任务进行说明。]]></description>
      <guid>https://arxiv.org/abs/2403.13064</guid>
      <pubDate>Thu, 21 Mar 2024 02:46:19 GMT</pubDate>
    </item>
    <item>
      <title>评估危险能力的前沿模型</title>
      <link>https://arxiv.org/abs/2403.13793</link>
      <description><![CDATA[要了解新的人工智能系统带来的风险，我们必须了解它能做什么和不能做什么。在之前工作的基础上，我们引入了新的“危险能力”评估计划，并在 Gemini 1.0 模型上进行了试点。我们的评估涵盖四个方面：（1）说服和欺骗； (2) 网络安全； (3)自我增殖； (4)自我推理。我们在评估的模型中没有发现强大危险能力的证据，但我们标记了早期预警信号。我们的目标是帮助推进严格的危险能力评估科学，为未来的模型做好准备。]]></description>
      <guid>https://arxiv.org/abs/2403.13793</guid>
      <pubDate>Thu, 21 Mar 2024 02:43:59 GMT</pubDate>
    </item>
    <item>
      <title>RewardBench：评估语言建模的奖励模型</title>
      <link>https://arxiv.org/abs/2403.13787</link>
      <description><![CDATA[奖励模型 (RM) 是 RLHF 成功的关键，使预训练模型符合人类偏好，但关注这些奖励模型评估的研究相对较少。评估奖励模型提供了一个机会来了解用于对齐语言模型的不透明技术以及其中嵌入了哪些价值。迄今为止，关于能力、训练方法或开源奖励模型的描述很少。在本文中，我们提出了 RewardBench，一个用于评估的基准数据集和代码库，以增强对奖励模型的科学理解。 RewardBench 数据集是涵盖聊天、推理和安全性的即时胜负三重奏的集合，用于衡量奖励模型在具有挑战性、结构化和分布外查询上的表现。我们为 RM 创建了特定的比较数据集，这些数据集具有微妙但可验证的原因（例如错误、不正确的事实），说明为什么一个答案应该优于另一个答案。在 RewardBench 排行榜上，我们评估了使用各种方法训练的奖励模型，例如分类器的直接 MLE 训练和直接偏好优化 (DPO) 的隐式奖励建模，以及一系列数据集。我们提出了许多关于拒绝倾向、推理局限性以及各种奖励模型的缺点的指导的发现，以更好地理解 RLHF 过程。]]></description>
      <guid>https://arxiv.org/abs/2403.13787</guid>
      <pubDate>Thu, 21 Mar 2024 02:41:06 GMT</pubDate>
    </item>
    <item>
      <title>逆向训练以护理逆向咒语</title>
      <link>https://arxiv.org/abs/2403.13799</link>
      <description><![CDATA[大型语言模型 (LLM) 有一个令人惊讶的失败：当训练“A 有一个特征 B”时，它们不会概括为“B 是 A 的一个特征”，这被称为“逆转诅咒”。即使使用数万亿个代币进行训练，由于齐普夫定律，这个问题仍然会出现 - 因此即使我们在整个互联网上进行训练。这项工作提出了一种替代训练方案，称为反向训练，其中所有单词都使用两次，使可用标记的数量加倍。通过反转训练字符串，同时保留（即不反转）选定的子字符串（例如实体），LLM 可以在正向和反向方向上进行训练。我们表明，数据匹配的反向训练模型在标准任务上提供了比标准模型更优越的性能，而计算匹配的反向训练模型在逆转任务上提供了远为优越的性能，有助于解决逆转诅咒问题。]]></description>
      <guid>https://arxiv.org/abs/2403.13799</guid>
      <pubDate>Thu, 21 Mar 2024 02:38:56 GMT</pubDate>
    </item>
    <item>
      <title>IDAdapter：学习混合特征以实现文本到图像模型的免调整个性化</title>
      <link>https://arxiv.org/abs/2403.13535</link>
      <description><![CDATA[利用稳定扩散来生成个性化肖像已成为一种强大且值得注意的工具，使用户能够根据其特定提示创建高保真、自定义角色头像。然而，现有的个性化方法面临着挑战，包括测试时微调、需要多个输入图像、身份保留度低以及生成结果的多样性有限。为了克服这些挑战，我们引入了 IDAdapter，这是一种免调整方法，可以增强从单张人脸图像生成个性化图像的多样性和身份保留。 IDAdapter 通过文本和视觉注入以及面部身份丢失的组合，将个性化概念集成到生成过程中。在训练阶段，我们融合了特定身份的多个参考图像的混合特征，以丰富与身份相关的内容细节，指导模型生成与之前的作品相比风格、表情和角度更加多样化的图像。广泛的评估证明了我们方法的有效性，在生成的图像中实现了多样性和身份保真度。]]></description>
      <guid>https://arxiv.org/abs/2403.13535</guid>
      <pubDate>Thu, 21 Mar 2024 02:36:47 GMT</pubDate>
    </item>
    <item>
      <title>Mora：通过多代理框架实现通用视频生成</title>
      <link>https://arxiv.org/abs/2403.13248</link>
      <description><![CDATA[Sora 是第一个大规模通用视频生成模型，引起了社会的广泛关注。自 OpenAI 于 2024 年 2 月推出以来，没有其他视频生成模型能够与 {Sora} 的性能或支持广泛视频生成任务的能力相媲美。此外，只有少数完全发布的视频生成模型，其中大多数是闭源的。为了解决这一差距，本文提出了一种新的多智能体框架 Mora，它结合了多个先进的视觉 AI 智能体来复制 Sora 演示的通用视频生成。特别是，Mora 可以利用多个视觉代理并在各种任务中成功模仿 Sora 的视频生成功能，例如（1）文本到视频生成，（2）文本条件图像到视频生成，（3）扩展生成视频、(4) 视频到视频编辑、(5) 连接视频和 (6) 模拟数字世界。我们大量的实验结果表明，Mora 在各种任务中都取得了与 Sora 相近的性能。但综合来看，我们的工作与Sora之间存在着明显的绩效差距。总而言之，我们希望这个项目能够通过协作人工智能代理来指导视频生成的未来轨迹。]]></description>
      <guid>https://arxiv.org/abs/2403.13248</guid>
      <pubDate>Thu, 21 Mar 2024 01:36:08 GMT</pubDate>
    </item>
    </channel>
</rss>